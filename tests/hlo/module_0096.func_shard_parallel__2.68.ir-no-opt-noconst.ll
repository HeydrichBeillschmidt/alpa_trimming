target datalayout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64"
target triple = "nvptx64-nvidia-cuda"

@rng_state = external dso_local addrspace(1) global i128
@buffer_for_constant_3 = external constant [8 x i8], align 128
@0 = external dso_local unnamed_addr constant [4 x i8]
@1 = external dso_local unnamed_addr constant [4 x i8]
@2 = external dso_local unnamed_addr constant [4 x i8]
@3 = external dso_local unnamed_addr constant [4 x i8]
@4 = external dso_local unnamed_addr constant [8 x i8]
@5 = external dso_local unnamed_addr constant [4 x i8]
@6 = external dso_local unnamed_addr constant [8 x i8]
@7 = external dso_local unnamed_addr constant [4 x i8]
@8 = external dso_local unnamed_addr constant [4 x i8]
@9 = external dso_local unnamed_addr constant [4 x i8]
@10 = external dso_local unnamed_addr constant [4 x i8]
@11 = external dso_local unnamed_addr constant [8 x i8]
@12 = external dso_local unnamed_addr constant [4 x i8]
@13 = external dso_local unnamed_addr constant [4 x i8]
@14 = external dso_local unnamed_addr constant [4 x i8]
@15 = external dso_local unnamed_addr constant [4 x i8]
@16 = external dso_local unnamed_addr constant [4 x i8]
@17 = external dso_local unnamed_addr constant [4 x i8]
@18 = external dso_local unnamed_addr constant [4 x i8]
@19 = external dso_local unnamed_addr constant [4 x i8]
@20 = external dso_local unnamed_addr constant [4 x i8]
@21 = external dso_local unnamed_addr constant [4 x i8]
@22 = external dso_local unnamed_addr constant [4 x i8]
@23 = external dso_local unnamed_addr constant [4 x i8]
@24 = external dso_local unnamed_addr constant [4 x i8]
@25 = external dso_local unnamed_addr constant [4 x i8]
@26 = external dso_local unnamed_addr constant [4 x i8]
@27 = external dso_local unnamed_addr constant [4 x i8]
@28 = external dso_local unnamed_addr constant [4 x i8]
@29 = external dso_local unnamed_addr constant [4 x i8]
@30 = external dso_local unnamed_addr constant [1 x i8]
@31 = external dso_local unnamed_addr constant [4 x i8]
@32 = external dso_local unnamed_addr constant [4 x i8]
@33 = external dso_local unnamed_addr constant [4 x i8]
@34 = external dso_local unnamed_addr constant [8 x i8]
@35 = external dso_local unnamed_addr constant [4 x i8]
@36 = external dso_local unnamed_addr constant [8 x i8]
@37 = external dso_local unnamed_addr constant [4 x i8]
@38 = external dso_local unnamed_addr constant [4 x i8]
@39 = external dso_local unnamed_addr constant [4 x i8]
@40 = external dso_local unnamed_addr constant [4 x i8]
@41 = external dso_local unnamed_addr constant [8 x i8]
@42 = external dso_local unnamed_addr constant [4 x i8]
@43 = external dso_local unnamed_addr constant [4 x i8]
@44 = external dso_local unnamed_addr constant [4 x i8]
@45 = external dso_local unnamed_addr constant [4 x i8]
@46 = external dso_local unnamed_addr constant [4 x i8]
@47 = external dso_local unnamed_addr constant [4 x i8]
@48 = external dso_local unnamed_addr constant [4 x i8]
@49 = external dso_local unnamed_addr constant [4 x i8]
@50 = external dso_local unnamed_addr constant [4 x i8]
@51 = external dso_local unnamed_addr constant [4 x i8]
@52 = external dso_local unnamed_addr constant [4 x i8]
@53 = external dso_local unnamed_addr constant [4 x i8]
@54 = external dso_local unnamed_addr constant [4 x i8]
@55 = external dso_local unnamed_addr constant [4 x i8]
@56 = external dso_local unnamed_addr constant [4 x i8]
@57 = external dso_local unnamed_addr constant [4 x i8]
@buffer_for_constant_5 = external constant [4 x i8], align 128

define void @rng_get_and_update_state(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 50176
  %1 = bitcast i8* %0 to [2 x i64]*
  %load_state = load i128, i128 addrspace(1)* @rng_state, align 16
  %2 = add i128 %load_state, 4096
  store i128 %2, i128 addrspace(1)* @rng_state, align 16
  %3 = bitcast [2 x i64]* %1 to i64*
  %rng_state_address = getelementptr inbounds i64, i64* %3, i64 0
  %4 = bitcast i64* %rng_state_address to i128*
  store i128 %load_state, i128* %4, align 16
  ret void
}

define void @fusion_5(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [256 x [16 x float]]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 50176
  %3 = bitcast i8* %2 to [2 x i64]*
  %4 = getelementptr inbounds i8, i8* %temp_buf, i64 16384
  %5 = bitcast i8* %4 to [256 x [16 x float]]*
  %6 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !20
  %7 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !21
  %8 = mul nuw nsw i32 %6, 256
  %linear_index = add nuw nsw i32 %8, %7
  %linear_index_in_range = icmp ult i32 %linear_index, 1024
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %9 = udiv i32 %linear_index_base, 1
  %10 = urem i32 %9, 16
  %11 = udiv i32 %linear_index_base, 16
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %12 = udiv i32 %linear_index1, 1
  %13 = urem i32 %12, 16
  %14 = udiv i32 %linear_index1, 16
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %15 = udiv i32 %linear_index2, 1
  %16 = urem i32 %15, 16
  %17 = udiv i32 %linear_index2, 16
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %18 = udiv i32 %linear_index3, 1
  %19 = urem i32 %18, 16
  %20 = udiv i32 %linear_index3, 16
  %21 = icmp ult i32 %linear_index_base, 4096
  br i1 %21, label %fusion_5.in_bounds-true, label %fusion_5.in_bounds-after

fusion_5.in_bounds-after:                         ; preds = %concatenate.226.merge535, %entry
  ret void

fusion_5.in_bounds-true:                          ; preds = %entry
  %22 = mul nuw nsw i32 %10, 1
  %23 = add nuw nsw i32 0, %22
  %24 = udiv i32 %23, 16
  %25 = mul nuw nsw i32 %11, 1
  %26 = add nuw nsw i32 0, %25
  %27 = urem i32 %26, 32
  %28 = udiv i32 %26, 32
  %29 = udiv i32 %28, 8
  %30 = mul nuw nsw i32 %23, 1
  %31 = add nuw nsw i32 0, %30
  %32 = mul nuw nsw i32 %27, 16
  %33 = add nuw nsw i32 %31, %32
  %34 = mul nuw nsw i32 %28, 512
  %35 = add nuw nsw i32 %33, %34
  %36 = urem i32 %35, 4
  %37 = udiv i32 %35, 4
  %38 = udiv i32 %37, 1024
  br label %concatenate.pivot.2.

concat_index_from_operand_id0:                    ; preds = %concatenate.pivot.0.
  %39 = phi i32 [ 0, %concatenate.pivot.0. ]
  %40 = sub nsw i32 %36, %39
  %41 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %42 = load i64, i64* %41, align 8, !invariant.load !22
  %43 = trunc i64 %42 to i32
  %44 = zext i32 %43 to i64
  %45 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %46 = lshr i64 %42, %45
  %shft.chk = icmp ult i64 %45, 64
  %47 = select i1 %shft.chk, i64 %46, i64 0
  %48 = trunc i64 %47 to i32
  %49 = zext i32 %48 to i64
  %50 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %51 = shl i64 %49, %50
  %shft.chk1 = icmp ult i64 %50, 64
  %52 = select i1 %shft.chk1, i64 %51, i64 0
  %53 = or i64 %44, %52
  %54 = mul nuw nsw i32 %37, 1
  %55 = add nuw nsw i32 0, %54
  %56 = zext i32 %55 to i64
  %57 = add i64 %53, %56
  %58 = trunc i64 %57 to i32
  %59 = zext i32 %58 to i64
  %region_0_243_constant_18 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %60 = mul i64 %59, %region_0_243_constant_18
  %61 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %62 = lshr i64 %60, %61
  %shft.chk2 = icmp ult i64 %61, 64
  %63 = select i1 %shft.chk2, i64 %62, i64 0
  %64 = trunc i64 %63 to i32
  %65 = icmp ult i64 %57, %53
  %66 = zext i1 %65 to i8
  %67 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %68 = load i64, i64* %67, align 8, !invariant.load !22
  %69 = trunc i64 %68 to i32
  %70 = zext i32 %69 to i64
  %71 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %72 = lshr i64 %68, %71
  %shft.chk3 = icmp ult i64 %71, 64
  %73 = select i1 %shft.chk3, i64 %72, i64 0
  %74 = trunc i64 %73 to i32
  %75 = zext i32 %74 to i64
  %76 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %77 = shl i64 %75, %76
  %shft.chk4 = icmp ult i64 %76, 64
  %78 = select i1 %shft.chk4, i64 %77, i64 0
  %79 = or i64 %70, %78
  %region_0_243_constant_34 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %80 = add i64 %79, %region_0_243_constant_34
  %81 = trunc i8 %66 to i1
  %82 = select i1 %81, i64 %80, i64 %79
  %83 = lshr i64 %82, %61
  %shft.chk5 = icmp ult i64 %61, 64
  %84 = select i1 %shft.chk5, i64 %83, i64 0
  %85 = trunc i64 %84 to i32
  %86 = xor i32 %64, %85
  %region_0_243_constant_42 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %87 = xor i32 %86, %region_0_243_constant_42
  %88 = zext i32 %87 to i64
  %region_0_243_constant_46 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %89 = mul i64 %88, %region_0_243_constant_46
  %90 = lshr i64 %89, %61
  %shft.chk6 = icmp ult i64 %61, 64
  %91 = select i1 %shft.chk6, i64 %90, i64 0
  %92 = trunc i64 %91 to i32
  %93 = trunc i64 %82 to i32
  %94 = zext i32 %93 to i64
  %95 = mul i64 %94, %region_0_243_constant_46
  %96 = trunc i64 %95 to i32
  %97 = xor i32 %92, %96
  %region_0_243_constant_56 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %98 = xor i32 %97, %region_0_243_constant_56
  %99 = zext i32 %98 to i64
  %100 = mul i64 %99, %region_0_243_constant_18
  %101 = lshr i64 %100, %61
  %shft.chk7 = icmp ult i64 %61, 64
  %102 = select i1 %shft.chk7, i64 %101, i64 0
  %103 = trunc i64 %102 to i32
  %104 = lshr i64 %95, %61
  %shft.chk8 = icmp ult i64 %61, 64
  %105 = select i1 %shft.chk8, i64 %104, i64 0
  %106 = trunc i64 %105 to i32
  %107 = lshr i64 %57, %61
  %shft.chk9 = icmp ult i64 %61, 64
  %108 = select i1 %shft.chk9, i64 %107, i64 0
  %109 = trunc i64 %108 to i32
  %110 = xor i32 %106, %109
  %region_0_243_constant_68 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %111 = xor i32 %110, %region_0_243_constant_68
  %112 = zext i32 %111 to i64
  %113 = mul i64 %112, %region_0_243_constant_18
  %114 = trunc i64 %113 to i32
  %115 = xor i32 %103, %114
  %region_0_243_constant_75 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %116 = xor i32 %115, %region_0_243_constant_75
  %117 = zext i32 %116 to i64
  %118 = mul i64 %117, %region_0_243_constant_46
  %119 = lshr i64 %118, %61
  %shft.chk10 = icmp ult i64 %61, 64
  %120 = select i1 %shft.chk10, i64 %119, i64 0
  %121 = trunc i64 %120 to i32
  %122 = lshr i64 %113, %61
  %shft.chk11 = icmp ult i64 %61, 64
  %123 = select i1 %shft.chk11, i64 %122, i64 0
  %124 = trunc i64 %123 to i32
  %125 = trunc i64 %60 to i32
  %126 = xor i32 %124, %125
  %region_0_243_constant_86 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %127 = xor i32 %126, %region_0_243_constant_86
  %128 = zext i32 %127 to i64
  %129 = mul i64 %128, %region_0_243_constant_46
  %130 = trunc i64 %129 to i32
  %131 = xor i32 %121, %130
  %region_0_243_constant_93 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %132 = xor i32 %131, %region_0_243_constant_93
  %133 = zext i32 %132 to i64
  %134 = mul i64 %133, %region_0_243_constant_18
  %135 = lshr i64 %134, %61
  %shft.chk12 = icmp ult i64 %61, 64
  %136 = select i1 %shft.chk12, i64 %135, i64 0
  %137 = trunc i64 %136 to i32
  %138 = lshr i64 %129, %61
  %shft.chk13 = icmp ult i64 %61, 64
  %139 = select i1 %shft.chk13, i64 %138, i64 0
  %140 = trunc i64 %139 to i32
  %141 = trunc i64 %89 to i32
  %142 = xor i32 %140, %141
  %region_0_243_constant_104 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %143 = xor i32 %142, %region_0_243_constant_104
  %144 = zext i32 %143 to i64
  %145 = mul i64 %144, %region_0_243_constant_18
  %146 = trunc i64 %145 to i32
  %147 = xor i32 %137, %146
  %region_0_243_constant_111 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %148 = xor i32 %147, %region_0_243_constant_111
  %149 = zext i32 %148 to i64
  %150 = mul i64 %149, %region_0_243_constant_46
  %151 = lshr i64 %150, %61
  %shft.chk14 = icmp ult i64 %61, 64
  %152 = select i1 %shft.chk14, i64 %151, i64 0
  %153 = trunc i64 %152 to i32
  %154 = lshr i64 %145, %61
  %shft.chk15 = icmp ult i64 %61, 64
  %155 = select i1 %shft.chk15, i64 %154, i64 0
  %156 = trunc i64 %155 to i32
  %157 = trunc i64 %100 to i32
  %158 = xor i32 %156, %157
  %region_0_243_constant_122 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %159 = xor i32 %158, %region_0_243_constant_122
  %160 = zext i32 %159 to i64
  %161 = mul i64 %160, %region_0_243_constant_46
  %162 = trunc i64 %161 to i32
  %163 = xor i32 %153, %162
  %region_0_243_constant_129 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %164 = xor i32 %163, %region_0_243_constant_129
  %165 = zext i32 %164 to i64
  %166 = mul i64 %165, %region_0_243_constant_18
  %167 = lshr i64 %166, %61
  %shft.chk16 = icmp ult i64 %61, 64
  %168 = select i1 %shft.chk16, i64 %167, i64 0
  %169 = trunc i64 %168 to i32
  %170 = lshr i64 %161, %61
  %shft.chk17 = icmp ult i64 %61, 64
  %171 = select i1 %shft.chk17, i64 %170, i64 0
  %172 = trunc i64 %171 to i32
  %173 = trunc i64 %118 to i32
  %174 = xor i32 %172, %173
  %region_0_243_constant_140 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %175 = xor i32 %174, %region_0_243_constant_140
  %176 = zext i32 %175 to i64
  %177 = mul i64 %176, %region_0_243_constant_18
  %178 = trunc i64 %177 to i32
  %179 = xor i32 %169, %178
  %region_0_243_constant_147 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %180 = xor i32 %179, %region_0_243_constant_147
  %181 = zext i32 %180 to i64
  %182 = mul i64 %181, %region_0_243_constant_46
  %183 = lshr i64 %182, %61
  %shft.chk18 = icmp ult i64 %61, 64
  %184 = select i1 %shft.chk18, i64 %183, i64 0
  %185 = trunc i64 %184 to i32
  %186 = lshr i64 %177, %61
  %shft.chk19 = icmp ult i64 %61, 64
  %187 = select i1 %shft.chk19, i64 %186, i64 0
  %188 = trunc i64 %187 to i32
  %189 = trunc i64 %134 to i32
  %190 = xor i32 %188, %189
  %region_0_243_constant_158 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %191 = xor i32 %190, %region_0_243_constant_158
  %192 = zext i32 %191 to i64
  %193 = mul i64 %192, %region_0_243_constant_46
  %194 = trunc i64 %193 to i32
  %195 = xor i32 %185, %194
  %region_0_243_constant_165 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %196 = xor i32 %195, %region_0_243_constant_165
  %197 = zext i32 %196 to i64
  %198 = mul i64 %197, %region_0_243_constant_18
  %199 = lshr i64 %198, %61
  %shft.chk20 = icmp ult i64 %61, 64
  %200 = select i1 %shft.chk20, i64 %199, i64 0
  %201 = trunc i64 %200 to i32
  %202 = lshr i64 %193, %61
  %shft.chk21 = icmp ult i64 %61, 64
  %203 = select i1 %shft.chk21, i64 %202, i64 0
  %204 = trunc i64 %203 to i32
  %205 = trunc i64 %150 to i32
  %206 = xor i32 %204, %205
  %region_0_243_constant_176 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %207 = xor i32 %206, %region_0_243_constant_176
  %208 = zext i32 %207 to i64
  %209 = mul i64 %208, %region_0_243_constant_18
  %210 = trunc i64 %209 to i32
  %211 = xor i32 %201, %210
  %region_0_243_constant_183 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %212 = xor i32 %211, %region_0_243_constant_183
  %213 = zext i32 %212 to i64
  %214 = mul i64 %213, %region_0_243_constant_46
  %215 = lshr i64 %214, %61
  %shft.chk22 = icmp ult i64 %61, 64
  %216 = select i1 %shft.chk22, i64 %215, i64 0
  %217 = trunc i64 %216 to i32
  %218 = lshr i64 %209, %61
  %shft.chk23 = icmp ult i64 %61, 64
  %219 = select i1 %shft.chk23, i64 %218, i64 0
  %220 = trunc i64 %219 to i32
  %221 = trunc i64 %166 to i32
  %222 = xor i32 %220, %221
  %region_0_243_constant_194 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %223 = xor i32 %222, %region_0_243_constant_194
  %224 = zext i32 %223 to i64
  %225 = mul i64 %224, %region_0_243_constant_46
  %226 = trunc i64 %225 to i32
  %227 = xor i32 %217, %226
  %region_0_243_constant_201 = load i32, i32* bitcast ([4 x i8]* @26 to i32*), align 4
  %228 = xor i32 %227, %region_0_243_constant_201
  br label %concatenate.226.merge

concat_index_from_operand_id1:                    ; preds = %concatenate.pivot.1.149
  %229 = phi i32 [ 1, %concatenate.pivot.1.149 ]
  %230 = sub nsw i32 %36, %229
  %231 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %232 = load i64, i64* %231, align 8, !invariant.load !22
  %233 = trunc i64 %232 to i32
  %234 = zext i32 %233 to i64
  %235 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %236 = lshr i64 %232, %235
  %shft.chk24 = icmp ult i64 %235, 64
  %237 = select i1 %shft.chk24, i64 %236, i64 0
  %238 = trunc i64 %237 to i32
  %239 = zext i32 %238 to i64
  %240 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %241 = shl i64 %239, %240
  %shft.chk25 = icmp ult i64 %240, 64
  %242 = select i1 %shft.chk25, i64 %241, i64 0
  %243 = or i64 %234, %242
  %244 = mul nuw nsw i32 %37, 1
  %245 = add nuw nsw i32 0, %244
  %246 = zext i32 %245 to i64
  %247 = add i64 %243, %246
  %248 = trunc i64 %247 to i32
  %249 = zext i32 %248 to i64
  %region_0_243_constant_1826 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %250 = mul i64 %249, %region_0_243_constant_1826
  %251 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %252 = lshr i64 %250, %251
  %shft.chk27 = icmp ult i64 %251, 64
  %253 = select i1 %shft.chk27, i64 %252, i64 0
  %254 = trunc i64 %253 to i32
  %255 = icmp ult i64 %247, %243
  %256 = zext i1 %255 to i8
  %257 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %258 = load i64, i64* %257, align 8, !invariant.load !22
  %259 = trunc i64 %258 to i32
  %260 = zext i32 %259 to i64
  %261 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %262 = lshr i64 %258, %261
  %shft.chk28 = icmp ult i64 %261, 64
  %263 = select i1 %shft.chk28, i64 %262, i64 0
  %264 = trunc i64 %263 to i32
  %265 = zext i32 %264 to i64
  %266 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %267 = shl i64 %265, %266
  %shft.chk29 = icmp ult i64 %266, 64
  %268 = select i1 %shft.chk29, i64 %267, i64 0
  %269 = or i64 %260, %268
  %region_0_243_constant_3430 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %270 = add i64 %269, %region_0_243_constant_3430
  %271 = trunc i8 %256 to i1
  %272 = select i1 %271, i64 %270, i64 %269
  %273 = lshr i64 %272, %251
  %shft.chk31 = icmp ult i64 %251, 64
  %274 = select i1 %shft.chk31, i64 %273, i64 0
  %275 = trunc i64 %274 to i32
  %276 = xor i32 %254, %275
  %region_0_243_constant_4232 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %277 = xor i32 %276, %region_0_243_constant_4232
  %278 = zext i32 %277 to i64
  %region_0_243_constant_4633 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %279 = mul i64 %278, %region_0_243_constant_4633
  %280 = lshr i64 %279, %251
  %shft.chk34 = icmp ult i64 %251, 64
  %281 = select i1 %shft.chk34, i64 %280, i64 0
  %282 = trunc i64 %281 to i32
  %283 = trunc i64 %272 to i32
  %284 = zext i32 %283 to i64
  %285 = mul i64 %284, %region_0_243_constant_4633
  %286 = trunc i64 %285 to i32
  %287 = xor i32 %282, %286
  %region_0_243_constant_5635 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %288 = xor i32 %287, %region_0_243_constant_5635
  %289 = zext i32 %288 to i64
  %290 = mul i64 %289, %region_0_243_constant_1826
  %291 = lshr i64 %290, %251
  %shft.chk36 = icmp ult i64 %251, 64
  %292 = select i1 %shft.chk36, i64 %291, i64 0
  %293 = trunc i64 %292 to i32
  %294 = lshr i64 %285, %251
  %shft.chk37 = icmp ult i64 %251, 64
  %295 = select i1 %shft.chk37, i64 %294, i64 0
  %296 = trunc i64 %295 to i32
  %297 = lshr i64 %247, %251
  %shft.chk38 = icmp ult i64 %251, 64
  %298 = select i1 %shft.chk38, i64 %297, i64 0
  %299 = trunc i64 %298 to i32
  %300 = xor i32 %296, %299
  %region_0_243_constant_6839 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %301 = xor i32 %300, %region_0_243_constant_6839
  %302 = zext i32 %301 to i64
  %303 = mul i64 %302, %region_0_243_constant_1826
  %304 = trunc i64 %303 to i32
  %305 = xor i32 %293, %304
  %region_0_243_constant_7540 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %306 = xor i32 %305, %region_0_243_constant_7540
  %307 = zext i32 %306 to i64
  %308 = mul i64 %307, %region_0_243_constant_4633
  %309 = lshr i64 %308, %251
  %shft.chk41 = icmp ult i64 %251, 64
  %310 = select i1 %shft.chk41, i64 %309, i64 0
  %311 = trunc i64 %310 to i32
  %312 = lshr i64 %303, %251
  %shft.chk42 = icmp ult i64 %251, 64
  %313 = select i1 %shft.chk42, i64 %312, i64 0
  %314 = trunc i64 %313 to i32
  %315 = trunc i64 %250 to i32
  %316 = xor i32 %314, %315
  %region_0_243_constant_8643 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %317 = xor i32 %316, %region_0_243_constant_8643
  %318 = zext i32 %317 to i64
  %319 = mul i64 %318, %region_0_243_constant_4633
  %320 = trunc i64 %319 to i32
  %321 = xor i32 %311, %320
  %region_0_243_constant_9344 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %322 = xor i32 %321, %region_0_243_constant_9344
  %323 = zext i32 %322 to i64
  %324 = mul i64 %323, %region_0_243_constant_1826
  %325 = lshr i64 %324, %251
  %shft.chk45 = icmp ult i64 %251, 64
  %326 = select i1 %shft.chk45, i64 %325, i64 0
  %327 = trunc i64 %326 to i32
  %328 = lshr i64 %319, %251
  %shft.chk46 = icmp ult i64 %251, 64
  %329 = select i1 %shft.chk46, i64 %328, i64 0
  %330 = trunc i64 %329 to i32
  %331 = trunc i64 %279 to i32
  %332 = xor i32 %330, %331
  %region_0_243_constant_10447 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %333 = xor i32 %332, %region_0_243_constant_10447
  %334 = zext i32 %333 to i64
  %335 = mul i64 %334, %region_0_243_constant_1826
  %336 = trunc i64 %335 to i32
  %337 = xor i32 %327, %336
  %region_0_243_constant_11148 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %338 = xor i32 %337, %region_0_243_constant_11148
  %339 = zext i32 %338 to i64
  %340 = mul i64 %339, %region_0_243_constant_4633
  %341 = lshr i64 %340, %251
  %shft.chk49 = icmp ult i64 %251, 64
  %342 = select i1 %shft.chk49, i64 %341, i64 0
  %343 = trunc i64 %342 to i32
  %344 = lshr i64 %335, %251
  %shft.chk50 = icmp ult i64 %251, 64
  %345 = select i1 %shft.chk50, i64 %344, i64 0
  %346 = trunc i64 %345 to i32
  %347 = trunc i64 %290 to i32
  %348 = xor i32 %346, %347
  %region_0_243_constant_12251 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %349 = xor i32 %348, %region_0_243_constant_12251
  %350 = zext i32 %349 to i64
  %351 = mul i64 %350, %region_0_243_constant_4633
  %352 = trunc i64 %351 to i32
  %353 = xor i32 %343, %352
  %region_0_243_constant_12952 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %354 = xor i32 %353, %region_0_243_constant_12952
  %355 = zext i32 %354 to i64
  %356 = mul i64 %355, %region_0_243_constant_1826
  %357 = lshr i64 %356, %251
  %shft.chk53 = icmp ult i64 %251, 64
  %358 = select i1 %shft.chk53, i64 %357, i64 0
  %359 = trunc i64 %358 to i32
  %360 = lshr i64 %351, %251
  %shft.chk54 = icmp ult i64 %251, 64
  %361 = select i1 %shft.chk54, i64 %360, i64 0
  %362 = trunc i64 %361 to i32
  %363 = trunc i64 %308 to i32
  %364 = xor i32 %362, %363
  %region_0_243_constant_14055 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %365 = xor i32 %364, %region_0_243_constant_14055
  %366 = zext i32 %365 to i64
  %367 = mul i64 %366, %region_0_243_constant_1826
  %368 = trunc i64 %367 to i32
  %369 = xor i32 %359, %368
  %region_0_243_constant_14756 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %370 = xor i32 %369, %region_0_243_constant_14756
  %371 = zext i32 %370 to i64
  %372 = mul i64 %371, %region_0_243_constant_4633
  %373 = lshr i64 %372, %251
  %shft.chk57 = icmp ult i64 %251, 64
  %374 = select i1 %shft.chk57, i64 %373, i64 0
  %375 = trunc i64 %374 to i32
  %376 = lshr i64 %367, %251
  %shft.chk58 = icmp ult i64 %251, 64
  %377 = select i1 %shft.chk58, i64 %376, i64 0
  %378 = trunc i64 %377 to i32
  %379 = trunc i64 %324 to i32
  %380 = xor i32 %378, %379
  %region_0_243_constant_15859 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %381 = xor i32 %380, %region_0_243_constant_15859
  %382 = zext i32 %381 to i64
  %383 = mul i64 %382, %region_0_243_constant_4633
  %384 = trunc i64 %383 to i32
  %385 = xor i32 %375, %384
  %region_0_243_constant_16560 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %386 = xor i32 %385, %region_0_243_constant_16560
  %387 = zext i32 %386 to i64
  %388 = mul i64 %387, %region_0_243_constant_1826
  %389 = lshr i64 %388, %251
  %shft.chk61 = icmp ult i64 %251, 64
  %390 = select i1 %shft.chk61, i64 %389, i64 0
  %391 = trunc i64 %390 to i32
  %392 = lshr i64 %383, %251
  %shft.chk62 = icmp ult i64 %251, 64
  %393 = select i1 %shft.chk62, i64 %392, i64 0
  %394 = trunc i64 %393 to i32
  %395 = trunc i64 %340 to i32
  %396 = xor i32 %394, %395
  %region_0_243_constant_17663 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %397 = xor i32 %396, %region_0_243_constant_17663
  %398 = zext i32 %397 to i64
  %399 = mul i64 %398, %region_0_243_constant_1826
  %400 = trunc i64 %399 to i32
  %401 = xor i32 %391, %400
  %region_0_243_constant_18364 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %402 = xor i32 %401, %region_0_243_constant_18364
  %403 = zext i32 %402 to i64
  %404 = mul i64 %403, %region_0_243_constant_4633
  %405 = trunc i64 %404 to i32
  br label %concatenate.226.merge

concat_index_from_operand_id2:                    ; preds = %concatenate.pivot.2.150
  %406 = phi i32 [ 2, %concatenate.pivot.2.150 ]
  %407 = sub nsw i32 %36, %406
  %408 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %409 = load i64, i64* %408, align 8, !invariant.load !22
  %410 = trunc i64 %409 to i32
  %411 = zext i32 %410 to i64
  %412 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %413 = lshr i64 %409, %412
  %shft.chk65 = icmp ult i64 %412, 64
  %414 = select i1 %shft.chk65, i64 %413, i64 0
  %415 = trunc i64 %414 to i32
  %416 = zext i32 %415 to i64
  %417 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %418 = shl i64 %416, %417
  %shft.chk66 = icmp ult i64 %417, 64
  %419 = select i1 %shft.chk66, i64 %418, i64 0
  %420 = or i64 %411, %419
  %421 = mul nuw nsw i32 %37, 1
  %422 = add nuw nsw i32 0, %421
  %423 = zext i32 %422 to i64
  %424 = add i64 %420, %423
  %425 = icmp ult i64 %424, %420
  %426 = zext i1 %425 to i8
  %427 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %428 = load i64, i64* %427, align 8, !invariant.load !22
  %429 = trunc i64 %428 to i32
  %430 = zext i32 %429 to i64
  %431 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %432 = lshr i64 %428, %431
  %shft.chk67 = icmp ult i64 %431, 64
  %433 = select i1 %shft.chk67, i64 %432, i64 0
  %434 = trunc i64 %433 to i32
  %435 = zext i32 %434 to i64
  %436 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %437 = shl i64 %435, %436
  %shft.chk68 = icmp ult i64 %436, 64
  %438 = select i1 %shft.chk68, i64 %437, i64 0
  %439 = or i64 %430, %438
  %region_0_243_constant_3469 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %440 = add i64 %439, %region_0_243_constant_3469
  %441 = trunc i8 %426 to i1
  %442 = select i1 %441, i64 %440, i64 %439
  %443 = trunc i64 %442 to i32
  %444 = zext i32 %443 to i64
  %region_0_243_constant_4670 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %445 = mul i64 %444, %region_0_243_constant_4670
  %446 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %447 = lshr i64 %445, %446
  %shft.chk71 = icmp ult i64 %446, 64
  %448 = select i1 %shft.chk71, i64 %447, i64 0
  %449 = trunc i64 %448 to i32
  %450 = lshr i64 %424, %446
  %shft.chk72 = icmp ult i64 %446, 64
  %451 = select i1 %shft.chk72, i64 %450, i64 0
  %452 = trunc i64 %451 to i32
  %453 = xor i32 %449, %452
  %region_0_243_constant_6873 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %454 = xor i32 %453, %region_0_243_constant_6873
  %455 = zext i32 %454 to i64
  %region_0_243_constant_1874 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %456 = mul i64 %455, %region_0_243_constant_1874
  %457 = lshr i64 %456, %446
  %shft.chk75 = icmp ult i64 %446, 64
  %458 = select i1 %shft.chk75, i64 %457, i64 0
  %459 = trunc i64 %458 to i32
  %460 = trunc i64 %424 to i32
  %461 = zext i32 %460 to i64
  %462 = mul i64 %461, %region_0_243_constant_1874
  %463 = trunc i64 %462 to i32
  %464 = xor i32 %459, %463
  %region_0_243_constant_8676 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %465 = xor i32 %464, %region_0_243_constant_8676
  %466 = zext i32 %465 to i64
  %467 = mul i64 %466, %region_0_243_constant_4670
  %468 = lshr i64 %467, %446
  %shft.chk77 = icmp ult i64 %446, 64
  %469 = select i1 %shft.chk77, i64 %468, i64 0
  %470 = trunc i64 %469 to i32
  %471 = lshr i64 %462, %446
  %shft.chk78 = icmp ult i64 %446, 64
  %472 = select i1 %shft.chk78, i64 %471, i64 0
  %473 = trunc i64 %472 to i32
  %474 = lshr i64 %442, %446
  %shft.chk79 = icmp ult i64 %446, 64
  %475 = select i1 %shft.chk79, i64 %474, i64 0
  %476 = trunc i64 %475 to i32
  %477 = xor i32 %473, %476
  %region_0_243_constant_4280 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %478 = xor i32 %477, %region_0_243_constant_4280
  %479 = zext i32 %478 to i64
  %480 = mul i64 %479, %region_0_243_constant_4670
  %481 = trunc i64 %480 to i32
  %482 = xor i32 %470, %481
  %region_0_243_constant_10481 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %483 = xor i32 %482, %region_0_243_constant_10481
  %484 = zext i32 %483 to i64
  %485 = mul i64 %484, %region_0_243_constant_1874
  %486 = lshr i64 %485, %446
  %shft.chk82 = icmp ult i64 %446, 64
  %487 = select i1 %shft.chk82, i64 %486, i64 0
  %488 = trunc i64 %487 to i32
  %489 = lshr i64 %480, %446
  %shft.chk83 = icmp ult i64 %446, 64
  %490 = select i1 %shft.chk83, i64 %489, i64 0
  %491 = trunc i64 %490 to i32
  %492 = trunc i64 %445 to i32
  %493 = xor i32 %491, %492
  %region_0_243_constant_5684 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %494 = xor i32 %493, %region_0_243_constant_5684
  %495 = zext i32 %494 to i64
  %496 = mul i64 %495, %region_0_243_constant_1874
  %497 = trunc i64 %496 to i32
  %498 = xor i32 %488, %497
  %region_0_243_constant_12285 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %499 = xor i32 %498, %region_0_243_constant_12285
  %500 = zext i32 %499 to i64
  %501 = mul i64 %500, %region_0_243_constant_4670
  %502 = lshr i64 %501, %446
  %shft.chk86 = icmp ult i64 %446, 64
  %503 = select i1 %shft.chk86, i64 %502, i64 0
  %504 = trunc i64 %503 to i32
  %505 = lshr i64 %496, %446
  %shft.chk87 = icmp ult i64 %446, 64
  %506 = select i1 %shft.chk87, i64 %505, i64 0
  %507 = trunc i64 %506 to i32
  %508 = trunc i64 %456 to i32
  %509 = xor i32 %507, %508
  %region_0_243_constant_7588 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %510 = xor i32 %509, %region_0_243_constant_7588
  %511 = zext i32 %510 to i64
  %512 = mul i64 %511, %region_0_243_constant_4670
  %513 = trunc i64 %512 to i32
  %514 = xor i32 %504, %513
  %region_0_243_constant_14089 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %515 = xor i32 %514, %region_0_243_constant_14089
  %516 = zext i32 %515 to i64
  %517 = mul i64 %516, %region_0_243_constant_1874
  %518 = lshr i64 %517, %446
  %shft.chk90 = icmp ult i64 %446, 64
  %519 = select i1 %shft.chk90, i64 %518, i64 0
  %520 = trunc i64 %519 to i32
  %521 = lshr i64 %512, %446
  %shft.chk91 = icmp ult i64 %446, 64
  %522 = select i1 %shft.chk91, i64 %521, i64 0
  %523 = trunc i64 %522 to i32
  %524 = trunc i64 %467 to i32
  %525 = xor i32 %523, %524
  %region_0_243_constant_9392 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %526 = xor i32 %525, %region_0_243_constant_9392
  %527 = zext i32 %526 to i64
  %528 = mul i64 %527, %region_0_243_constant_1874
  %529 = trunc i64 %528 to i32
  %530 = xor i32 %520, %529
  %region_0_243_constant_15893 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %531 = xor i32 %530, %region_0_243_constant_15893
  %532 = zext i32 %531 to i64
  %533 = mul i64 %532, %region_0_243_constant_4670
  %534 = lshr i64 %533, %446
  %shft.chk94 = icmp ult i64 %446, 64
  %535 = select i1 %shft.chk94, i64 %534, i64 0
  %536 = trunc i64 %535 to i32
  %537 = lshr i64 %528, %446
  %shft.chk95 = icmp ult i64 %446, 64
  %538 = select i1 %shft.chk95, i64 %537, i64 0
  %539 = trunc i64 %538 to i32
  %540 = trunc i64 %485 to i32
  %541 = xor i32 %539, %540
  %region_0_243_constant_11196 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %542 = xor i32 %541, %region_0_243_constant_11196
  %543 = zext i32 %542 to i64
  %544 = mul i64 %543, %region_0_243_constant_4670
  %545 = trunc i64 %544 to i32
  %546 = xor i32 %536, %545
  %region_0_243_constant_17697 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %547 = xor i32 %546, %region_0_243_constant_17697
  %548 = zext i32 %547 to i64
  %549 = mul i64 %548, %region_0_243_constant_1874
  %550 = lshr i64 %549, %446
  %shft.chk98 = icmp ult i64 %446, 64
  %551 = select i1 %shft.chk98, i64 %550, i64 0
  %552 = trunc i64 %551 to i32
  %553 = lshr i64 %544, %446
  %shft.chk99 = icmp ult i64 %446, 64
  %554 = select i1 %shft.chk99, i64 %553, i64 0
  %555 = trunc i64 %554 to i32
  %556 = trunc i64 %501 to i32
  %557 = xor i32 %555, %556
  %region_0_243_constant_129100 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %558 = xor i32 %557, %region_0_243_constant_129100
  %559 = zext i32 %558 to i64
  %560 = mul i64 %559, %region_0_243_constant_1874
  %561 = trunc i64 %560 to i32
  %562 = xor i32 %552, %561
  %region_0_243_constant_194101 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %563 = xor i32 %562, %region_0_243_constant_194101
  %564 = zext i32 %563 to i64
  %565 = mul i64 %564, %region_0_243_constant_4670
  %566 = lshr i64 %565, %446
  %shft.chk102 = icmp ult i64 %446, 64
  %567 = select i1 %shft.chk102, i64 %566, i64 0
  %568 = trunc i64 %567 to i32
  %569 = lshr i64 %560, %446
  %shft.chk103 = icmp ult i64 %446, 64
  %570 = select i1 %shft.chk103, i64 %569, i64 0
  %571 = trunc i64 %570 to i32
  %572 = trunc i64 %517 to i32
  %573 = xor i32 %571, %572
  %region_0_243_constant_147104 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %574 = xor i32 %573, %region_0_243_constant_147104
  %575 = zext i32 %574 to i64
  %576 = mul i64 %575, %region_0_243_constant_4670
  %577 = trunc i64 %576 to i32
  %578 = xor i32 %568, %577
  %region_0_243_constant_211 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %579 = xor i32 %578, %region_0_243_constant_211
  %580 = zext i32 %579 to i64
  %581 = mul i64 %580, %region_0_243_constant_1874
  %582 = lshr i64 %581, %446
  %shft.chk105 = icmp ult i64 %446, 64
  %583 = select i1 %shft.chk105, i64 %582, i64 0
  %584 = trunc i64 %583 to i32
  %585 = lshr i64 %576, %446
  %shft.chk106 = icmp ult i64 %446, 64
  %586 = select i1 %shft.chk106, i64 %585, i64 0
  %587 = trunc i64 %586 to i32
  %588 = trunc i64 %533 to i32
  %589 = xor i32 %587, %588
  %region_0_243_constant_165107 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %590 = xor i32 %589, %region_0_243_constant_165107
  %591 = zext i32 %590 to i64
  %592 = mul i64 %591, %region_0_243_constant_1874
  %593 = trunc i64 %592 to i32
  %594 = xor i32 %584, %593
  %region_0_243_constant_220 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %595 = xor i32 %594, %region_0_243_constant_220
  br label %concatenate.226.merge

concat_index_from_operand_id3:                    ; preds = %concatenate.pivot.3.151
  %596 = phi i32 [ 3, %concatenate.pivot.3.151 ]
  %597 = sub nsw i32 %36, %596
  %598 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %599 = load i64, i64* %598, align 8, !invariant.load !22
  %600 = trunc i64 %599 to i32
  %601 = zext i32 %600 to i64
  %602 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %603 = lshr i64 %599, %602
  %shft.chk108 = icmp ult i64 %602, 64
  %604 = select i1 %shft.chk108, i64 %603, i64 0
  %605 = trunc i64 %604 to i32
  %606 = zext i32 %605 to i64
  %607 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %608 = shl i64 %606, %607
  %shft.chk109 = icmp ult i64 %607, 64
  %609 = select i1 %shft.chk109, i64 %608, i64 0
  %610 = or i64 %601, %609
  %611 = mul nuw nsw i32 %37, 1
  %612 = add nuw nsw i32 0, %611
  %613 = zext i32 %612 to i64
  %614 = add i64 %610, %613
  %615 = icmp ult i64 %614, %610
  %616 = zext i1 %615 to i8
  %617 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %618 = load i64, i64* %617, align 8, !invariant.load !22
  %619 = trunc i64 %618 to i32
  %620 = zext i32 %619 to i64
  %621 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %622 = lshr i64 %618, %621
  %shft.chk110 = icmp ult i64 %621, 64
  %623 = select i1 %shft.chk110, i64 %622, i64 0
  %624 = trunc i64 %623 to i32
  %625 = zext i32 %624 to i64
  %626 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %627 = shl i64 %625, %626
  %shft.chk111 = icmp ult i64 %626, 64
  %628 = select i1 %shft.chk111, i64 %627, i64 0
  %629 = or i64 %620, %628
  %region_0_243_constant_34112 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %630 = add i64 %629, %region_0_243_constant_34112
  %631 = trunc i8 %616 to i1
  %632 = select i1 %631, i64 %630, i64 %629
  %633 = trunc i64 %632 to i32
  %634 = zext i32 %633 to i64
  %region_0_243_constant_46113 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %635 = mul i64 %634, %region_0_243_constant_46113
  %636 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %637 = lshr i64 %635, %636
  %shft.chk114 = icmp ult i64 %636, 64
  %638 = select i1 %shft.chk114, i64 %637, i64 0
  %639 = trunc i64 %638 to i32
  %640 = lshr i64 %614, %636
  %shft.chk115 = icmp ult i64 %636, 64
  %641 = select i1 %shft.chk115, i64 %640, i64 0
  %642 = trunc i64 %641 to i32
  %643 = xor i32 %639, %642
  %region_0_243_constant_68116 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %644 = xor i32 %643, %region_0_243_constant_68116
  %645 = zext i32 %644 to i64
  %region_0_243_constant_18117 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %646 = mul i64 %645, %region_0_243_constant_18117
  %647 = lshr i64 %646, %636
  %shft.chk118 = icmp ult i64 %636, 64
  %648 = select i1 %shft.chk118, i64 %647, i64 0
  %649 = trunc i64 %648 to i32
  %650 = trunc i64 %614 to i32
  %651 = zext i32 %650 to i64
  %652 = mul i64 %651, %region_0_243_constant_18117
  %653 = trunc i64 %652 to i32
  %654 = xor i32 %649, %653
  %region_0_243_constant_86119 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %655 = xor i32 %654, %region_0_243_constant_86119
  %656 = zext i32 %655 to i64
  %657 = mul i64 %656, %region_0_243_constant_46113
  %658 = lshr i64 %657, %636
  %shft.chk120 = icmp ult i64 %636, 64
  %659 = select i1 %shft.chk120, i64 %658, i64 0
  %660 = trunc i64 %659 to i32
  %661 = lshr i64 %652, %636
  %shft.chk121 = icmp ult i64 %636, 64
  %662 = select i1 %shft.chk121, i64 %661, i64 0
  %663 = trunc i64 %662 to i32
  %664 = lshr i64 %632, %636
  %shft.chk122 = icmp ult i64 %636, 64
  %665 = select i1 %shft.chk122, i64 %664, i64 0
  %666 = trunc i64 %665 to i32
  %667 = xor i32 %663, %666
  %region_0_243_constant_42123 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %668 = xor i32 %667, %region_0_243_constant_42123
  %669 = zext i32 %668 to i64
  %670 = mul i64 %669, %region_0_243_constant_46113
  %671 = trunc i64 %670 to i32
  %672 = xor i32 %660, %671
  %region_0_243_constant_104124 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %673 = xor i32 %672, %region_0_243_constant_104124
  %674 = zext i32 %673 to i64
  %675 = mul i64 %674, %region_0_243_constant_18117
  %676 = lshr i64 %675, %636
  %shft.chk125 = icmp ult i64 %636, 64
  %677 = select i1 %shft.chk125, i64 %676, i64 0
  %678 = trunc i64 %677 to i32
  %679 = lshr i64 %670, %636
  %shft.chk126 = icmp ult i64 %636, 64
  %680 = select i1 %shft.chk126, i64 %679, i64 0
  %681 = trunc i64 %680 to i32
  %682 = trunc i64 %635 to i32
  %683 = xor i32 %681, %682
  %region_0_243_constant_56127 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %684 = xor i32 %683, %region_0_243_constant_56127
  %685 = zext i32 %684 to i64
  %686 = mul i64 %685, %region_0_243_constant_18117
  %687 = trunc i64 %686 to i32
  %688 = xor i32 %678, %687
  %region_0_243_constant_122128 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %689 = xor i32 %688, %region_0_243_constant_122128
  %690 = zext i32 %689 to i64
  %691 = mul i64 %690, %region_0_243_constant_46113
  %692 = lshr i64 %691, %636
  %shft.chk129 = icmp ult i64 %636, 64
  %693 = select i1 %shft.chk129, i64 %692, i64 0
  %694 = trunc i64 %693 to i32
  %695 = lshr i64 %686, %636
  %shft.chk130 = icmp ult i64 %636, 64
  %696 = select i1 %shft.chk130, i64 %695, i64 0
  %697 = trunc i64 %696 to i32
  %698 = trunc i64 %646 to i32
  %699 = xor i32 %697, %698
  %region_0_243_constant_75131 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %700 = xor i32 %699, %region_0_243_constant_75131
  %701 = zext i32 %700 to i64
  %702 = mul i64 %701, %region_0_243_constant_46113
  %703 = trunc i64 %702 to i32
  %704 = xor i32 %694, %703
  %region_0_243_constant_140132 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %705 = xor i32 %704, %region_0_243_constant_140132
  %706 = zext i32 %705 to i64
  %707 = mul i64 %706, %region_0_243_constant_18117
  %708 = lshr i64 %707, %636
  %shft.chk133 = icmp ult i64 %636, 64
  %709 = select i1 %shft.chk133, i64 %708, i64 0
  %710 = trunc i64 %709 to i32
  %711 = lshr i64 %702, %636
  %shft.chk134 = icmp ult i64 %636, 64
  %712 = select i1 %shft.chk134, i64 %711, i64 0
  %713 = trunc i64 %712 to i32
  %714 = trunc i64 %657 to i32
  %715 = xor i32 %713, %714
  %region_0_243_constant_93135 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %716 = xor i32 %715, %region_0_243_constant_93135
  %717 = zext i32 %716 to i64
  %718 = mul i64 %717, %region_0_243_constant_18117
  %719 = trunc i64 %718 to i32
  %720 = xor i32 %710, %719
  %region_0_243_constant_158136 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %721 = xor i32 %720, %region_0_243_constant_158136
  %722 = zext i32 %721 to i64
  %723 = mul i64 %722, %region_0_243_constant_46113
  %724 = lshr i64 %723, %636
  %shft.chk137 = icmp ult i64 %636, 64
  %725 = select i1 %shft.chk137, i64 %724, i64 0
  %726 = trunc i64 %725 to i32
  %727 = lshr i64 %718, %636
  %shft.chk138 = icmp ult i64 %636, 64
  %728 = select i1 %shft.chk138, i64 %727, i64 0
  %729 = trunc i64 %728 to i32
  %730 = trunc i64 %675 to i32
  %731 = xor i32 %729, %730
  %region_0_243_constant_111139 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %732 = xor i32 %731, %region_0_243_constant_111139
  %733 = zext i32 %732 to i64
  %734 = mul i64 %733, %region_0_243_constant_46113
  %735 = trunc i64 %734 to i32
  %736 = xor i32 %726, %735
  %region_0_243_constant_176140 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %737 = xor i32 %736, %region_0_243_constant_176140
  %738 = zext i32 %737 to i64
  %739 = mul i64 %738, %region_0_243_constant_18117
  %740 = lshr i64 %739, %636
  %shft.chk141 = icmp ult i64 %636, 64
  %741 = select i1 %shft.chk141, i64 %740, i64 0
  %742 = trunc i64 %741 to i32
  %743 = lshr i64 %734, %636
  %shft.chk142 = icmp ult i64 %636, 64
  %744 = select i1 %shft.chk142, i64 %743, i64 0
  %745 = trunc i64 %744 to i32
  %746 = trunc i64 %691 to i32
  %747 = xor i32 %745, %746
  %region_0_243_constant_129143 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %748 = xor i32 %747, %region_0_243_constant_129143
  %749 = zext i32 %748 to i64
  %750 = mul i64 %749, %region_0_243_constant_18117
  %751 = trunc i64 %750 to i32
  %752 = xor i32 %742, %751
  %region_0_243_constant_194144 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %753 = xor i32 %752, %region_0_243_constant_194144
  %754 = zext i32 %753 to i64
  %755 = mul i64 %754, %region_0_243_constant_46113
  %756 = lshr i64 %755, %636
  %shft.chk145 = icmp ult i64 %636, 64
  %757 = select i1 %shft.chk145, i64 %756, i64 0
  %758 = trunc i64 %757 to i32
  %759 = lshr i64 %750, %636
  %shft.chk146 = icmp ult i64 %636, 64
  %760 = select i1 %shft.chk146, i64 %759, i64 0
  %761 = trunc i64 %760 to i32
  %762 = trunc i64 %707 to i32
  %763 = xor i32 %761, %762
  %region_0_243_constant_147147 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %764 = xor i32 %763, %region_0_243_constant_147147
  %765 = zext i32 %764 to i64
  %766 = mul i64 %765, %region_0_243_constant_46113
  %767 = trunc i64 %766 to i32
  %768 = xor i32 %758, %767
  %region_0_243_constant_211148 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %769 = xor i32 %768, %region_0_243_constant_211148
  %770 = zext i32 %769 to i64
  %771 = mul i64 %770, %region_0_243_constant_18117
  %772 = trunc i64 %771 to i32
  br label %concatenate.226.merge

concatenate.pivot.2.:                             ; preds = %fusion_5.in_bounds-true
  %773 = icmp ult i32 %36, 2
  br i1 %773, label %concatenate.pivot.1., label %concatenate.pivot.3.

concatenate.pivot.1.:                             ; preds = %concatenate.pivot.2.
  %774 = icmp ult i32 %36, 1
  br i1 %774, label %concatenate.pivot.0., label %concatenate.pivot.1.149

concatenate.pivot.0.:                             ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id0

concatenate.pivot.1.149:                          ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id1

concatenate.pivot.3.:                             ; preds = %concatenate.pivot.2.
  %775 = icmp ult i32 %36, 3
  br i1 %775, label %concatenate.pivot.2.150, label %concatenate.pivot.3.151

concatenate.pivot.2.150:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id2

concatenate.pivot.3.151:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id3

concatenate.226.merge:                            ; preds = %concat_index_from_operand_id3, %concat_index_from_operand_id2, %concat_index_from_operand_id1, %concat_index_from_operand_id0
  %776 = phi i32 [ %228, %concat_index_from_operand_id0 ], [ %405, %concat_index_from_operand_id1 ], [ %595, %concat_index_from_operand_id2 ], [ %772, %concat_index_from_operand_id3 ]
  %region_0_243_constant_227 = load i32, i32* bitcast ([4 x i8]* @3 to i32*), align 4
  %777 = lshr i32 %776, %region_0_243_constant_227
  %shft.chk152 = icmp ult i32 %region_0_243_constant_227, 32
  %778 = select i1 %shft.chk152, i32 %777, i32 0
  %779 = uitofp i32 %778 to float
  %region_0_243_constant_231 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %multiply.233 = fmul float %779, %region_0_243_constant_231
  %region_0_243_constant_234 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %compare.236 = fcmp olt float %multiply.233, %region_0_243_constant_234
  %780 = zext i1 %compare.236 to i8
  %781 = mul nuw nsw i32 %23, 1
  %782 = add nuw nsw i32 0, %781
  %783 = udiv i32 %782, 16
  %784 = mul nuw nsw i32 %27, 1
  %785 = add nuw nsw i32 0, %784
  %786 = mul nuw nsw i32 %28, 32
  %787 = add nuw nsw i32 %785, %786
  %788 = udiv i32 %787, 256
  %789 = bitcast [256 x [16 x float]]* %1 to float*
  %790 = getelementptr inbounds float, float* %789, i32 %linear_index_base
  %791 = load float, float* %790, align 4, !invariant.load !22
  %region_0_243_constant_239 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %792 = trunc i8 %780 to i1
  %793 = select i1 %792, float %791, float %region_0_243_constant_239
  %794 = bitcast [256 x [16 x float]]* %5 to float*
  %795 = getelementptr inbounds float, float* %794, i32 %linear_index_base
  store float %793, float* %795, align 4
  %796 = mul nuw nsw i32 %13, 1
  %797 = add nuw nsw i32 0, %796
  %798 = udiv i32 %797, 16
  %799 = mul nuw nsw i32 %14, 1
  %800 = add nuw nsw i32 0, %799
  %801 = urem i32 %800, 32
  %802 = udiv i32 %800, 32
  %803 = udiv i32 %802, 8
  %804 = mul nuw nsw i32 %797, 1
  %805 = add nuw nsw i32 0, %804
  %806 = mul nuw nsw i32 %801, 16
  %807 = add nuw nsw i32 %805, %806
  %808 = mul nuw nsw i32 %802, 512
  %809 = add nuw nsw i32 %807, %808
  %810 = urem i32 %809, 4
  %811 = udiv i32 %809, 4
  %812 = udiv i32 %811, 1024
  br label %concatenate.pivot.2.330

concat_index_from_operand_id0154:                 ; preds = %concatenate.pivot.0.332
  %813 = phi i32 [ 0, %concatenate.pivot.0.332 ]
  %814 = sub nsw i32 %810, %813
  %815 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %816 = load i64, i64* %815, align 8, !invariant.load !22
  %817 = trunc i64 %816 to i32
  %818 = zext i32 %817 to i64
  %819 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %820 = lshr i64 %816, %819
  %shft.chk155 = icmp ult i64 %819, 64
  %821 = select i1 %shft.chk155, i64 %820, i64 0
  %822 = trunc i64 %821 to i32
  %823 = zext i32 %822 to i64
  %824 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %825 = shl i64 %823, %824
  %shft.chk156 = icmp ult i64 %824, 64
  %826 = select i1 %shft.chk156, i64 %825, i64 0
  %827 = or i64 %818, %826
  %828 = mul nuw nsw i32 %811, 1
  %829 = add nuw nsw i32 0, %828
  %830 = zext i32 %829 to i64
  %831 = add i64 %827, %830
  %832 = trunc i64 %831 to i32
  %833 = zext i32 %832 to i64
  %region_0_243_constant_18157 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %834 = mul i64 %833, %region_0_243_constant_18157
  %835 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %836 = lshr i64 %834, %835
  %shft.chk158 = icmp ult i64 %835, 64
  %837 = select i1 %shft.chk158, i64 %836, i64 0
  %838 = trunc i64 %837 to i32
  %839 = icmp ult i64 %831, %827
  %840 = zext i1 %839 to i8
  %841 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %842 = load i64, i64* %841, align 8, !invariant.load !22
  %843 = trunc i64 %842 to i32
  %844 = zext i32 %843 to i64
  %845 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %846 = lshr i64 %842, %845
  %shft.chk159 = icmp ult i64 %845, 64
  %847 = select i1 %shft.chk159, i64 %846, i64 0
  %848 = trunc i64 %847 to i32
  %849 = zext i32 %848 to i64
  %850 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %851 = shl i64 %849, %850
  %shft.chk160 = icmp ult i64 %850, 64
  %852 = select i1 %shft.chk160, i64 %851, i64 0
  %853 = or i64 %844, %852
  %region_0_243_constant_34161 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %854 = add i64 %853, %region_0_243_constant_34161
  %855 = trunc i8 %840 to i1
  %856 = select i1 %855, i64 %854, i64 %853
  %857 = lshr i64 %856, %835
  %shft.chk162 = icmp ult i64 %835, 64
  %858 = select i1 %shft.chk162, i64 %857, i64 0
  %859 = trunc i64 %858 to i32
  %860 = xor i32 %838, %859
  %region_0_243_constant_42163 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %861 = xor i32 %860, %region_0_243_constant_42163
  %862 = zext i32 %861 to i64
  %region_0_243_constant_46164 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %863 = mul i64 %862, %region_0_243_constant_46164
  %864 = lshr i64 %863, %835
  %shft.chk165 = icmp ult i64 %835, 64
  %865 = select i1 %shft.chk165, i64 %864, i64 0
  %866 = trunc i64 %865 to i32
  %867 = trunc i64 %856 to i32
  %868 = zext i32 %867 to i64
  %869 = mul i64 %868, %region_0_243_constant_46164
  %870 = trunc i64 %869 to i32
  %871 = xor i32 %866, %870
  %region_0_243_constant_56166 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %872 = xor i32 %871, %region_0_243_constant_56166
  %873 = zext i32 %872 to i64
  %874 = mul i64 %873, %region_0_243_constant_18157
  %875 = lshr i64 %874, %835
  %shft.chk167 = icmp ult i64 %835, 64
  %876 = select i1 %shft.chk167, i64 %875, i64 0
  %877 = trunc i64 %876 to i32
  %878 = lshr i64 %869, %835
  %shft.chk168 = icmp ult i64 %835, 64
  %879 = select i1 %shft.chk168, i64 %878, i64 0
  %880 = trunc i64 %879 to i32
  %881 = lshr i64 %831, %835
  %shft.chk169 = icmp ult i64 %835, 64
  %882 = select i1 %shft.chk169, i64 %881, i64 0
  %883 = trunc i64 %882 to i32
  %884 = xor i32 %880, %883
  %region_0_243_constant_68170 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %885 = xor i32 %884, %region_0_243_constant_68170
  %886 = zext i32 %885 to i64
  %887 = mul i64 %886, %region_0_243_constant_18157
  %888 = trunc i64 %887 to i32
  %889 = xor i32 %877, %888
  %region_0_243_constant_75171 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %890 = xor i32 %889, %region_0_243_constant_75171
  %891 = zext i32 %890 to i64
  %892 = mul i64 %891, %region_0_243_constant_46164
  %893 = lshr i64 %892, %835
  %shft.chk172 = icmp ult i64 %835, 64
  %894 = select i1 %shft.chk172, i64 %893, i64 0
  %895 = trunc i64 %894 to i32
  %896 = lshr i64 %887, %835
  %shft.chk173 = icmp ult i64 %835, 64
  %897 = select i1 %shft.chk173, i64 %896, i64 0
  %898 = trunc i64 %897 to i32
  %899 = trunc i64 %834 to i32
  %900 = xor i32 %898, %899
  %region_0_243_constant_86174 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %901 = xor i32 %900, %region_0_243_constant_86174
  %902 = zext i32 %901 to i64
  %903 = mul i64 %902, %region_0_243_constant_46164
  %904 = trunc i64 %903 to i32
  %905 = xor i32 %895, %904
  %region_0_243_constant_93175 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %906 = xor i32 %905, %region_0_243_constant_93175
  %907 = zext i32 %906 to i64
  %908 = mul i64 %907, %region_0_243_constant_18157
  %909 = lshr i64 %908, %835
  %shft.chk176 = icmp ult i64 %835, 64
  %910 = select i1 %shft.chk176, i64 %909, i64 0
  %911 = trunc i64 %910 to i32
  %912 = lshr i64 %903, %835
  %shft.chk177 = icmp ult i64 %835, 64
  %913 = select i1 %shft.chk177, i64 %912, i64 0
  %914 = trunc i64 %913 to i32
  %915 = trunc i64 %863 to i32
  %916 = xor i32 %914, %915
  %region_0_243_constant_104178 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %917 = xor i32 %916, %region_0_243_constant_104178
  %918 = zext i32 %917 to i64
  %919 = mul i64 %918, %region_0_243_constant_18157
  %920 = trunc i64 %919 to i32
  %921 = xor i32 %911, %920
  %region_0_243_constant_111179 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %922 = xor i32 %921, %region_0_243_constant_111179
  %923 = zext i32 %922 to i64
  %924 = mul i64 %923, %region_0_243_constant_46164
  %925 = lshr i64 %924, %835
  %shft.chk180 = icmp ult i64 %835, 64
  %926 = select i1 %shft.chk180, i64 %925, i64 0
  %927 = trunc i64 %926 to i32
  %928 = lshr i64 %919, %835
  %shft.chk181 = icmp ult i64 %835, 64
  %929 = select i1 %shft.chk181, i64 %928, i64 0
  %930 = trunc i64 %929 to i32
  %931 = trunc i64 %874 to i32
  %932 = xor i32 %930, %931
  %region_0_243_constant_122182 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %933 = xor i32 %932, %region_0_243_constant_122182
  %934 = zext i32 %933 to i64
  %935 = mul i64 %934, %region_0_243_constant_46164
  %936 = trunc i64 %935 to i32
  %937 = xor i32 %927, %936
  %region_0_243_constant_129183 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %938 = xor i32 %937, %region_0_243_constant_129183
  %939 = zext i32 %938 to i64
  %940 = mul i64 %939, %region_0_243_constant_18157
  %941 = lshr i64 %940, %835
  %shft.chk184 = icmp ult i64 %835, 64
  %942 = select i1 %shft.chk184, i64 %941, i64 0
  %943 = trunc i64 %942 to i32
  %944 = lshr i64 %935, %835
  %shft.chk185 = icmp ult i64 %835, 64
  %945 = select i1 %shft.chk185, i64 %944, i64 0
  %946 = trunc i64 %945 to i32
  %947 = trunc i64 %892 to i32
  %948 = xor i32 %946, %947
  %region_0_243_constant_140186 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %949 = xor i32 %948, %region_0_243_constant_140186
  %950 = zext i32 %949 to i64
  %951 = mul i64 %950, %region_0_243_constant_18157
  %952 = trunc i64 %951 to i32
  %953 = xor i32 %943, %952
  %region_0_243_constant_147187 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %954 = xor i32 %953, %region_0_243_constant_147187
  %955 = zext i32 %954 to i64
  %956 = mul i64 %955, %region_0_243_constant_46164
  %957 = lshr i64 %956, %835
  %shft.chk188 = icmp ult i64 %835, 64
  %958 = select i1 %shft.chk188, i64 %957, i64 0
  %959 = trunc i64 %958 to i32
  %960 = lshr i64 %951, %835
  %shft.chk189 = icmp ult i64 %835, 64
  %961 = select i1 %shft.chk189, i64 %960, i64 0
  %962 = trunc i64 %961 to i32
  %963 = trunc i64 %908 to i32
  %964 = xor i32 %962, %963
  %region_0_243_constant_158190 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %965 = xor i32 %964, %region_0_243_constant_158190
  %966 = zext i32 %965 to i64
  %967 = mul i64 %966, %region_0_243_constant_46164
  %968 = trunc i64 %967 to i32
  %969 = xor i32 %959, %968
  %region_0_243_constant_165191 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %970 = xor i32 %969, %region_0_243_constant_165191
  %971 = zext i32 %970 to i64
  %972 = mul i64 %971, %region_0_243_constant_18157
  %973 = lshr i64 %972, %835
  %shft.chk192 = icmp ult i64 %835, 64
  %974 = select i1 %shft.chk192, i64 %973, i64 0
  %975 = trunc i64 %974 to i32
  %976 = lshr i64 %967, %835
  %shft.chk193 = icmp ult i64 %835, 64
  %977 = select i1 %shft.chk193, i64 %976, i64 0
  %978 = trunc i64 %977 to i32
  %979 = trunc i64 %924 to i32
  %980 = xor i32 %978, %979
  %region_0_243_constant_176194 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %981 = xor i32 %980, %region_0_243_constant_176194
  %982 = zext i32 %981 to i64
  %983 = mul i64 %982, %region_0_243_constant_18157
  %984 = trunc i64 %983 to i32
  %985 = xor i32 %975, %984
  %region_0_243_constant_183195 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %986 = xor i32 %985, %region_0_243_constant_183195
  %987 = zext i32 %986 to i64
  %988 = mul i64 %987, %region_0_243_constant_46164
  %989 = lshr i64 %988, %835
  %shft.chk196 = icmp ult i64 %835, 64
  %990 = select i1 %shft.chk196, i64 %989, i64 0
  %991 = trunc i64 %990 to i32
  %992 = lshr i64 %983, %835
  %shft.chk197 = icmp ult i64 %835, 64
  %993 = select i1 %shft.chk197, i64 %992, i64 0
  %994 = trunc i64 %993 to i32
  %995 = trunc i64 %940 to i32
  %996 = xor i32 %994, %995
  %region_0_243_constant_194198 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %997 = xor i32 %996, %region_0_243_constant_194198
  %998 = zext i32 %997 to i64
  %999 = mul i64 %998, %region_0_243_constant_46164
  %1000 = trunc i64 %999 to i32
  %1001 = xor i32 %991, %1000
  %region_0_243_constant_201199 = load i32, i32* bitcast ([4 x i8]* @26 to i32*), align 4
  %1002 = xor i32 %1001, %region_0_243_constant_201199
  br label %concatenate.226.merge153

concat_index_from_operand_id1200:                 ; preds = %concatenate.pivot.1.333
  %1003 = phi i32 [ 1, %concatenate.pivot.1.333 ]
  %1004 = sub nsw i32 %810, %1003
  %1005 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1006 = load i64, i64* %1005, align 8, !invariant.load !22
  %1007 = trunc i64 %1006 to i32
  %1008 = zext i32 %1007 to i64
  %1009 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1010 = lshr i64 %1006, %1009
  %shft.chk201 = icmp ult i64 %1009, 64
  %1011 = select i1 %shft.chk201, i64 %1010, i64 0
  %1012 = trunc i64 %1011 to i32
  %1013 = zext i32 %1012 to i64
  %1014 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1015 = shl i64 %1013, %1014
  %shft.chk202 = icmp ult i64 %1014, 64
  %1016 = select i1 %shft.chk202, i64 %1015, i64 0
  %1017 = or i64 %1008, %1016
  %1018 = mul nuw nsw i32 %811, 1
  %1019 = add nuw nsw i32 0, %1018
  %1020 = zext i32 %1019 to i64
  %1021 = add i64 %1017, %1020
  %1022 = trunc i64 %1021 to i32
  %1023 = zext i32 %1022 to i64
  %region_0_243_constant_18203 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %1024 = mul i64 %1023, %region_0_243_constant_18203
  %1025 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1026 = lshr i64 %1024, %1025
  %shft.chk204 = icmp ult i64 %1025, 64
  %1027 = select i1 %shft.chk204, i64 %1026, i64 0
  %1028 = trunc i64 %1027 to i32
  %1029 = icmp ult i64 %1021, %1017
  %1030 = zext i1 %1029 to i8
  %1031 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1032 = load i64, i64* %1031, align 8, !invariant.load !22
  %1033 = trunc i64 %1032 to i32
  %1034 = zext i32 %1033 to i64
  %1035 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1036 = lshr i64 %1032, %1035
  %shft.chk205 = icmp ult i64 %1035, 64
  %1037 = select i1 %shft.chk205, i64 %1036, i64 0
  %1038 = trunc i64 %1037 to i32
  %1039 = zext i32 %1038 to i64
  %1040 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1041 = shl i64 %1039, %1040
  %shft.chk206 = icmp ult i64 %1040, 64
  %1042 = select i1 %shft.chk206, i64 %1041, i64 0
  %1043 = or i64 %1034, %1042
  %region_0_243_constant_34207 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1044 = add i64 %1043, %region_0_243_constant_34207
  %1045 = trunc i8 %1030 to i1
  %1046 = select i1 %1045, i64 %1044, i64 %1043
  %1047 = lshr i64 %1046, %1025
  %shft.chk208 = icmp ult i64 %1025, 64
  %1048 = select i1 %shft.chk208, i64 %1047, i64 0
  %1049 = trunc i64 %1048 to i32
  %1050 = xor i32 %1028, %1049
  %region_0_243_constant_42209 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %1051 = xor i32 %1050, %region_0_243_constant_42209
  %1052 = zext i32 %1051 to i64
  %region_0_243_constant_46210 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1053 = mul i64 %1052, %region_0_243_constant_46210
  %1054 = lshr i64 %1053, %1025
  %shft.chk211 = icmp ult i64 %1025, 64
  %1055 = select i1 %shft.chk211, i64 %1054, i64 0
  %1056 = trunc i64 %1055 to i32
  %1057 = trunc i64 %1046 to i32
  %1058 = zext i32 %1057 to i64
  %1059 = mul i64 %1058, %region_0_243_constant_46210
  %1060 = trunc i64 %1059 to i32
  %1061 = xor i32 %1056, %1060
  %region_0_243_constant_56212 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %1062 = xor i32 %1061, %region_0_243_constant_56212
  %1063 = zext i32 %1062 to i64
  %1064 = mul i64 %1063, %region_0_243_constant_18203
  %1065 = lshr i64 %1064, %1025
  %shft.chk213 = icmp ult i64 %1025, 64
  %1066 = select i1 %shft.chk213, i64 %1065, i64 0
  %1067 = trunc i64 %1066 to i32
  %1068 = lshr i64 %1059, %1025
  %shft.chk214 = icmp ult i64 %1025, 64
  %1069 = select i1 %shft.chk214, i64 %1068, i64 0
  %1070 = trunc i64 %1069 to i32
  %1071 = lshr i64 %1021, %1025
  %shft.chk215 = icmp ult i64 %1025, 64
  %1072 = select i1 %shft.chk215, i64 %1071, i64 0
  %1073 = trunc i64 %1072 to i32
  %1074 = xor i32 %1070, %1073
  %region_0_243_constant_68216 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %1075 = xor i32 %1074, %region_0_243_constant_68216
  %1076 = zext i32 %1075 to i64
  %1077 = mul i64 %1076, %region_0_243_constant_18203
  %1078 = trunc i64 %1077 to i32
  %1079 = xor i32 %1067, %1078
  %region_0_243_constant_75217 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %1080 = xor i32 %1079, %region_0_243_constant_75217
  %1081 = zext i32 %1080 to i64
  %1082 = mul i64 %1081, %region_0_243_constant_46210
  %1083 = lshr i64 %1082, %1025
  %shft.chk218 = icmp ult i64 %1025, 64
  %1084 = select i1 %shft.chk218, i64 %1083, i64 0
  %1085 = trunc i64 %1084 to i32
  %1086 = lshr i64 %1077, %1025
  %shft.chk219 = icmp ult i64 %1025, 64
  %1087 = select i1 %shft.chk219, i64 %1086, i64 0
  %1088 = trunc i64 %1087 to i32
  %1089 = trunc i64 %1024 to i32
  %1090 = xor i32 %1088, %1089
  %region_0_243_constant_86220 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %1091 = xor i32 %1090, %region_0_243_constant_86220
  %1092 = zext i32 %1091 to i64
  %1093 = mul i64 %1092, %region_0_243_constant_46210
  %1094 = trunc i64 %1093 to i32
  %1095 = xor i32 %1085, %1094
  %region_0_243_constant_93221 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %1096 = xor i32 %1095, %region_0_243_constant_93221
  %1097 = zext i32 %1096 to i64
  %1098 = mul i64 %1097, %region_0_243_constant_18203
  %1099 = lshr i64 %1098, %1025
  %shft.chk222 = icmp ult i64 %1025, 64
  %1100 = select i1 %shft.chk222, i64 %1099, i64 0
  %1101 = trunc i64 %1100 to i32
  %1102 = lshr i64 %1093, %1025
  %shft.chk223 = icmp ult i64 %1025, 64
  %1103 = select i1 %shft.chk223, i64 %1102, i64 0
  %1104 = trunc i64 %1103 to i32
  %1105 = trunc i64 %1053 to i32
  %1106 = xor i32 %1104, %1105
  %region_0_243_constant_104224 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %1107 = xor i32 %1106, %region_0_243_constant_104224
  %1108 = zext i32 %1107 to i64
  %1109 = mul i64 %1108, %region_0_243_constant_18203
  %1110 = trunc i64 %1109 to i32
  %1111 = xor i32 %1101, %1110
  %region_0_243_constant_111225 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1112 = xor i32 %1111, %region_0_243_constant_111225
  %1113 = zext i32 %1112 to i64
  %1114 = mul i64 %1113, %region_0_243_constant_46210
  %1115 = lshr i64 %1114, %1025
  %shft.chk226 = icmp ult i64 %1025, 64
  %1116 = select i1 %shft.chk226, i64 %1115, i64 0
  %1117 = trunc i64 %1116 to i32
  %1118 = lshr i64 %1109, %1025
  %shft.chk227 = icmp ult i64 %1025, 64
  %1119 = select i1 %shft.chk227, i64 %1118, i64 0
  %1120 = trunc i64 %1119 to i32
  %1121 = trunc i64 %1064 to i32
  %1122 = xor i32 %1120, %1121
  %region_0_243_constant_122228 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %1123 = xor i32 %1122, %region_0_243_constant_122228
  %1124 = zext i32 %1123 to i64
  %1125 = mul i64 %1124, %region_0_243_constant_46210
  %1126 = trunc i64 %1125 to i32
  %1127 = xor i32 %1117, %1126
  %region_0_243_constant_129229 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %1128 = xor i32 %1127, %region_0_243_constant_129229
  %1129 = zext i32 %1128 to i64
  %1130 = mul i64 %1129, %region_0_243_constant_18203
  %1131 = lshr i64 %1130, %1025
  %shft.chk230 = icmp ult i64 %1025, 64
  %1132 = select i1 %shft.chk230, i64 %1131, i64 0
  %1133 = trunc i64 %1132 to i32
  %1134 = lshr i64 %1125, %1025
  %shft.chk231 = icmp ult i64 %1025, 64
  %1135 = select i1 %shft.chk231, i64 %1134, i64 0
  %1136 = trunc i64 %1135 to i32
  %1137 = trunc i64 %1082 to i32
  %1138 = xor i32 %1136, %1137
  %region_0_243_constant_140232 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %1139 = xor i32 %1138, %region_0_243_constant_140232
  %1140 = zext i32 %1139 to i64
  %1141 = mul i64 %1140, %region_0_243_constant_18203
  %1142 = trunc i64 %1141 to i32
  %1143 = xor i32 %1133, %1142
  %region_0_243_constant_147233 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %1144 = xor i32 %1143, %region_0_243_constant_147233
  %1145 = zext i32 %1144 to i64
  %1146 = mul i64 %1145, %region_0_243_constant_46210
  %1147 = lshr i64 %1146, %1025
  %shft.chk234 = icmp ult i64 %1025, 64
  %1148 = select i1 %shft.chk234, i64 %1147, i64 0
  %1149 = trunc i64 %1148 to i32
  %1150 = lshr i64 %1141, %1025
  %shft.chk235 = icmp ult i64 %1025, 64
  %1151 = select i1 %shft.chk235, i64 %1150, i64 0
  %1152 = trunc i64 %1151 to i32
  %1153 = trunc i64 %1098 to i32
  %1154 = xor i32 %1152, %1153
  %region_0_243_constant_158236 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1155 = xor i32 %1154, %region_0_243_constant_158236
  %1156 = zext i32 %1155 to i64
  %1157 = mul i64 %1156, %region_0_243_constant_46210
  %1158 = trunc i64 %1157 to i32
  %1159 = xor i32 %1149, %1158
  %region_0_243_constant_165237 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1160 = xor i32 %1159, %region_0_243_constant_165237
  %1161 = zext i32 %1160 to i64
  %1162 = mul i64 %1161, %region_0_243_constant_18203
  %1163 = lshr i64 %1162, %1025
  %shft.chk238 = icmp ult i64 %1025, 64
  %1164 = select i1 %shft.chk238, i64 %1163, i64 0
  %1165 = trunc i64 %1164 to i32
  %1166 = lshr i64 %1157, %1025
  %shft.chk239 = icmp ult i64 %1025, 64
  %1167 = select i1 %shft.chk239, i64 %1166, i64 0
  %1168 = trunc i64 %1167 to i32
  %1169 = trunc i64 %1114 to i32
  %1170 = xor i32 %1168, %1169
  %region_0_243_constant_176240 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %1171 = xor i32 %1170, %region_0_243_constant_176240
  %1172 = zext i32 %1171 to i64
  %1173 = mul i64 %1172, %region_0_243_constant_18203
  %1174 = trunc i64 %1173 to i32
  %1175 = xor i32 %1165, %1174
  %region_0_243_constant_183241 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1176 = xor i32 %1175, %region_0_243_constant_183241
  %1177 = zext i32 %1176 to i64
  %1178 = mul i64 %1177, %region_0_243_constant_46210
  %1179 = trunc i64 %1178 to i32
  br label %concatenate.226.merge153

concat_index_from_operand_id2242:                 ; preds = %concatenate.pivot.2.335
  %1180 = phi i32 [ 2, %concatenate.pivot.2.335 ]
  %1181 = sub nsw i32 %810, %1180
  %1182 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1183 = load i64, i64* %1182, align 8, !invariant.load !22
  %1184 = trunc i64 %1183 to i32
  %1185 = zext i32 %1184 to i64
  %1186 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1187 = lshr i64 %1183, %1186
  %shft.chk243 = icmp ult i64 %1186, 64
  %1188 = select i1 %shft.chk243, i64 %1187, i64 0
  %1189 = trunc i64 %1188 to i32
  %1190 = zext i32 %1189 to i64
  %1191 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1192 = shl i64 %1190, %1191
  %shft.chk244 = icmp ult i64 %1191, 64
  %1193 = select i1 %shft.chk244, i64 %1192, i64 0
  %1194 = or i64 %1185, %1193
  %1195 = mul nuw nsw i32 %811, 1
  %1196 = add nuw nsw i32 0, %1195
  %1197 = zext i32 %1196 to i64
  %1198 = add i64 %1194, %1197
  %1199 = icmp ult i64 %1198, %1194
  %1200 = zext i1 %1199 to i8
  %1201 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1202 = load i64, i64* %1201, align 8, !invariant.load !22
  %1203 = trunc i64 %1202 to i32
  %1204 = zext i32 %1203 to i64
  %1205 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1206 = lshr i64 %1202, %1205
  %shft.chk245 = icmp ult i64 %1205, 64
  %1207 = select i1 %shft.chk245, i64 %1206, i64 0
  %1208 = trunc i64 %1207 to i32
  %1209 = zext i32 %1208 to i64
  %1210 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1211 = shl i64 %1209, %1210
  %shft.chk246 = icmp ult i64 %1210, 64
  %1212 = select i1 %shft.chk246, i64 %1211, i64 0
  %1213 = or i64 %1204, %1212
  %region_0_243_constant_34247 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1214 = add i64 %1213, %region_0_243_constant_34247
  %1215 = trunc i8 %1200 to i1
  %1216 = select i1 %1215, i64 %1214, i64 %1213
  %1217 = trunc i64 %1216 to i32
  %1218 = zext i32 %1217 to i64
  %region_0_243_constant_46248 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1219 = mul i64 %1218, %region_0_243_constant_46248
  %1220 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1221 = lshr i64 %1219, %1220
  %shft.chk249 = icmp ult i64 %1220, 64
  %1222 = select i1 %shft.chk249, i64 %1221, i64 0
  %1223 = trunc i64 %1222 to i32
  %1224 = lshr i64 %1198, %1220
  %shft.chk250 = icmp ult i64 %1220, 64
  %1225 = select i1 %shft.chk250, i64 %1224, i64 0
  %1226 = trunc i64 %1225 to i32
  %1227 = xor i32 %1223, %1226
  %region_0_243_constant_68251 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %1228 = xor i32 %1227, %region_0_243_constant_68251
  %1229 = zext i32 %1228 to i64
  %region_0_243_constant_18252 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %1230 = mul i64 %1229, %region_0_243_constant_18252
  %1231 = lshr i64 %1230, %1220
  %shft.chk253 = icmp ult i64 %1220, 64
  %1232 = select i1 %shft.chk253, i64 %1231, i64 0
  %1233 = trunc i64 %1232 to i32
  %1234 = trunc i64 %1198 to i32
  %1235 = zext i32 %1234 to i64
  %1236 = mul i64 %1235, %region_0_243_constant_18252
  %1237 = trunc i64 %1236 to i32
  %1238 = xor i32 %1233, %1237
  %region_0_243_constant_86254 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %1239 = xor i32 %1238, %region_0_243_constant_86254
  %1240 = zext i32 %1239 to i64
  %1241 = mul i64 %1240, %region_0_243_constant_46248
  %1242 = lshr i64 %1241, %1220
  %shft.chk255 = icmp ult i64 %1220, 64
  %1243 = select i1 %shft.chk255, i64 %1242, i64 0
  %1244 = trunc i64 %1243 to i32
  %1245 = lshr i64 %1236, %1220
  %shft.chk256 = icmp ult i64 %1220, 64
  %1246 = select i1 %shft.chk256, i64 %1245, i64 0
  %1247 = trunc i64 %1246 to i32
  %1248 = lshr i64 %1216, %1220
  %shft.chk257 = icmp ult i64 %1220, 64
  %1249 = select i1 %shft.chk257, i64 %1248, i64 0
  %1250 = trunc i64 %1249 to i32
  %1251 = xor i32 %1247, %1250
  %region_0_243_constant_42258 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %1252 = xor i32 %1251, %region_0_243_constant_42258
  %1253 = zext i32 %1252 to i64
  %1254 = mul i64 %1253, %region_0_243_constant_46248
  %1255 = trunc i64 %1254 to i32
  %1256 = xor i32 %1244, %1255
  %region_0_243_constant_104259 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %1257 = xor i32 %1256, %region_0_243_constant_104259
  %1258 = zext i32 %1257 to i64
  %1259 = mul i64 %1258, %region_0_243_constant_18252
  %1260 = lshr i64 %1259, %1220
  %shft.chk260 = icmp ult i64 %1220, 64
  %1261 = select i1 %shft.chk260, i64 %1260, i64 0
  %1262 = trunc i64 %1261 to i32
  %1263 = lshr i64 %1254, %1220
  %shft.chk261 = icmp ult i64 %1220, 64
  %1264 = select i1 %shft.chk261, i64 %1263, i64 0
  %1265 = trunc i64 %1264 to i32
  %1266 = trunc i64 %1219 to i32
  %1267 = xor i32 %1265, %1266
  %region_0_243_constant_56262 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %1268 = xor i32 %1267, %region_0_243_constant_56262
  %1269 = zext i32 %1268 to i64
  %1270 = mul i64 %1269, %region_0_243_constant_18252
  %1271 = trunc i64 %1270 to i32
  %1272 = xor i32 %1262, %1271
  %region_0_243_constant_122263 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %1273 = xor i32 %1272, %region_0_243_constant_122263
  %1274 = zext i32 %1273 to i64
  %1275 = mul i64 %1274, %region_0_243_constant_46248
  %1276 = lshr i64 %1275, %1220
  %shft.chk264 = icmp ult i64 %1220, 64
  %1277 = select i1 %shft.chk264, i64 %1276, i64 0
  %1278 = trunc i64 %1277 to i32
  %1279 = lshr i64 %1270, %1220
  %shft.chk265 = icmp ult i64 %1220, 64
  %1280 = select i1 %shft.chk265, i64 %1279, i64 0
  %1281 = trunc i64 %1280 to i32
  %1282 = trunc i64 %1230 to i32
  %1283 = xor i32 %1281, %1282
  %region_0_243_constant_75266 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %1284 = xor i32 %1283, %region_0_243_constant_75266
  %1285 = zext i32 %1284 to i64
  %1286 = mul i64 %1285, %region_0_243_constant_46248
  %1287 = trunc i64 %1286 to i32
  %1288 = xor i32 %1278, %1287
  %region_0_243_constant_140267 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %1289 = xor i32 %1288, %region_0_243_constant_140267
  %1290 = zext i32 %1289 to i64
  %1291 = mul i64 %1290, %region_0_243_constant_18252
  %1292 = lshr i64 %1291, %1220
  %shft.chk268 = icmp ult i64 %1220, 64
  %1293 = select i1 %shft.chk268, i64 %1292, i64 0
  %1294 = trunc i64 %1293 to i32
  %1295 = lshr i64 %1286, %1220
  %shft.chk269 = icmp ult i64 %1220, 64
  %1296 = select i1 %shft.chk269, i64 %1295, i64 0
  %1297 = trunc i64 %1296 to i32
  %1298 = trunc i64 %1241 to i32
  %1299 = xor i32 %1297, %1298
  %region_0_243_constant_93270 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %1300 = xor i32 %1299, %region_0_243_constant_93270
  %1301 = zext i32 %1300 to i64
  %1302 = mul i64 %1301, %region_0_243_constant_18252
  %1303 = trunc i64 %1302 to i32
  %1304 = xor i32 %1294, %1303
  %region_0_243_constant_158271 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1305 = xor i32 %1304, %region_0_243_constant_158271
  %1306 = zext i32 %1305 to i64
  %1307 = mul i64 %1306, %region_0_243_constant_46248
  %1308 = lshr i64 %1307, %1220
  %shft.chk272 = icmp ult i64 %1220, 64
  %1309 = select i1 %shft.chk272, i64 %1308, i64 0
  %1310 = trunc i64 %1309 to i32
  %1311 = lshr i64 %1302, %1220
  %shft.chk273 = icmp ult i64 %1220, 64
  %1312 = select i1 %shft.chk273, i64 %1311, i64 0
  %1313 = trunc i64 %1312 to i32
  %1314 = trunc i64 %1259 to i32
  %1315 = xor i32 %1313, %1314
  %region_0_243_constant_111274 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1316 = xor i32 %1315, %region_0_243_constant_111274
  %1317 = zext i32 %1316 to i64
  %1318 = mul i64 %1317, %region_0_243_constant_46248
  %1319 = trunc i64 %1318 to i32
  %1320 = xor i32 %1310, %1319
  %region_0_243_constant_176275 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %1321 = xor i32 %1320, %region_0_243_constant_176275
  %1322 = zext i32 %1321 to i64
  %1323 = mul i64 %1322, %region_0_243_constant_18252
  %1324 = lshr i64 %1323, %1220
  %shft.chk276 = icmp ult i64 %1220, 64
  %1325 = select i1 %shft.chk276, i64 %1324, i64 0
  %1326 = trunc i64 %1325 to i32
  %1327 = lshr i64 %1318, %1220
  %shft.chk277 = icmp ult i64 %1220, 64
  %1328 = select i1 %shft.chk277, i64 %1327, i64 0
  %1329 = trunc i64 %1328 to i32
  %1330 = trunc i64 %1275 to i32
  %1331 = xor i32 %1329, %1330
  %region_0_243_constant_129278 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %1332 = xor i32 %1331, %region_0_243_constant_129278
  %1333 = zext i32 %1332 to i64
  %1334 = mul i64 %1333, %region_0_243_constant_18252
  %1335 = trunc i64 %1334 to i32
  %1336 = xor i32 %1326, %1335
  %region_0_243_constant_194279 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %1337 = xor i32 %1336, %region_0_243_constant_194279
  %1338 = zext i32 %1337 to i64
  %1339 = mul i64 %1338, %region_0_243_constant_46248
  %1340 = lshr i64 %1339, %1220
  %shft.chk280 = icmp ult i64 %1220, 64
  %1341 = select i1 %shft.chk280, i64 %1340, i64 0
  %1342 = trunc i64 %1341 to i32
  %1343 = lshr i64 %1334, %1220
  %shft.chk281 = icmp ult i64 %1220, 64
  %1344 = select i1 %shft.chk281, i64 %1343, i64 0
  %1345 = trunc i64 %1344 to i32
  %1346 = trunc i64 %1291 to i32
  %1347 = xor i32 %1345, %1346
  %region_0_243_constant_147282 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %1348 = xor i32 %1347, %region_0_243_constant_147282
  %1349 = zext i32 %1348 to i64
  %1350 = mul i64 %1349, %region_0_243_constant_46248
  %1351 = trunc i64 %1350 to i32
  %1352 = xor i32 %1342, %1351
  %region_0_243_constant_211283 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %1353 = xor i32 %1352, %region_0_243_constant_211283
  %1354 = zext i32 %1353 to i64
  %1355 = mul i64 %1354, %region_0_243_constant_18252
  %1356 = lshr i64 %1355, %1220
  %shft.chk284 = icmp ult i64 %1220, 64
  %1357 = select i1 %shft.chk284, i64 %1356, i64 0
  %1358 = trunc i64 %1357 to i32
  %1359 = lshr i64 %1350, %1220
  %shft.chk285 = icmp ult i64 %1220, 64
  %1360 = select i1 %shft.chk285, i64 %1359, i64 0
  %1361 = trunc i64 %1360 to i32
  %1362 = trunc i64 %1307 to i32
  %1363 = xor i32 %1361, %1362
  %region_0_243_constant_165286 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1364 = xor i32 %1363, %region_0_243_constant_165286
  %1365 = zext i32 %1364 to i64
  %1366 = mul i64 %1365, %region_0_243_constant_18252
  %1367 = trunc i64 %1366 to i32
  %1368 = xor i32 %1358, %1367
  %region_0_243_constant_220287 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1369 = xor i32 %1368, %region_0_243_constant_220287
  br label %concatenate.226.merge153

concat_index_from_operand_id3288:                 ; preds = %concatenate.pivot.3.336
  %1370 = phi i32 [ 3, %concatenate.pivot.3.336 ]
  %1371 = sub nsw i32 %810, %1370
  %1372 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1373 = load i64, i64* %1372, align 8, !invariant.load !22
  %1374 = trunc i64 %1373 to i32
  %1375 = zext i32 %1374 to i64
  %1376 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1377 = lshr i64 %1373, %1376
  %shft.chk289 = icmp ult i64 %1376, 64
  %1378 = select i1 %shft.chk289, i64 %1377, i64 0
  %1379 = trunc i64 %1378 to i32
  %1380 = zext i32 %1379 to i64
  %1381 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1382 = shl i64 %1380, %1381
  %shft.chk290 = icmp ult i64 %1381, 64
  %1383 = select i1 %shft.chk290, i64 %1382, i64 0
  %1384 = or i64 %1375, %1383
  %1385 = mul nuw nsw i32 %811, 1
  %1386 = add nuw nsw i32 0, %1385
  %1387 = zext i32 %1386 to i64
  %1388 = add i64 %1384, %1387
  %1389 = icmp ult i64 %1388, %1384
  %1390 = zext i1 %1389 to i8
  %1391 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1392 = load i64, i64* %1391, align 8, !invariant.load !22
  %1393 = trunc i64 %1392 to i32
  %1394 = zext i32 %1393 to i64
  %1395 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1396 = lshr i64 %1392, %1395
  %shft.chk291 = icmp ult i64 %1395, 64
  %1397 = select i1 %shft.chk291, i64 %1396, i64 0
  %1398 = trunc i64 %1397 to i32
  %1399 = zext i32 %1398 to i64
  %1400 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1401 = shl i64 %1399, %1400
  %shft.chk292 = icmp ult i64 %1400, 64
  %1402 = select i1 %shft.chk292, i64 %1401, i64 0
  %1403 = or i64 %1394, %1402
  %region_0_243_constant_34293 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1404 = add i64 %1403, %region_0_243_constant_34293
  %1405 = trunc i8 %1390 to i1
  %1406 = select i1 %1405, i64 %1404, i64 %1403
  %1407 = trunc i64 %1406 to i32
  %1408 = zext i32 %1407 to i64
  %region_0_243_constant_46294 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1409 = mul i64 %1408, %region_0_243_constant_46294
  %1410 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1411 = lshr i64 %1409, %1410
  %shft.chk295 = icmp ult i64 %1410, 64
  %1412 = select i1 %shft.chk295, i64 %1411, i64 0
  %1413 = trunc i64 %1412 to i32
  %1414 = lshr i64 %1388, %1410
  %shft.chk296 = icmp ult i64 %1410, 64
  %1415 = select i1 %shft.chk296, i64 %1414, i64 0
  %1416 = trunc i64 %1415 to i32
  %1417 = xor i32 %1413, %1416
  %region_0_243_constant_68297 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %1418 = xor i32 %1417, %region_0_243_constant_68297
  %1419 = zext i32 %1418 to i64
  %region_0_243_constant_18298 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %1420 = mul i64 %1419, %region_0_243_constant_18298
  %1421 = lshr i64 %1420, %1410
  %shft.chk299 = icmp ult i64 %1410, 64
  %1422 = select i1 %shft.chk299, i64 %1421, i64 0
  %1423 = trunc i64 %1422 to i32
  %1424 = trunc i64 %1388 to i32
  %1425 = zext i32 %1424 to i64
  %1426 = mul i64 %1425, %region_0_243_constant_18298
  %1427 = trunc i64 %1426 to i32
  %1428 = xor i32 %1423, %1427
  %region_0_243_constant_86300 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %1429 = xor i32 %1428, %region_0_243_constant_86300
  %1430 = zext i32 %1429 to i64
  %1431 = mul i64 %1430, %region_0_243_constant_46294
  %1432 = lshr i64 %1431, %1410
  %shft.chk301 = icmp ult i64 %1410, 64
  %1433 = select i1 %shft.chk301, i64 %1432, i64 0
  %1434 = trunc i64 %1433 to i32
  %1435 = lshr i64 %1426, %1410
  %shft.chk302 = icmp ult i64 %1410, 64
  %1436 = select i1 %shft.chk302, i64 %1435, i64 0
  %1437 = trunc i64 %1436 to i32
  %1438 = lshr i64 %1406, %1410
  %shft.chk303 = icmp ult i64 %1410, 64
  %1439 = select i1 %shft.chk303, i64 %1438, i64 0
  %1440 = trunc i64 %1439 to i32
  %1441 = xor i32 %1437, %1440
  %region_0_243_constant_42304 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %1442 = xor i32 %1441, %region_0_243_constant_42304
  %1443 = zext i32 %1442 to i64
  %1444 = mul i64 %1443, %region_0_243_constant_46294
  %1445 = trunc i64 %1444 to i32
  %1446 = xor i32 %1434, %1445
  %region_0_243_constant_104305 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %1447 = xor i32 %1446, %region_0_243_constant_104305
  %1448 = zext i32 %1447 to i64
  %1449 = mul i64 %1448, %region_0_243_constant_18298
  %1450 = lshr i64 %1449, %1410
  %shft.chk306 = icmp ult i64 %1410, 64
  %1451 = select i1 %shft.chk306, i64 %1450, i64 0
  %1452 = trunc i64 %1451 to i32
  %1453 = lshr i64 %1444, %1410
  %shft.chk307 = icmp ult i64 %1410, 64
  %1454 = select i1 %shft.chk307, i64 %1453, i64 0
  %1455 = trunc i64 %1454 to i32
  %1456 = trunc i64 %1409 to i32
  %1457 = xor i32 %1455, %1456
  %region_0_243_constant_56308 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %1458 = xor i32 %1457, %region_0_243_constant_56308
  %1459 = zext i32 %1458 to i64
  %1460 = mul i64 %1459, %region_0_243_constant_18298
  %1461 = trunc i64 %1460 to i32
  %1462 = xor i32 %1452, %1461
  %region_0_243_constant_122309 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %1463 = xor i32 %1462, %region_0_243_constant_122309
  %1464 = zext i32 %1463 to i64
  %1465 = mul i64 %1464, %region_0_243_constant_46294
  %1466 = lshr i64 %1465, %1410
  %shft.chk310 = icmp ult i64 %1410, 64
  %1467 = select i1 %shft.chk310, i64 %1466, i64 0
  %1468 = trunc i64 %1467 to i32
  %1469 = lshr i64 %1460, %1410
  %shft.chk311 = icmp ult i64 %1410, 64
  %1470 = select i1 %shft.chk311, i64 %1469, i64 0
  %1471 = trunc i64 %1470 to i32
  %1472 = trunc i64 %1420 to i32
  %1473 = xor i32 %1471, %1472
  %region_0_243_constant_75312 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %1474 = xor i32 %1473, %region_0_243_constant_75312
  %1475 = zext i32 %1474 to i64
  %1476 = mul i64 %1475, %region_0_243_constant_46294
  %1477 = trunc i64 %1476 to i32
  %1478 = xor i32 %1468, %1477
  %region_0_243_constant_140313 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %1479 = xor i32 %1478, %region_0_243_constant_140313
  %1480 = zext i32 %1479 to i64
  %1481 = mul i64 %1480, %region_0_243_constant_18298
  %1482 = lshr i64 %1481, %1410
  %shft.chk314 = icmp ult i64 %1410, 64
  %1483 = select i1 %shft.chk314, i64 %1482, i64 0
  %1484 = trunc i64 %1483 to i32
  %1485 = lshr i64 %1476, %1410
  %shft.chk315 = icmp ult i64 %1410, 64
  %1486 = select i1 %shft.chk315, i64 %1485, i64 0
  %1487 = trunc i64 %1486 to i32
  %1488 = trunc i64 %1431 to i32
  %1489 = xor i32 %1487, %1488
  %region_0_243_constant_93316 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %1490 = xor i32 %1489, %region_0_243_constant_93316
  %1491 = zext i32 %1490 to i64
  %1492 = mul i64 %1491, %region_0_243_constant_18298
  %1493 = trunc i64 %1492 to i32
  %1494 = xor i32 %1484, %1493
  %region_0_243_constant_158317 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1495 = xor i32 %1494, %region_0_243_constant_158317
  %1496 = zext i32 %1495 to i64
  %1497 = mul i64 %1496, %region_0_243_constant_46294
  %1498 = lshr i64 %1497, %1410
  %shft.chk318 = icmp ult i64 %1410, 64
  %1499 = select i1 %shft.chk318, i64 %1498, i64 0
  %1500 = trunc i64 %1499 to i32
  %1501 = lshr i64 %1492, %1410
  %shft.chk319 = icmp ult i64 %1410, 64
  %1502 = select i1 %shft.chk319, i64 %1501, i64 0
  %1503 = trunc i64 %1502 to i32
  %1504 = trunc i64 %1449 to i32
  %1505 = xor i32 %1503, %1504
  %region_0_243_constant_111320 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1506 = xor i32 %1505, %region_0_243_constant_111320
  %1507 = zext i32 %1506 to i64
  %1508 = mul i64 %1507, %region_0_243_constant_46294
  %1509 = trunc i64 %1508 to i32
  %1510 = xor i32 %1500, %1509
  %region_0_243_constant_176321 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %1511 = xor i32 %1510, %region_0_243_constant_176321
  %1512 = zext i32 %1511 to i64
  %1513 = mul i64 %1512, %region_0_243_constant_18298
  %1514 = lshr i64 %1513, %1410
  %shft.chk322 = icmp ult i64 %1410, 64
  %1515 = select i1 %shft.chk322, i64 %1514, i64 0
  %1516 = trunc i64 %1515 to i32
  %1517 = lshr i64 %1508, %1410
  %shft.chk323 = icmp ult i64 %1410, 64
  %1518 = select i1 %shft.chk323, i64 %1517, i64 0
  %1519 = trunc i64 %1518 to i32
  %1520 = trunc i64 %1465 to i32
  %1521 = xor i32 %1519, %1520
  %region_0_243_constant_129324 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %1522 = xor i32 %1521, %region_0_243_constant_129324
  %1523 = zext i32 %1522 to i64
  %1524 = mul i64 %1523, %region_0_243_constant_18298
  %1525 = trunc i64 %1524 to i32
  %1526 = xor i32 %1516, %1525
  %region_0_243_constant_194325 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %1527 = xor i32 %1526, %region_0_243_constant_194325
  %1528 = zext i32 %1527 to i64
  %1529 = mul i64 %1528, %region_0_243_constant_46294
  %1530 = lshr i64 %1529, %1410
  %shft.chk326 = icmp ult i64 %1410, 64
  %1531 = select i1 %shft.chk326, i64 %1530, i64 0
  %1532 = trunc i64 %1531 to i32
  %1533 = lshr i64 %1524, %1410
  %shft.chk327 = icmp ult i64 %1410, 64
  %1534 = select i1 %shft.chk327, i64 %1533, i64 0
  %1535 = trunc i64 %1534 to i32
  %1536 = trunc i64 %1481 to i32
  %1537 = xor i32 %1535, %1536
  %region_0_243_constant_147328 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %1538 = xor i32 %1537, %region_0_243_constant_147328
  %1539 = zext i32 %1538 to i64
  %1540 = mul i64 %1539, %region_0_243_constant_46294
  %1541 = trunc i64 %1540 to i32
  %1542 = xor i32 %1532, %1541
  %region_0_243_constant_211329 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %1543 = xor i32 %1542, %region_0_243_constant_211329
  %1544 = zext i32 %1543 to i64
  %1545 = mul i64 %1544, %region_0_243_constant_18298
  %1546 = trunc i64 %1545 to i32
  br label %concatenate.226.merge153

concatenate.pivot.2.330:                          ; preds = %concatenate.226.merge
  %1547 = icmp ult i32 %810, 2
  br i1 %1547, label %concatenate.pivot.1.331, label %concatenate.pivot.3.334

concatenate.pivot.1.331:                          ; preds = %concatenate.pivot.2.330
  %1548 = icmp ult i32 %810, 1
  br i1 %1548, label %concatenate.pivot.0.332, label %concatenate.pivot.1.333

concatenate.pivot.0.332:                          ; preds = %concatenate.pivot.1.331
  br label %concat_index_from_operand_id0154

concatenate.pivot.1.333:                          ; preds = %concatenate.pivot.1.331
  br label %concat_index_from_operand_id1200

concatenate.pivot.3.334:                          ; preds = %concatenate.pivot.2.330
  %1549 = icmp ult i32 %810, 3
  br i1 %1549, label %concatenate.pivot.2.335, label %concatenate.pivot.3.336

concatenate.pivot.2.335:                          ; preds = %concatenate.pivot.3.334
  br label %concat_index_from_operand_id2242

concatenate.pivot.3.336:                          ; preds = %concatenate.pivot.3.334
  br label %concat_index_from_operand_id3288

concatenate.226.merge153:                         ; preds = %concat_index_from_operand_id3288, %concat_index_from_operand_id2242, %concat_index_from_operand_id1200, %concat_index_from_operand_id0154
  %1550 = phi i32 [ %1002, %concat_index_from_operand_id0154 ], [ %1179, %concat_index_from_operand_id1200 ], [ %1369, %concat_index_from_operand_id2242 ], [ %1546, %concat_index_from_operand_id3288 ]
  %region_0_243_constant_227337 = load i32, i32* bitcast ([4 x i8]* @3 to i32*), align 4
  %1551 = lshr i32 %1550, %region_0_243_constant_227337
  %shft.chk338 = icmp ult i32 %region_0_243_constant_227337, 32
  %1552 = select i1 %shft.chk338, i32 %1551, i32 0
  %1553 = uitofp i32 %1552 to float
  %region_0_243_constant_231339 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %multiply.233340 = fmul float %1553, %region_0_243_constant_231339
  %region_0_243_constant_234341 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %compare.236342 = fcmp olt float %multiply.233340, %region_0_243_constant_234341
  %1554 = zext i1 %compare.236342 to i8
  %1555 = mul nuw nsw i32 %797, 1
  %1556 = add nuw nsw i32 0, %1555
  %1557 = udiv i32 %1556, 16
  %1558 = mul nuw nsw i32 %801, 1
  %1559 = add nuw nsw i32 0, %1558
  %1560 = mul nuw nsw i32 %802, 32
  %1561 = add nuw nsw i32 %1559, %1560
  %1562 = udiv i32 %1561, 256
  %1563 = bitcast [256 x [16 x float]]* %1 to float*
  %1564 = getelementptr inbounds float, float* %1563, i32 %linear_index1
  %1565 = load float, float* %1564, align 4, !invariant.load !22
  %region_0_243_constant_239343 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %1566 = trunc i8 %1554 to i1
  %1567 = select i1 %1566, float %1565, float %region_0_243_constant_239343
  %1568 = bitcast [256 x [16 x float]]* %5 to float*
  %1569 = getelementptr inbounds float, float* %1568, i32 %linear_index1
  store float %1567, float* %1569, align 4
  %1570 = mul nuw nsw i32 %16, 1
  %1571 = add nuw nsw i32 0, %1570
  %1572 = udiv i32 %1571, 16
  %1573 = mul nuw nsw i32 %17, 1
  %1574 = add nuw nsw i32 0, %1573
  %1575 = urem i32 %1574, 32
  %1576 = udiv i32 %1574, 32
  %1577 = udiv i32 %1576, 8
  %1578 = mul nuw nsw i32 %1571, 1
  %1579 = add nuw nsw i32 0, %1578
  %1580 = mul nuw nsw i32 %1575, 16
  %1581 = add nuw nsw i32 %1579, %1580
  %1582 = mul nuw nsw i32 %1576, 512
  %1583 = add nuw nsw i32 %1581, %1582
  %1584 = urem i32 %1583, 4
  %1585 = udiv i32 %1583, 4
  %1586 = udiv i32 %1585, 1024
  br label %concatenate.pivot.2.521

concat_index_from_operand_id0345:                 ; preds = %concatenate.pivot.0.523
  %1587 = phi i32 [ 0, %concatenate.pivot.0.523 ]
  %1588 = sub nsw i32 %1584, %1587
  %1589 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1590 = load i64, i64* %1589, align 8, !invariant.load !22
  %1591 = trunc i64 %1590 to i32
  %1592 = zext i32 %1591 to i64
  %1593 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1594 = lshr i64 %1590, %1593
  %shft.chk346 = icmp ult i64 %1593, 64
  %1595 = select i1 %shft.chk346, i64 %1594, i64 0
  %1596 = trunc i64 %1595 to i32
  %1597 = zext i32 %1596 to i64
  %1598 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1599 = shl i64 %1597, %1598
  %shft.chk347 = icmp ult i64 %1598, 64
  %1600 = select i1 %shft.chk347, i64 %1599, i64 0
  %1601 = or i64 %1592, %1600
  %1602 = mul nuw nsw i32 %1585, 1
  %1603 = add nuw nsw i32 0, %1602
  %1604 = zext i32 %1603 to i64
  %1605 = add i64 %1601, %1604
  %1606 = trunc i64 %1605 to i32
  %1607 = zext i32 %1606 to i64
  %region_0_243_constant_18348 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %1608 = mul i64 %1607, %region_0_243_constant_18348
  %1609 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1610 = lshr i64 %1608, %1609
  %shft.chk349 = icmp ult i64 %1609, 64
  %1611 = select i1 %shft.chk349, i64 %1610, i64 0
  %1612 = trunc i64 %1611 to i32
  %1613 = icmp ult i64 %1605, %1601
  %1614 = zext i1 %1613 to i8
  %1615 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1616 = load i64, i64* %1615, align 8, !invariant.load !22
  %1617 = trunc i64 %1616 to i32
  %1618 = zext i32 %1617 to i64
  %1619 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1620 = lshr i64 %1616, %1619
  %shft.chk350 = icmp ult i64 %1619, 64
  %1621 = select i1 %shft.chk350, i64 %1620, i64 0
  %1622 = trunc i64 %1621 to i32
  %1623 = zext i32 %1622 to i64
  %1624 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1625 = shl i64 %1623, %1624
  %shft.chk351 = icmp ult i64 %1624, 64
  %1626 = select i1 %shft.chk351, i64 %1625, i64 0
  %1627 = or i64 %1618, %1626
  %region_0_243_constant_34352 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1628 = add i64 %1627, %region_0_243_constant_34352
  %1629 = trunc i8 %1614 to i1
  %1630 = select i1 %1629, i64 %1628, i64 %1627
  %1631 = lshr i64 %1630, %1609
  %shft.chk353 = icmp ult i64 %1609, 64
  %1632 = select i1 %shft.chk353, i64 %1631, i64 0
  %1633 = trunc i64 %1632 to i32
  %1634 = xor i32 %1612, %1633
  %region_0_243_constant_42354 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %1635 = xor i32 %1634, %region_0_243_constant_42354
  %1636 = zext i32 %1635 to i64
  %region_0_243_constant_46355 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1637 = mul i64 %1636, %region_0_243_constant_46355
  %1638 = lshr i64 %1637, %1609
  %shft.chk356 = icmp ult i64 %1609, 64
  %1639 = select i1 %shft.chk356, i64 %1638, i64 0
  %1640 = trunc i64 %1639 to i32
  %1641 = trunc i64 %1630 to i32
  %1642 = zext i32 %1641 to i64
  %1643 = mul i64 %1642, %region_0_243_constant_46355
  %1644 = trunc i64 %1643 to i32
  %1645 = xor i32 %1640, %1644
  %region_0_243_constant_56357 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %1646 = xor i32 %1645, %region_0_243_constant_56357
  %1647 = zext i32 %1646 to i64
  %1648 = mul i64 %1647, %region_0_243_constant_18348
  %1649 = lshr i64 %1648, %1609
  %shft.chk358 = icmp ult i64 %1609, 64
  %1650 = select i1 %shft.chk358, i64 %1649, i64 0
  %1651 = trunc i64 %1650 to i32
  %1652 = lshr i64 %1643, %1609
  %shft.chk359 = icmp ult i64 %1609, 64
  %1653 = select i1 %shft.chk359, i64 %1652, i64 0
  %1654 = trunc i64 %1653 to i32
  %1655 = lshr i64 %1605, %1609
  %shft.chk360 = icmp ult i64 %1609, 64
  %1656 = select i1 %shft.chk360, i64 %1655, i64 0
  %1657 = trunc i64 %1656 to i32
  %1658 = xor i32 %1654, %1657
  %region_0_243_constant_68361 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %1659 = xor i32 %1658, %region_0_243_constant_68361
  %1660 = zext i32 %1659 to i64
  %1661 = mul i64 %1660, %region_0_243_constant_18348
  %1662 = trunc i64 %1661 to i32
  %1663 = xor i32 %1651, %1662
  %region_0_243_constant_75362 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %1664 = xor i32 %1663, %region_0_243_constant_75362
  %1665 = zext i32 %1664 to i64
  %1666 = mul i64 %1665, %region_0_243_constant_46355
  %1667 = lshr i64 %1666, %1609
  %shft.chk363 = icmp ult i64 %1609, 64
  %1668 = select i1 %shft.chk363, i64 %1667, i64 0
  %1669 = trunc i64 %1668 to i32
  %1670 = lshr i64 %1661, %1609
  %shft.chk364 = icmp ult i64 %1609, 64
  %1671 = select i1 %shft.chk364, i64 %1670, i64 0
  %1672 = trunc i64 %1671 to i32
  %1673 = trunc i64 %1608 to i32
  %1674 = xor i32 %1672, %1673
  %region_0_243_constant_86365 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %1675 = xor i32 %1674, %region_0_243_constant_86365
  %1676 = zext i32 %1675 to i64
  %1677 = mul i64 %1676, %region_0_243_constant_46355
  %1678 = trunc i64 %1677 to i32
  %1679 = xor i32 %1669, %1678
  %region_0_243_constant_93366 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %1680 = xor i32 %1679, %region_0_243_constant_93366
  %1681 = zext i32 %1680 to i64
  %1682 = mul i64 %1681, %region_0_243_constant_18348
  %1683 = lshr i64 %1682, %1609
  %shft.chk367 = icmp ult i64 %1609, 64
  %1684 = select i1 %shft.chk367, i64 %1683, i64 0
  %1685 = trunc i64 %1684 to i32
  %1686 = lshr i64 %1677, %1609
  %shft.chk368 = icmp ult i64 %1609, 64
  %1687 = select i1 %shft.chk368, i64 %1686, i64 0
  %1688 = trunc i64 %1687 to i32
  %1689 = trunc i64 %1637 to i32
  %1690 = xor i32 %1688, %1689
  %region_0_243_constant_104369 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %1691 = xor i32 %1690, %region_0_243_constant_104369
  %1692 = zext i32 %1691 to i64
  %1693 = mul i64 %1692, %region_0_243_constant_18348
  %1694 = trunc i64 %1693 to i32
  %1695 = xor i32 %1685, %1694
  %region_0_243_constant_111370 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1696 = xor i32 %1695, %region_0_243_constant_111370
  %1697 = zext i32 %1696 to i64
  %1698 = mul i64 %1697, %region_0_243_constant_46355
  %1699 = lshr i64 %1698, %1609
  %shft.chk371 = icmp ult i64 %1609, 64
  %1700 = select i1 %shft.chk371, i64 %1699, i64 0
  %1701 = trunc i64 %1700 to i32
  %1702 = lshr i64 %1693, %1609
  %shft.chk372 = icmp ult i64 %1609, 64
  %1703 = select i1 %shft.chk372, i64 %1702, i64 0
  %1704 = trunc i64 %1703 to i32
  %1705 = trunc i64 %1648 to i32
  %1706 = xor i32 %1704, %1705
  %region_0_243_constant_122373 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %1707 = xor i32 %1706, %region_0_243_constant_122373
  %1708 = zext i32 %1707 to i64
  %1709 = mul i64 %1708, %region_0_243_constant_46355
  %1710 = trunc i64 %1709 to i32
  %1711 = xor i32 %1701, %1710
  %region_0_243_constant_129374 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %1712 = xor i32 %1711, %region_0_243_constant_129374
  %1713 = zext i32 %1712 to i64
  %1714 = mul i64 %1713, %region_0_243_constant_18348
  %1715 = lshr i64 %1714, %1609
  %shft.chk375 = icmp ult i64 %1609, 64
  %1716 = select i1 %shft.chk375, i64 %1715, i64 0
  %1717 = trunc i64 %1716 to i32
  %1718 = lshr i64 %1709, %1609
  %shft.chk376 = icmp ult i64 %1609, 64
  %1719 = select i1 %shft.chk376, i64 %1718, i64 0
  %1720 = trunc i64 %1719 to i32
  %1721 = trunc i64 %1666 to i32
  %1722 = xor i32 %1720, %1721
  %region_0_243_constant_140377 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %1723 = xor i32 %1722, %region_0_243_constant_140377
  %1724 = zext i32 %1723 to i64
  %1725 = mul i64 %1724, %region_0_243_constant_18348
  %1726 = trunc i64 %1725 to i32
  %1727 = xor i32 %1717, %1726
  %region_0_243_constant_147378 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %1728 = xor i32 %1727, %region_0_243_constant_147378
  %1729 = zext i32 %1728 to i64
  %1730 = mul i64 %1729, %region_0_243_constant_46355
  %1731 = lshr i64 %1730, %1609
  %shft.chk379 = icmp ult i64 %1609, 64
  %1732 = select i1 %shft.chk379, i64 %1731, i64 0
  %1733 = trunc i64 %1732 to i32
  %1734 = lshr i64 %1725, %1609
  %shft.chk380 = icmp ult i64 %1609, 64
  %1735 = select i1 %shft.chk380, i64 %1734, i64 0
  %1736 = trunc i64 %1735 to i32
  %1737 = trunc i64 %1682 to i32
  %1738 = xor i32 %1736, %1737
  %region_0_243_constant_158381 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1739 = xor i32 %1738, %region_0_243_constant_158381
  %1740 = zext i32 %1739 to i64
  %1741 = mul i64 %1740, %region_0_243_constant_46355
  %1742 = trunc i64 %1741 to i32
  %1743 = xor i32 %1733, %1742
  %region_0_243_constant_165382 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1744 = xor i32 %1743, %region_0_243_constant_165382
  %1745 = zext i32 %1744 to i64
  %1746 = mul i64 %1745, %region_0_243_constant_18348
  %1747 = lshr i64 %1746, %1609
  %shft.chk383 = icmp ult i64 %1609, 64
  %1748 = select i1 %shft.chk383, i64 %1747, i64 0
  %1749 = trunc i64 %1748 to i32
  %1750 = lshr i64 %1741, %1609
  %shft.chk384 = icmp ult i64 %1609, 64
  %1751 = select i1 %shft.chk384, i64 %1750, i64 0
  %1752 = trunc i64 %1751 to i32
  %1753 = trunc i64 %1698 to i32
  %1754 = xor i32 %1752, %1753
  %region_0_243_constant_176385 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %1755 = xor i32 %1754, %region_0_243_constant_176385
  %1756 = zext i32 %1755 to i64
  %1757 = mul i64 %1756, %region_0_243_constant_18348
  %1758 = trunc i64 %1757 to i32
  %1759 = xor i32 %1749, %1758
  %region_0_243_constant_183386 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1760 = xor i32 %1759, %region_0_243_constant_183386
  %1761 = zext i32 %1760 to i64
  %1762 = mul i64 %1761, %region_0_243_constant_46355
  %1763 = lshr i64 %1762, %1609
  %shft.chk387 = icmp ult i64 %1609, 64
  %1764 = select i1 %shft.chk387, i64 %1763, i64 0
  %1765 = trunc i64 %1764 to i32
  %1766 = lshr i64 %1757, %1609
  %shft.chk388 = icmp ult i64 %1609, 64
  %1767 = select i1 %shft.chk388, i64 %1766, i64 0
  %1768 = trunc i64 %1767 to i32
  %1769 = trunc i64 %1714 to i32
  %1770 = xor i32 %1768, %1769
  %region_0_243_constant_194389 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %1771 = xor i32 %1770, %region_0_243_constant_194389
  %1772 = zext i32 %1771 to i64
  %1773 = mul i64 %1772, %region_0_243_constant_46355
  %1774 = trunc i64 %1773 to i32
  %1775 = xor i32 %1765, %1774
  %region_0_243_constant_201390 = load i32, i32* bitcast ([4 x i8]* @26 to i32*), align 4
  %1776 = xor i32 %1775, %region_0_243_constant_201390
  br label %concatenate.226.merge344

concat_index_from_operand_id1391:                 ; preds = %concatenate.pivot.1.524
  %1777 = phi i32 [ 1, %concatenate.pivot.1.524 ]
  %1778 = sub nsw i32 %1584, %1777
  %1779 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1780 = load i64, i64* %1779, align 8, !invariant.load !22
  %1781 = trunc i64 %1780 to i32
  %1782 = zext i32 %1781 to i64
  %1783 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1784 = lshr i64 %1780, %1783
  %shft.chk392 = icmp ult i64 %1783, 64
  %1785 = select i1 %shft.chk392, i64 %1784, i64 0
  %1786 = trunc i64 %1785 to i32
  %1787 = zext i32 %1786 to i64
  %1788 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1789 = shl i64 %1787, %1788
  %shft.chk393 = icmp ult i64 %1788, 64
  %1790 = select i1 %shft.chk393, i64 %1789, i64 0
  %1791 = or i64 %1782, %1790
  %1792 = mul nuw nsw i32 %1585, 1
  %1793 = add nuw nsw i32 0, %1792
  %1794 = zext i32 %1793 to i64
  %1795 = add i64 %1791, %1794
  %1796 = trunc i64 %1795 to i32
  %1797 = zext i32 %1796 to i64
  %region_0_243_constant_18394 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %1798 = mul i64 %1797, %region_0_243_constant_18394
  %1799 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1800 = lshr i64 %1798, %1799
  %shft.chk395 = icmp ult i64 %1799, 64
  %1801 = select i1 %shft.chk395, i64 %1800, i64 0
  %1802 = trunc i64 %1801 to i32
  %1803 = icmp ult i64 %1795, %1791
  %1804 = zext i1 %1803 to i8
  %1805 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1806 = load i64, i64* %1805, align 8, !invariant.load !22
  %1807 = trunc i64 %1806 to i32
  %1808 = zext i32 %1807 to i64
  %1809 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1810 = lshr i64 %1806, %1809
  %shft.chk396 = icmp ult i64 %1809, 64
  %1811 = select i1 %shft.chk396, i64 %1810, i64 0
  %1812 = trunc i64 %1811 to i32
  %1813 = zext i32 %1812 to i64
  %1814 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1815 = shl i64 %1813, %1814
  %shft.chk397 = icmp ult i64 %1814, 64
  %1816 = select i1 %shft.chk397, i64 %1815, i64 0
  %1817 = or i64 %1808, %1816
  %region_0_243_constant_34398 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1818 = add i64 %1817, %region_0_243_constant_34398
  %1819 = trunc i8 %1804 to i1
  %1820 = select i1 %1819, i64 %1818, i64 %1817
  %1821 = lshr i64 %1820, %1799
  %shft.chk399 = icmp ult i64 %1799, 64
  %1822 = select i1 %shft.chk399, i64 %1821, i64 0
  %1823 = trunc i64 %1822 to i32
  %1824 = xor i32 %1802, %1823
  %region_0_243_constant_42400 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %1825 = xor i32 %1824, %region_0_243_constant_42400
  %1826 = zext i32 %1825 to i64
  %region_0_243_constant_46401 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1827 = mul i64 %1826, %region_0_243_constant_46401
  %1828 = lshr i64 %1827, %1799
  %shft.chk402 = icmp ult i64 %1799, 64
  %1829 = select i1 %shft.chk402, i64 %1828, i64 0
  %1830 = trunc i64 %1829 to i32
  %1831 = trunc i64 %1820 to i32
  %1832 = zext i32 %1831 to i64
  %1833 = mul i64 %1832, %region_0_243_constant_46401
  %1834 = trunc i64 %1833 to i32
  %1835 = xor i32 %1830, %1834
  %region_0_243_constant_56403 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %1836 = xor i32 %1835, %region_0_243_constant_56403
  %1837 = zext i32 %1836 to i64
  %1838 = mul i64 %1837, %region_0_243_constant_18394
  %1839 = lshr i64 %1838, %1799
  %shft.chk404 = icmp ult i64 %1799, 64
  %1840 = select i1 %shft.chk404, i64 %1839, i64 0
  %1841 = trunc i64 %1840 to i32
  %1842 = lshr i64 %1833, %1799
  %shft.chk405 = icmp ult i64 %1799, 64
  %1843 = select i1 %shft.chk405, i64 %1842, i64 0
  %1844 = trunc i64 %1843 to i32
  %1845 = lshr i64 %1795, %1799
  %shft.chk406 = icmp ult i64 %1799, 64
  %1846 = select i1 %shft.chk406, i64 %1845, i64 0
  %1847 = trunc i64 %1846 to i32
  %1848 = xor i32 %1844, %1847
  %region_0_243_constant_68407 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %1849 = xor i32 %1848, %region_0_243_constant_68407
  %1850 = zext i32 %1849 to i64
  %1851 = mul i64 %1850, %region_0_243_constant_18394
  %1852 = trunc i64 %1851 to i32
  %1853 = xor i32 %1841, %1852
  %region_0_243_constant_75408 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %1854 = xor i32 %1853, %region_0_243_constant_75408
  %1855 = zext i32 %1854 to i64
  %1856 = mul i64 %1855, %region_0_243_constant_46401
  %1857 = lshr i64 %1856, %1799
  %shft.chk409 = icmp ult i64 %1799, 64
  %1858 = select i1 %shft.chk409, i64 %1857, i64 0
  %1859 = trunc i64 %1858 to i32
  %1860 = lshr i64 %1851, %1799
  %shft.chk410 = icmp ult i64 %1799, 64
  %1861 = select i1 %shft.chk410, i64 %1860, i64 0
  %1862 = trunc i64 %1861 to i32
  %1863 = trunc i64 %1798 to i32
  %1864 = xor i32 %1862, %1863
  %region_0_243_constant_86411 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %1865 = xor i32 %1864, %region_0_243_constant_86411
  %1866 = zext i32 %1865 to i64
  %1867 = mul i64 %1866, %region_0_243_constant_46401
  %1868 = trunc i64 %1867 to i32
  %1869 = xor i32 %1859, %1868
  %region_0_243_constant_93412 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %1870 = xor i32 %1869, %region_0_243_constant_93412
  %1871 = zext i32 %1870 to i64
  %1872 = mul i64 %1871, %region_0_243_constant_18394
  %1873 = lshr i64 %1872, %1799
  %shft.chk413 = icmp ult i64 %1799, 64
  %1874 = select i1 %shft.chk413, i64 %1873, i64 0
  %1875 = trunc i64 %1874 to i32
  %1876 = lshr i64 %1867, %1799
  %shft.chk414 = icmp ult i64 %1799, 64
  %1877 = select i1 %shft.chk414, i64 %1876, i64 0
  %1878 = trunc i64 %1877 to i32
  %1879 = trunc i64 %1827 to i32
  %1880 = xor i32 %1878, %1879
  %region_0_243_constant_104415 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %1881 = xor i32 %1880, %region_0_243_constant_104415
  %1882 = zext i32 %1881 to i64
  %1883 = mul i64 %1882, %region_0_243_constant_18394
  %1884 = trunc i64 %1883 to i32
  %1885 = xor i32 %1875, %1884
  %region_0_243_constant_111416 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1886 = xor i32 %1885, %region_0_243_constant_111416
  %1887 = zext i32 %1886 to i64
  %1888 = mul i64 %1887, %region_0_243_constant_46401
  %1889 = lshr i64 %1888, %1799
  %shft.chk417 = icmp ult i64 %1799, 64
  %1890 = select i1 %shft.chk417, i64 %1889, i64 0
  %1891 = trunc i64 %1890 to i32
  %1892 = lshr i64 %1883, %1799
  %shft.chk418 = icmp ult i64 %1799, 64
  %1893 = select i1 %shft.chk418, i64 %1892, i64 0
  %1894 = trunc i64 %1893 to i32
  %1895 = trunc i64 %1838 to i32
  %1896 = xor i32 %1894, %1895
  %region_0_243_constant_122419 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %1897 = xor i32 %1896, %region_0_243_constant_122419
  %1898 = zext i32 %1897 to i64
  %1899 = mul i64 %1898, %region_0_243_constant_46401
  %1900 = trunc i64 %1899 to i32
  %1901 = xor i32 %1891, %1900
  %region_0_243_constant_129420 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %1902 = xor i32 %1901, %region_0_243_constant_129420
  %1903 = zext i32 %1902 to i64
  %1904 = mul i64 %1903, %region_0_243_constant_18394
  %1905 = lshr i64 %1904, %1799
  %shft.chk421 = icmp ult i64 %1799, 64
  %1906 = select i1 %shft.chk421, i64 %1905, i64 0
  %1907 = trunc i64 %1906 to i32
  %1908 = lshr i64 %1899, %1799
  %shft.chk422 = icmp ult i64 %1799, 64
  %1909 = select i1 %shft.chk422, i64 %1908, i64 0
  %1910 = trunc i64 %1909 to i32
  %1911 = trunc i64 %1856 to i32
  %1912 = xor i32 %1910, %1911
  %region_0_243_constant_140423 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %1913 = xor i32 %1912, %region_0_243_constant_140423
  %1914 = zext i32 %1913 to i64
  %1915 = mul i64 %1914, %region_0_243_constant_18394
  %1916 = trunc i64 %1915 to i32
  %1917 = xor i32 %1907, %1916
  %region_0_243_constant_147424 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %1918 = xor i32 %1917, %region_0_243_constant_147424
  %1919 = zext i32 %1918 to i64
  %1920 = mul i64 %1919, %region_0_243_constant_46401
  %1921 = lshr i64 %1920, %1799
  %shft.chk425 = icmp ult i64 %1799, 64
  %1922 = select i1 %shft.chk425, i64 %1921, i64 0
  %1923 = trunc i64 %1922 to i32
  %1924 = lshr i64 %1915, %1799
  %shft.chk426 = icmp ult i64 %1799, 64
  %1925 = select i1 %shft.chk426, i64 %1924, i64 0
  %1926 = trunc i64 %1925 to i32
  %1927 = trunc i64 %1872 to i32
  %1928 = xor i32 %1926, %1927
  %region_0_243_constant_158427 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1929 = xor i32 %1928, %region_0_243_constant_158427
  %1930 = zext i32 %1929 to i64
  %1931 = mul i64 %1930, %region_0_243_constant_46401
  %1932 = trunc i64 %1931 to i32
  %1933 = xor i32 %1923, %1932
  %region_0_243_constant_165428 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1934 = xor i32 %1933, %region_0_243_constant_165428
  %1935 = zext i32 %1934 to i64
  %1936 = mul i64 %1935, %region_0_243_constant_18394
  %1937 = lshr i64 %1936, %1799
  %shft.chk429 = icmp ult i64 %1799, 64
  %1938 = select i1 %shft.chk429, i64 %1937, i64 0
  %1939 = trunc i64 %1938 to i32
  %1940 = lshr i64 %1931, %1799
  %shft.chk430 = icmp ult i64 %1799, 64
  %1941 = select i1 %shft.chk430, i64 %1940, i64 0
  %1942 = trunc i64 %1941 to i32
  %1943 = trunc i64 %1888 to i32
  %1944 = xor i32 %1942, %1943
  %region_0_243_constant_176431 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %1945 = xor i32 %1944, %region_0_243_constant_176431
  %1946 = zext i32 %1945 to i64
  %1947 = mul i64 %1946, %region_0_243_constant_18394
  %1948 = trunc i64 %1947 to i32
  %1949 = xor i32 %1939, %1948
  %region_0_243_constant_183432 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1950 = xor i32 %1949, %region_0_243_constant_183432
  %1951 = zext i32 %1950 to i64
  %1952 = mul i64 %1951, %region_0_243_constant_46401
  %1953 = trunc i64 %1952 to i32
  br label %concatenate.226.merge344

concat_index_from_operand_id2433:                 ; preds = %concatenate.pivot.2.526
  %1954 = phi i32 [ 2, %concatenate.pivot.2.526 ]
  %1955 = sub nsw i32 %1584, %1954
  %1956 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1957 = load i64, i64* %1956, align 8, !invariant.load !22
  %1958 = trunc i64 %1957 to i32
  %1959 = zext i32 %1958 to i64
  %1960 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1961 = lshr i64 %1957, %1960
  %shft.chk434 = icmp ult i64 %1960, 64
  %1962 = select i1 %shft.chk434, i64 %1961, i64 0
  %1963 = trunc i64 %1962 to i32
  %1964 = zext i32 %1963 to i64
  %1965 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1966 = shl i64 %1964, %1965
  %shft.chk435 = icmp ult i64 %1965, 64
  %1967 = select i1 %shft.chk435, i64 %1966, i64 0
  %1968 = or i64 %1959, %1967
  %1969 = mul nuw nsw i32 %1585, 1
  %1970 = add nuw nsw i32 0, %1969
  %1971 = zext i32 %1970 to i64
  %1972 = add i64 %1968, %1971
  %1973 = icmp ult i64 %1972, %1968
  %1974 = zext i1 %1973 to i8
  %1975 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1976 = load i64, i64* %1975, align 8, !invariant.load !22
  %1977 = trunc i64 %1976 to i32
  %1978 = zext i32 %1977 to i64
  %1979 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1980 = lshr i64 %1976, %1979
  %shft.chk436 = icmp ult i64 %1979, 64
  %1981 = select i1 %shft.chk436, i64 %1980, i64 0
  %1982 = trunc i64 %1981 to i32
  %1983 = zext i32 %1982 to i64
  %1984 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1985 = shl i64 %1983, %1984
  %shft.chk437 = icmp ult i64 %1984, 64
  %1986 = select i1 %shft.chk437, i64 %1985, i64 0
  %1987 = or i64 %1978, %1986
  %region_0_243_constant_34438 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %1988 = add i64 %1987, %region_0_243_constant_34438
  %1989 = trunc i8 %1974 to i1
  %1990 = select i1 %1989, i64 %1988, i64 %1987
  %1991 = trunc i64 %1990 to i32
  %1992 = zext i32 %1991 to i64
  %region_0_243_constant_46439 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %1993 = mul i64 %1992, %region_0_243_constant_46439
  %1994 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1995 = lshr i64 %1993, %1994
  %shft.chk440 = icmp ult i64 %1994, 64
  %1996 = select i1 %shft.chk440, i64 %1995, i64 0
  %1997 = trunc i64 %1996 to i32
  %1998 = lshr i64 %1972, %1994
  %shft.chk441 = icmp ult i64 %1994, 64
  %1999 = select i1 %shft.chk441, i64 %1998, i64 0
  %2000 = trunc i64 %1999 to i32
  %2001 = xor i32 %1997, %2000
  %region_0_243_constant_68442 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2002 = xor i32 %2001, %region_0_243_constant_68442
  %2003 = zext i32 %2002 to i64
  %region_0_243_constant_18443 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2004 = mul i64 %2003, %region_0_243_constant_18443
  %2005 = lshr i64 %2004, %1994
  %shft.chk444 = icmp ult i64 %1994, 64
  %2006 = select i1 %shft.chk444, i64 %2005, i64 0
  %2007 = trunc i64 %2006 to i32
  %2008 = trunc i64 %1972 to i32
  %2009 = zext i32 %2008 to i64
  %2010 = mul i64 %2009, %region_0_243_constant_18443
  %2011 = trunc i64 %2010 to i32
  %2012 = xor i32 %2007, %2011
  %region_0_243_constant_86445 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2013 = xor i32 %2012, %region_0_243_constant_86445
  %2014 = zext i32 %2013 to i64
  %2015 = mul i64 %2014, %region_0_243_constant_46439
  %2016 = lshr i64 %2015, %1994
  %shft.chk446 = icmp ult i64 %1994, 64
  %2017 = select i1 %shft.chk446, i64 %2016, i64 0
  %2018 = trunc i64 %2017 to i32
  %2019 = lshr i64 %2010, %1994
  %shft.chk447 = icmp ult i64 %1994, 64
  %2020 = select i1 %shft.chk447, i64 %2019, i64 0
  %2021 = trunc i64 %2020 to i32
  %2022 = lshr i64 %1990, %1994
  %shft.chk448 = icmp ult i64 %1994, 64
  %2023 = select i1 %shft.chk448, i64 %2022, i64 0
  %2024 = trunc i64 %2023 to i32
  %2025 = xor i32 %2021, %2024
  %region_0_243_constant_42449 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2026 = xor i32 %2025, %region_0_243_constant_42449
  %2027 = zext i32 %2026 to i64
  %2028 = mul i64 %2027, %region_0_243_constant_46439
  %2029 = trunc i64 %2028 to i32
  %2030 = xor i32 %2018, %2029
  %region_0_243_constant_104450 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2031 = xor i32 %2030, %region_0_243_constant_104450
  %2032 = zext i32 %2031 to i64
  %2033 = mul i64 %2032, %region_0_243_constant_18443
  %2034 = lshr i64 %2033, %1994
  %shft.chk451 = icmp ult i64 %1994, 64
  %2035 = select i1 %shft.chk451, i64 %2034, i64 0
  %2036 = trunc i64 %2035 to i32
  %2037 = lshr i64 %2028, %1994
  %shft.chk452 = icmp ult i64 %1994, 64
  %2038 = select i1 %shft.chk452, i64 %2037, i64 0
  %2039 = trunc i64 %2038 to i32
  %2040 = trunc i64 %1993 to i32
  %2041 = xor i32 %2039, %2040
  %region_0_243_constant_56453 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %2042 = xor i32 %2041, %region_0_243_constant_56453
  %2043 = zext i32 %2042 to i64
  %2044 = mul i64 %2043, %region_0_243_constant_18443
  %2045 = trunc i64 %2044 to i32
  %2046 = xor i32 %2036, %2045
  %region_0_243_constant_122454 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %2047 = xor i32 %2046, %region_0_243_constant_122454
  %2048 = zext i32 %2047 to i64
  %2049 = mul i64 %2048, %region_0_243_constant_46439
  %2050 = lshr i64 %2049, %1994
  %shft.chk455 = icmp ult i64 %1994, 64
  %2051 = select i1 %shft.chk455, i64 %2050, i64 0
  %2052 = trunc i64 %2051 to i32
  %2053 = lshr i64 %2044, %1994
  %shft.chk456 = icmp ult i64 %1994, 64
  %2054 = select i1 %shft.chk456, i64 %2053, i64 0
  %2055 = trunc i64 %2054 to i32
  %2056 = trunc i64 %2004 to i32
  %2057 = xor i32 %2055, %2056
  %region_0_243_constant_75457 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %2058 = xor i32 %2057, %region_0_243_constant_75457
  %2059 = zext i32 %2058 to i64
  %2060 = mul i64 %2059, %region_0_243_constant_46439
  %2061 = trunc i64 %2060 to i32
  %2062 = xor i32 %2052, %2061
  %region_0_243_constant_140458 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %2063 = xor i32 %2062, %region_0_243_constant_140458
  %2064 = zext i32 %2063 to i64
  %2065 = mul i64 %2064, %region_0_243_constant_18443
  %2066 = lshr i64 %2065, %1994
  %shft.chk459 = icmp ult i64 %1994, 64
  %2067 = select i1 %shft.chk459, i64 %2066, i64 0
  %2068 = trunc i64 %2067 to i32
  %2069 = lshr i64 %2060, %1994
  %shft.chk460 = icmp ult i64 %1994, 64
  %2070 = select i1 %shft.chk460, i64 %2069, i64 0
  %2071 = trunc i64 %2070 to i32
  %2072 = trunc i64 %2015 to i32
  %2073 = xor i32 %2071, %2072
  %region_0_243_constant_93461 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %2074 = xor i32 %2073, %region_0_243_constant_93461
  %2075 = zext i32 %2074 to i64
  %2076 = mul i64 %2075, %region_0_243_constant_18443
  %2077 = trunc i64 %2076 to i32
  %2078 = xor i32 %2068, %2077
  %region_0_243_constant_158462 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2079 = xor i32 %2078, %region_0_243_constant_158462
  %2080 = zext i32 %2079 to i64
  %2081 = mul i64 %2080, %region_0_243_constant_46439
  %2082 = lshr i64 %2081, %1994
  %shft.chk463 = icmp ult i64 %1994, 64
  %2083 = select i1 %shft.chk463, i64 %2082, i64 0
  %2084 = trunc i64 %2083 to i32
  %2085 = lshr i64 %2076, %1994
  %shft.chk464 = icmp ult i64 %1994, 64
  %2086 = select i1 %shft.chk464, i64 %2085, i64 0
  %2087 = trunc i64 %2086 to i32
  %2088 = trunc i64 %2033 to i32
  %2089 = xor i32 %2087, %2088
  %region_0_243_constant_111465 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2090 = xor i32 %2089, %region_0_243_constant_111465
  %2091 = zext i32 %2090 to i64
  %2092 = mul i64 %2091, %region_0_243_constant_46439
  %2093 = trunc i64 %2092 to i32
  %2094 = xor i32 %2084, %2093
  %region_0_243_constant_176466 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %2095 = xor i32 %2094, %region_0_243_constant_176466
  %2096 = zext i32 %2095 to i64
  %2097 = mul i64 %2096, %region_0_243_constant_18443
  %2098 = lshr i64 %2097, %1994
  %shft.chk467 = icmp ult i64 %1994, 64
  %2099 = select i1 %shft.chk467, i64 %2098, i64 0
  %2100 = trunc i64 %2099 to i32
  %2101 = lshr i64 %2092, %1994
  %shft.chk468 = icmp ult i64 %1994, 64
  %2102 = select i1 %shft.chk468, i64 %2101, i64 0
  %2103 = trunc i64 %2102 to i32
  %2104 = trunc i64 %2049 to i32
  %2105 = xor i32 %2103, %2104
  %region_0_243_constant_129469 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %2106 = xor i32 %2105, %region_0_243_constant_129469
  %2107 = zext i32 %2106 to i64
  %2108 = mul i64 %2107, %region_0_243_constant_18443
  %2109 = trunc i64 %2108 to i32
  %2110 = xor i32 %2100, %2109
  %region_0_243_constant_194470 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2111 = xor i32 %2110, %region_0_243_constant_194470
  %2112 = zext i32 %2111 to i64
  %2113 = mul i64 %2112, %region_0_243_constant_46439
  %2114 = lshr i64 %2113, %1994
  %shft.chk471 = icmp ult i64 %1994, 64
  %2115 = select i1 %shft.chk471, i64 %2114, i64 0
  %2116 = trunc i64 %2115 to i32
  %2117 = lshr i64 %2108, %1994
  %shft.chk472 = icmp ult i64 %1994, 64
  %2118 = select i1 %shft.chk472, i64 %2117, i64 0
  %2119 = trunc i64 %2118 to i32
  %2120 = trunc i64 %2065 to i32
  %2121 = xor i32 %2119, %2120
  %region_0_243_constant_147473 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %2122 = xor i32 %2121, %region_0_243_constant_147473
  %2123 = zext i32 %2122 to i64
  %2124 = mul i64 %2123, %region_0_243_constant_46439
  %2125 = trunc i64 %2124 to i32
  %2126 = xor i32 %2116, %2125
  %region_0_243_constant_211474 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %2127 = xor i32 %2126, %region_0_243_constant_211474
  %2128 = zext i32 %2127 to i64
  %2129 = mul i64 %2128, %region_0_243_constant_18443
  %2130 = lshr i64 %2129, %1994
  %shft.chk475 = icmp ult i64 %1994, 64
  %2131 = select i1 %shft.chk475, i64 %2130, i64 0
  %2132 = trunc i64 %2131 to i32
  %2133 = lshr i64 %2124, %1994
  %shft.chk476 = icmp ult i64 %1994, 64
  %2134 = select i1 %shft.chk476, i64 %2133, i64 0
  %2135 = trunc i64 %2134 to i32
  %2136 = trunc i64 %2081 to i32
  %2137 = xor i32 %2135, %2136
  %region_0_243_constant_165477 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2138 = xor i32 %2137, %region_0_243_constant_165477
  %2139 = zext i32 %2138 to i64
  %2140 = mul i64 %2139, %region_0_243_constant_18443
  %2141 = trunc i64 %2140 to i32
  %2142 = xor i32 %2132, %2141
  %region_0_243_constant_220478 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2143 = xor i32 %2142, %region_0_243_constant_220478
  br label %concatenate.226.merge344

concat_index_from_operand_id3479:                 ; preds = %concatenate.pivot.3.527
  %2144 = phi i32 [ 3, %concatenate.pivot.3.527 ]
  %2145 = sub nsw i32 %1584, %2144
  %2146 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2147 = load i64, i64* %2146, align 8, !invariant.load !22
  %2148 = trunc i64 %2147 to i32
  %2149 = zext i32 %2148 to i64
  %2150 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2151 = lshr i64 %2147, %2150
  %shft.chk480 = icmp ult i64 %2150, 64
  %2152 = select i1 %shft.chk480, i64 %2151, i64 0
  %2153 = trunc i64 %2152 to i32
  %2154 = zext i32 %2153 to i64
  %2155 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2156 = shl i64 %2154, %2155
  %shft.chk481 = icmp ult i64 %2155, 64
  %2157 = select i1 %shft.chk481, i64 %2156, i64 0
  %2158 = or i64 %2149, %2157
  %2159 = mul nuw nsw i32 %1585, 1
  %2160 = add nuw nsw i32 0, %2159
  %2161 = zext i32 %2160 to i64
  %2162 = add i64 %2158, %2161
  %2163 = icmp ult i64 %2162, %2158
  %2164 = zext i1 %2163 to i8
  %2165 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2166 = load i64, i64* %2165, align 8, !invariant.load !22
  %2167 = trunc i64 %2166 to i32
  %2168 = zext i32 %2167 to i64
  %2169 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2170 = lshr i64 %2166, %2169
  %shft.chk482 = icmp ult i64 %2169, 64
  %2171 = select i1 %shft.chk482, i64 %2170, i64 0
  %2172 = trunc i64 %2171 to i32
  %2173 = zext i32 %2172 to i64
  %2174 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2175 = shl i64 %2173, %2174
  %shft.chk483 = icmp ult i64 %2174, 64
  %2176 = select i1 %shft.chk483, i64 %2175, i64 0
  %2177 = or i64 %2168, %2176
  %region_0_243_constant_34484 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %2178 = add i64 %2177, %region_0_243_constant_34484
  %2179 = trunc i8 %2164 to i1
  %2180 = select i1 %2179, i64 %2178, i64 %2177
  %2181 = trunc i64 %2180 to i32
  %2182 = zext i32 %2181 to i64
  %region_0_243_constant_46485 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %2183 = mul i64 %2182, %region_0_243_constant_46485
  %2184 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2185 = lshr i64 %2183, %2184
  %shft.chk486 = icmp ult i64 %2184, 64
  %2186 = select i1 %shft.chk486, i64 %2185, i64 0
  %2187 = trunc i64 %2186 to i32
  %2188 = lshr i64 %2162, %2184
  %shft.chk487 = icmp ult i64 %2184, 64
  %2189 = select i1 %shft.chk487, i64 %2188, i64 0
  %2190 = trunc i64 %2189 to i32
  %2191 = xor i32 %2187, %2190
  %region_0_243_constant_68488 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2192 = xor i32 %2191, %region_0_243_constant_68488
  %2193 = zext i32 %2192 to i64
  %region_0_243_constant_18489 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2194 = mul i64 %2193, %region_0_243_constant_18489
  %2195 = lshr i64 %2194, %2184
  %shft.chk490 = icmp ult i64 %2184, 64
  %2196 = select i1 %shft.chk490, i64 %2195, i64 0
  %2197 = trunc i64 %2196 to i32
  %2198 = trunc i64 %2162 to i32
  %2199 = zext i32 %2198 to i64
  %2200 = mul i64 %2199, %region_0_243_constant_18489
  %2201 = trunc i64 %2200 to i32
  %2202 = xor i32 %2197, %2201
  %region_0_243_constant_86491 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2203 = xor i32 %2202, %region_0_243_constant_86491
  %2204 = zext i32 %2203 to i64
  %2205 = mul i64 %2204, %region_0_243_constant_46485
  %2206 = lshr i64 %2205, %2184
  %shft.chk492 = icmp ult i64 %2184, 64
  %2207 = select i1 %shft.chk492, i64 %2206, i64 0
  %2208 = trunc i64 %2207 to i32
  %2209 = lshr i64 %2200, %2184
  %shft.chk493 = icmp ult i64 %2184, 64
  %2210 = select i1 %shft.chk493, i64 %2209, i64 0
  %2211 = trunc i64 %2210 to i32
  %2212 = lshr i64 %2180, %2184
  %shft.chk494 = icmp ult i64 %2184, 64
  %2213 = select i1 %shft.chk494, i64 %2212, i64 0
  %2214 = trunc i64 %2213 to i32
  %2215 = xor i32 %2211, %2214
  %region_0_243_constant_42495 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2216 = xor i32 %2215, %region_0_243_constant_42495
  %2217 = zext i32 %2216 to i64
  %2218 = mul i64 %2217, %region_0_243_constant_46485
  %2219 = trunc i64 %2218 to i32
  %2220 = xor i32 %2208, %2219
  %region_0_243_constant_104496 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2221 = xor i32 %2220, %region_0_243_constant_104496
  %2222 = zext i32 %2221 to i64
  %2223 = mul i64 %2222, %region_0_243_constant_18489
  %2224 = lshr i64 %2223, %2184
  %shft.chk497 = icmp ult i64 %2184, 64
  %2225 = select i1 %shft.chk497, i64 %2224, i64 0
  %2226 = trunc i64 %2225 to i32
  %2227 = lshr i64 %2218, %2184
  %shft.chk498 = icmp ult i64 %2184, 64
  %2228 = select i1 %shft.chk498, i64 %2227, i64 0
  %2229 = trunc i64 %2228 to i32
  %2230 = trunc i64 %2183 to i32
  %2231 = xor i32 %2229, %2230
  %region_0_243_constant_56499 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %2232 = xor i32 %2231, %region_0_243_constant_56499
  %2233 = zext i32 %2232 to i64
  %2234 = mul i64 %2233, %region_0_243_constant_18489
  %2235 = trunc i64 %2234 to i32
  %2236 = xor i32 %2226, %2235
  %region_0_243_constant_122500 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %2237 = xor i32 %2236, %region_0_243_constant_122500
  %2238 = zext i32 %2237 to i64
  %2239 = mul i64 %2238, %region_0_243_constant_46485
  %2240 = lshr i64 %2239, %2184
  %shft.chk501 = icmp ult i64 %2184, 64
  %2241 = select i1 %shft.chk501, i64 %2240, i64 0
  %2242 = trunc i64 %2241 to i32
  %2243 = lshr i64 %2234, %2184
  %shft.chk502 = icmp ult i64 %2184, 64
  %2244 = select i1 %shft.chk502, i64 %2243, i64 0
  %2245 = trunc i64 %2244 to i32
  %2246 = trunc i64 %2194 to i32
  %2247 = xor i32 %2245, %2246
  %region_0_243_constant_75503 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %2248 = xor i32 %2247, %region_0_243_constant_75503
  %2249 = zext i32 %2248 to i64
  %2250 = mul i64 %2249, %region_0_243_constant_46485
  %2251 = trunc i64 %2250 to i32
  %2252 = xor i32 %2242, %2251
  %region_0_243_constant_140504 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %2253 = xor i32 %2252, %region_0_243_constant_140504
  %2254 = zext i32 %2253 to i64
  %2255 = mul i64 %2254, %region_0_243_constant_18489
  %2256 = lshr i64 %2255, %2184
  %shft.chk505 = icmp ult i64 %2184, 64
  %2257 = select i1 %shft.chk505, i64 %2256, i64 0
  %2258 = trunc i64 %2257 to i32
  %2259 = lshr i64 %2250, %2184
  %shft.chk506 = icmp ult i64 %2184, 64
  %2260 = select i1 %shft.chk506, i64 %2259, i64 0
  %2261 = trunc i64 %2260 to i32
  %2262 = trunc i64 %2205 to i32
  %2263 = xor i32 %2261, %2262
  %region_0_243_constant_93507 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %2264 = xor i32 %2263, %region_0_243_constant_93507
  %2265 = zext i32 %2264 to i64
  %2266 = mul i64 %2265, %region_0_243_constant_18489
  %2267 = trunc i64 %2266 to i32
  %2268 = xor i32 %2258, %2267
  %region_0_243_constant_158508 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2269 = xor i32 %2268, %region_0_243_constant_158508
  %2270 = zext i32 %2269 to i64
  %2271 = mul i64 %2270, %region_0_243_constant_46485
  %2272 = lshr i64 %2271, %2184
  %shft.chk509 = icmp ult i64 %2184, 64
  %2273 = select i1 %shft.chk509, i64 %2272, i64 0
  %2274 = trunc i64 %2273 to i32
  %2275 = lshr i64 %2266, %2184
  %shft.chk510 = icmp ult i64 %2184, 64
  %2276 = select i1 %shft.chk510, i64 %2275, i64 0
  %2277 = trunc i64 %2276 to i32
  %2278 = trunc i64 %2223 to i32
  %2279 = xor i32 %2277, %2278
  %region_0_243_constant_111511 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2280 = xor i32 %2279, %region_0_243_constant_111511
  %2281 = zext i32 %2280 to i64
  %2282 = mul i64 %2281, %region_0_243_constant_46485
  %2283 = trunc i64 %2282 to i32
  %2284 = xor i32 %2274, %2283
  %region_0_243_constant_176512 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %2285 = xor i32 %2284, %region_0_243_constant_176512
  %2286 = zext i32 %2285 to i64
  %2287 = mul i64 %2286, %region_0_243_constant_18489
  %2288 = lshr i64 %2287, %2184
  %shft.chk513 = icmp ult i64 %2184, 64
  %2289 = select i1 %shft.chk513, i64 %2288, i64 0
  %2290 = trunc i64 %2289 to i32
  %2291 = lshr i64 %2282, %2184
  %shft.chk514 = icmp ult i64 %2184, 64
  %2292 = select i1 %shft.chk514, i64 %2291, i64 0
  %2293 = trunc i64 %2292 to i32
  %2294 = trunc i64 %2239 to i32
  %2295 = xor i32 %2293, %2294
  %region_0_243_constant_129515 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %2296 = xor i32 %2295, %region_0_243_constant_129515
  %2297 = zext i32 %2296 to i64
  %2298 = mul i64 %2297, %region_0_243_constant_18489
  %2299 = trunc i64 %2298 to i32
  %2300 = xor i32 %2290, %2299
  %region_0_243_constant_194516 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2301 = xor i32 %2300, %region_0_243_constant_194516
  %2302 = zext i32 %2301 to i64
  %2303 = mul i64 %2302, %region_0_243_constant_46485
  %2304 = lshr i64 %2303, %2184
  %shft.chk517 = icmp ult i64 %2184, 64
  %2305 = select i1 %shft.chk517, i64 %2304, i64 0
  %2306 = trunc i64 %2305 to i32
  %2307 = lshr i64 %2298, %2184
  %shft.chk518 = icmp ult i64 %2184, 64
  %2308 = select i1 %shft.chk518, i64 %2307, i64 0
  %2309 = trunc i64 %2308 to i32
  %2310 = trunc i64 %2255 to i32
  %2311 = xor i32 %2309, %2310
  %region_0_243_constant_147519 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %2312 = xor i32 %2311, %region_0_243_constant_147519
  %2313 = zext i32 %2312 to i64
  %2314 = mul i64 %2313, %region_0_243_constant_46485
  %2315 = trunc i64 %2314 to i32
  %2316 = xor i32 %2306, %2315
  %region_0_243_constant_211520 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %2317 = xor i32 %2316, %region_0_243_constant_211520
  %2318 = zext i32 %2317 to i64
  %2319 = mul i64 %2318, %region_0_243_constant_18489
  %2320 = trunc i64 %2319 to i32
  br label %concatenate.226.merge344

concatenate.pivot.2.521:                          ; preds = %concatenate.226.merge153
  %2321 = icmp ult i32 %1584, 2
  br i1 %2321, label %concatenate.pivot.1.522, label %concatenate.pivot.3.525

concatenate.pivot.1.522:                          ; preds = %concatenate.pivot.2.521
  %2322 = icmp ult i32 %1584, 1
  br i1 %2322, label %concatenate.pivot.0.523, label %concatenate.pivot.1.524

concatenate.pivot.0.523:                          ; preds = %concatenate.pivot.1.522
  br label %concat_index_from_operand_id0345

concatenate.pivot.1.524:                          ; preds = %concatenate.pivot.1.522
  br label %concat_index_from_operand_id1391

concatenate.pivot.3.525:                          ; preds = %concatenate.pivot.2.521
  %2323 = icmp ult i32 %1584, 3
  br i1 %2323, label %concatenate.pivot.2.526, label %concatenate.pivot.3.527

concatenate.pivot.2.526:                          ; preds = %concatenate.pivot.3.525
  br label %concat_index_from_operand_id2433

concatenate.pivot.3.527:                          ; preds = %concatenate.pivot.3.525
  br label %concat_index_from_operand_id3479

concatenate.226.merge344:                         ; preds = %concat_index_from_operand_id3479, %concat_index_from_operand_id2433, %concat_index_from_operand_id1391, %concat_index_from_operand_id0345
  %2324 = phi i32 [ %1776, %concat_index_from_operand_id0345 ], [ %1953, %concat_index_from_operand_id1391 ], [ %2143, %concat_index_from_operand_id2433 ], [ %2320, %concat_index_from_operand_id3479 ]
  %region_0_243_constant_227528 = load i32, i32* bitcast ([4 x i8]* @3 to i32*), align 4
  %2325 = lshr i32 %2324, %region_0_243_constant_227528
  %shft.chk529 = icmp ult i32 %region_0_243_constant_227528, 32
  %2326 = select i1 %shft.chk529, i32 %2325, i32 0
  %2327 = uitofp i32 %2326 to float
  %region_0_243_constant_231530 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %multiply.233531 = fmul float %2327, %region_0_243_constant_231530
  %region_0_243_constant_234532 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %compare.236533 = fcmp olt float %multiply.233531, %region_0_243_constant_234532
  %2328 = zext i1 %compare.236533 to i8
  %2329 = mul nuw nsw i32 %1571, 1
  %2330 = add nuw nsw i32 0, %2329
  %2331 = udiv i32 %2330, 16
  %2332 = mul nuw nsw i32 %1575, 1
  %2333 = add nuw nsw i32 0, %2332
  %2334 = mul nuw nsw i32 %1576, 32
  %2335 = add nuw nsw i32 %2333, %2334
  %2336 = udiv i32 %2335, 256
  %2337 = bitcast [256 x [16 x float]]* %1 to float*
  %2338 = getelementptr inbounds float, float* %2337, i32 %linear_index2
  %2339 = load float, float* %2338, align 4, !invariant.load !22
  %region_0_243_constant_239534 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %2340 = trunc i8 %2328 to i1
  %2341 = select i1 %2340, float %2339, float %region_0_243_constant_239534
  %2342 = bitcast [256 x [16 x float]]* %5 to float*
  %2343 = getelementptr inbounds float, float* %2342, i32 %linear_index2
  store float %2341, float* %2343, align 4
  %2344 = mul nuw nsw i32 %19, 1
  %2345 = add nuw nsw i32 0, %2344
  %2346 = udiv i32 %2345, 16
  %2347 = mul nuw nsw i32 %20, 1
  %2348 = add nuw nsw i32 0, %2347
  %2349 = urem i32 %2348, 32
  %2350 = udiv i32 %2348, 32
  %2351 = udiv i32 %2350, 8
  %2352 = mul nuw nsw i32 %2345, 1
  %2353 = add nuw nsw i32 0, %2352
  %2354 = mul nuw nsw i32 %2349, 16
  %2355 = add nuw nsw i32 %2353, %2354
  %2356 = mul nuw nsw i32 %2350, 512
  %2357 = add nuw nsw i32 %2355, %2356
  %2358 = urem i32 %2357, 4
  %2359 = udiv i32 %2357, 4
  %2360 = udiv i32 %2359, 1024
  br label %concatenate.pivot.2.712

concat_index_from_operand_id0536:                 ; preds = %concatenate.pivot.0.714
  %2361 = phi i32 [ 0, %concatenate.pivot.0.714 ]
  %2362 = sub nsw i32 %2358, %2361
  %2363 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2364 = load i64, i64* %2363, align 8, !invariant.load !22
  %2365 = trunc i64 %2364 to i32
  %2366 = zext i32 %2365 to i64
  %2367 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2368 = lshr i64 %2364, %2367
  %shft.chk537 = icmp ult i64 %2367, 64
  %2369 = select i1 %shft.chk537, i64 %2368, i64 0
  %2370 = trunc i64 %2369 to i32
  %2371 = zext i32 %2370 to i64
  %2372 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2373 = shl i64 %2371, %2372
  %shft.chk538 = icmp ult i64 %2372, 64
  %2374 = select i1 %shft.chk538, i64 %2373, i64 0
  %2375 = or i64 %2366, %2374
  %2376 = mul nuw nsw i32 %2359, 1
  %2377 = add nuw nsw i32 0, %2376
  %2378 = zext i32 %2377 to i64
  %2379 = add i64 %2375, %2378
  %2380 = trunc i64 %2379 to i32
  %2381 = zext i32 %2380 to i64
  %region_0_243_constant_18539 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2382 = mul i64 %2381, %region_0_243_constant_18539
  %2383 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2384 = lshr i64 %2382, %2383
  %shft.chk540 = icmp ult i64 %2383, 64
  %2385 = select i1 %shft.chk540, i64 %2384, i64 0
  %2386 = trunc i64 %2385 to i32
  %2387 = icmp ult i64 %2379, %2375
  %2388 = zext i1 %2387 to i8
  %2389 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2390 = load i64, i64* %2389, align 8, !invariant.load !22
  %2391 = trunc i64 %2390 to i32
  %2392 = zext i32 %2391 to i64
  %2393 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2394 = lshr i64 %2390, %2393
  %shft.chk541 = icmp ult i64 %2393, 64
  %2395 = select i1 %shft.chk541, i64 %2394, i64 0
  %2396 = trunc i64 %2395 to i32
  %2397 = zext i32 %2396 to i64
  %2398 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2399 = shl i64 %2397, %2398
  %shft.chk542 = icmp ult i64 %2398, 64
  %2400 = select i1 %shft.chk542, i64 %2399, i64 0
  %2401 = or i64 %2392, %2400
  %region_0_243_constant_34543 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %2402 = add i64 %2401, %region_0_243_constant_34543
  %2403 = trunc i8 %2388 to i1
  %2404 = select i1 %2403, i64 %2402, i64 %2401
  %2405 = lshr i64 %2404, %2383
  %shft.chk544 = icmp ult i64 %2383, 64
  %2406 = select i1 %shft.chk544, i64 %2405, i64 0
  %2407 = trunc i64 %2406 to i32
  %2408 = xor i32 %2386, %2407
  %region_0_243_constant_42545 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2409 = xor i32 %2408, %region_0_243_constant_42545
  %2410 = zext i32 %2409 to i64
  %region_0_243_constant_46546 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %2411 = mul i64 %2410, %region_0_243_constant_46546
  %2412 = lshr i64 %2411, %2383
  %shft.chk547 = icmp ult i64 %2383, 64
  %2413 = select i1 %shft.chk547, i64 %2412, i64 0
  %2414 = trunc i64 %2413 to i32
  %2415 = trunc i64 %2404 to i32
  %2416 = zext i32 %2415 to i64
  %2417 = mul i64 %2416, %region_0_243_constant_46546
  %2418 = trunc i64 %2417 to i32
  %2419 = xor i32 %2414, %2418
  %region_0_243_constant_56548 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %2420 = xor i32 %2419, %region_0_243_constant_56548
  %2421 = zext i32 %2420 to i64
  %2422 = mul i64 %2421, %region_0_243_constant_18539
  %2423 = lshr i64 %2422, %2383
  %shft.chk549 = icmp ult i64 %2383, 64
  %2424 = select i1 %shft.chk549, i64 %2423, i64 0
  %2425 = trunc i64 %2424 to i32
  %2426 = lshr i64 %2417, %2383
  %shft.chk550 = icmp ult i64 %2383, 64
  %2427 = select i1 %shft.chk550, i64 %2426, i64 0
  %2428 = trunc i64 %2427 to i32
  %2429 = lshr i64 %2379, %2383
  %shft.chk551 = icmp ult i64 %2383, 64
  %2430 = select i1 %shft.chk551, i64 %2429, i64 0
  %2431 = trunc i64 %2430 to i32
  %2432 = xor i32 %2428, %2431
  %region_0_243_constant_68552 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2433 = xor i32 %2432, %region_0_243_constant_68552
  %2434 = zext i32 %2433 to i64
  %2435 = mul i64 %2434, %region_0_243_constant_18539
  %2436 = trunc i64 %2435 to i32
  %2437 = xor i32 %2425, %2436
  %region_0_243_constant_75553 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %2438 = xor i32 %2437, %region_0_243_constant_75553
  %2439 = zext i32 %2438 to i64
  %2440 = mul i64 %2439, %region_0_243_constant_46546
  %2441 = lshr i64 %2440, %2383
  %shft.chk554 = icmp ult i64 %2383, 64
  %2442 = select i1 %shft.chk554, i64 %2441, i64 0
  %2443 = trunc i64 %2442 to i32
  %2444 = lshr i64 %2435, %2383
  %shft.chk555 = icmp ult i64 %2383, 64
  %2445 = select i1 %shft.chk555, i64 %2444, i64 0
  %2446 = trunc i64 %2445 to i32
  %2447 = trunc i64 %2382 to i32
  %2448 = xor i32 %2446, %2447
  %region_0_243_constant_86556 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2449 = xor i32 %2448, %region_0_243_constant_86556
  %2450 = zext i32 %2449 to i64
  %2451 = mul i64 %2450, %region_0_243_constant_46546
  %2452 = trunc i64 %2451 to i32
  %2453 = xor i32 %2443, %2452
  %region_0_243_constant_93557 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %2454 = xor i32 %2453, %region_0_243_constant_93557
  %2455 = zext i32 %2454 to i64
  %2456 = mul i64 %2455, %region_0_243_constant_18539
  %2457 = lshr i64 %2456, %2383
  %shft.chk558 = icmp ult i64 %2383, 64
  %2458 = select i1 %shft.chk558, i64 %2457, i64 0
  %2459 = trunc i64 %2458 to i32
  %2460 = lshr i64 %2451, %2383
  %shft.chk559 = icmp ult i64 %2383, 64
  %2461 = select i1 %shft.chk559, i64 %2460, i64 0
  %2462 = trunc i64 %2461 to i32
  %2463 = trunc i64 %2411 to i32
  %2464 = xor i32 %2462, %2463
  %region_0_243_constant_104560 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2465 = xor i32 %2464, %region_0_243_constant_104560
  %2466 = zext i32 %2465 to i64
  %2467 = mul i64 %2466, %region_0_243_constant_18539
  %2468 = trunc i64 %2467 to i32
  %2469 = xor i32 %2459, %2468
  %region_0_243_constant_111561 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2470 = xor i32 %2469, %region_0_243_constant_111561
  %2471 = zext i32 %2470 to i64
  %2472 = mul i64 %2471, %region_0_243_constant_46546
  %2473 = lshr i64 %2472, %2383
  %shft.chk562 = icmp ult i64 %2383, 64
  %2474 = select i1 %shft.chk562, i64 %2473, i64 0
  %2475 = trunc i64 %2474 to i32
  %2476 = lshr i64 %2467, %2383
  %shft.chk563 = icmp ult i64 %2383, 64
  %2477 = select i1 %shft.chk563, i64 %2476, i64 0
  %2478 = trunc i64 %2477 to i32
  %2479 = trunc i64 %2422 to i32
  %2480 = xor i32 %2478, %2479
  %region_0_243_constant_122564 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %2481 = xor i32 %2480, %region_0_243_constant_122564
  %2482 = zext i32 %2481 to i64
  %2483 = mul i64 %2482, %region_0_243_constant_46546
  %2484 = trunc i64 %2483 to i32
  %2485 = xor i32 %2475, %2484
  %region_0_243_constant_129565 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %2486 = xor i32 %2485, %region_0_243_constant_129565
  %2487 = zext i32 %2486 to i64
  %2488 = mul i64 %2487, %region_0_243_constant_18539
  %2489 = lshr i64 %2488, %2383
  %shft.chk566 = icmp ult i64 %2383, 64
  %2490 = select i1 %shft.chk566, i64 %2489, i64 0
  %2491 = trunc i64 %2490 to i32
  %2492 = lshr i64 %2483, %2383
  %shft.chk567 = icmp ult i64 %2383, 64
  %2493 = select i1 %shft.chk567, i64 %2492, i64 0
  %2494 = trunc i64 %2493 to i32
  %2495 = trunc i64 %2440 to i32
  %2496 = xor i32 %2494, %2495
  %region_0_243_constant_140568 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %2497 = xor i32 %2496, %region_0_243_constant_140568
  %2498 = zext i32 %2497 to i64
  %2499 = mul i64 %2498, %region_0_243_constant_18539
  %2500 = trunc i64 %2499 to i32
  %2501 = xor i32 %2491, %2500
  %region_0_243_constant_147569 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %2502 = xor i32 %2501, %region_0_243_constant_147569
  %2503 = zext i32 %2502 to i64
  %2504 = mul i64 %2503, %region_0_243_constant_46546
  %2505 = lshr i64 %2504, %2383
  %shft.chk570 = icmp ult i64 %2383, 64
  %2506 = select i1 %shft.chk570, i64 %2505, i64 0
  %2507 = trunc i64 %2506 to i32
  %2508 = lshr i64 %2499, %2383
  %shft.chk571 = icmp ult i64 %2383, 64
  %2509 = select i1 %shft.chk571, i64 %2508, i64 0
  %2510 = trunc i64 %2509 to i32
  %2511 = trunc i64 %2456 to i32
  %2512 = xor i32 %2510, %2511
  %region_0_243_constant_158572 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2513 = xor i32 %2512, %region_0_243_constant_158572
  %2514 = zext i32 %2513 to i64
  %2515 = mul i64 %2514, %region_0_243_constant_46546
  %2516 = trunc i64 %2515 to i32
  %2517 = xor i32 %2507, %2516
  %region_0_243_constant_165573 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2518 = xor i32 %2517, %region_0_243_constant_165573
  %2519 = zext i32 %2518 to i64
  %2520 = mul i64 %2519, %region_0_243_constant_18539
  %2521 = lshr i64 %2520, %2383
  %shft.chk574 = icmp ult i64 %2383, 64
  %2522 = select i1 %shft.chk574, i64 %2521, i64 0
  %2523 = trunc i64 %2522 to i32
  %2524 = lshr i64 %2515, %2383
  %shft.chk575 = icmp ult i64 %2383, 64
  %2525 = select i1 %shft.chk575, i64 %2524, i64 0
  %2526 = trunc i64 %2525 to i32
  %2527 = trunc i64 %2472 to i32
  %2528 = xor i32 %2526, %2527
  %region_0_243_constant_176576 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %2529 = xor i32 %2528, %region_0_243_constant_176576
  %2530 = zext i32 %2529 to i64
  %2531 = mul i64 %2530, %region_0_243_constant_18539
  %2532 = trunc i64 %2531 to i32
  %2533 = xor i32 %2523, %2532
  %region_0_243_constant_183577 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2534 = xor i32 %2533, %region_0_243_constant_183577
  %2535 = zext i32 %2534 to i64
  %2536 = mul i64 %2535, %region_0_243_constant_46546
  %2537 = lshr i64 %2536, %2383
  %shft.chk578 = icmp ult i64 %2383, 64
  %2538 = select i1 %shft.chk578, i64 %2537, i64 0
  %2539 = trunc i64 %2538 to i32
  %2540 = lshr i64 %2531, %2383
  %shft.chk579 = icmp ult i64 %2383, 64
  %2541 = select i1 %shft.chk579, i64 %2540, i64 0
  %2542 = trunc i64 %2541 to i32
  %2543 = trunc i64 %2488 to i32
  %2544 = xor i32 %2542, %2543
  %region_0_243_constant_194580 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2545 = xor i32 %2544, %region_0_243_constant_194580
  %2546 = zext i32 %2545 to i64
  %2547 = mul i64 %2546, %region_0_243_constant_46546
  %2548 = trunc i64 %2547 to i32
  %2549 = xor i32 %2539, %2548
  %region_0_243_constant_201581 = load i32, i32* bitcast ([4 x i8]* @26 to i32*), align 4
  %2550 = xor i32 %2549, %region_0_243_constant_201581
  br label %concatenate.226.merge535

concat_index_from_operand_id1582:                 ; preds = %concatenate.pivot.1.715
  %2551 = phi i32 [ 1, %concatenate.pivot.1.715 ]
  %2552 = sub nsw i32 %2358, %2551
  %2553 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2554 = load i64, i64* %2553, align 8, !invariant.load !22
  %2555 = trunc i64 %2554 to i32
  %2556 = zext i32 %2555 to i64
  %2557 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2558 = lshr i64 %2554, %2557
  %shft.chk583 = icmp ult i64 %2557, 64
  %2559 = select i1 %shft.chk583, i64 %2558, i64 0
  %2560 = trunc i64 %2559 to i32
  %2561 = zext i32 %2560 to i64
  %2562 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2563 = shl i64 %2561, %2562
  %shft.chk584 = icmp ult i64 %2562, 64
  %2564 = select i1 %shft.chk584, i64 %2563, i64 0
  %2565 = or i64 %2556, %2564
  %2566 = mul nuw nsw i32 %2359, 1
  %2567 = add nuw nsw i32 0, %2566
  %2568 = zext i32 %2567 to i64
  %2569 = add i64 %2565, %2568
  %2570 = trunc i64 %2569 to i32
  %2571 = zext i32 %2570 to i64
  %region_0_243_constant_18585 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2572 = mul i64 %2571, %region_0_243_constant_18585
  %2573 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2574 = lshr i64 %2572, %2573
  %shft.chk586 = icmp ult i64 %2573, 64
  %2575 = select i1 %shft.chk586, i64 %2574, i64 0
  %2576 = trunc i64 %2575 to i32
  %2577 = icmp ult i64 %2569, %2565
  %2578 = zext i1 %2577 to i8
  %2579 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2580 = load i64, i64* %2579, align 8, !invariant.load !22
  %2581 = trunc i64 %2580 to i32
  %2582 = zext i32 %2581 to i64
  %2583 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2584 = lshr i64 %2580, %2583
  %shft.chk587 = icmp ult i64 %2583, 64
  %2585 = select i1 %shft.chk587, i64 %2584, i64 0
  %2586 = trunc i64 %2585 to i32
  %2587 = zext i32 %2586 to i64
  %2588 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2589 = shl i64 %2587, %2588
  %shft.chk588 = icmp ult i64 %2588, 64
  %2590 = select i1 %shft.chk588, i64 %2589, i64 0
  %2591 = or i64 %2582, %2590
  %region_0_243_constant_34589 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %2592 = add i64 %2591, %region_0_243_constant_34589
  %2593 = trunc i8 %2578 to i1
  %2594 = select i1 %2593, i64 %2592, i64 %2591
  %2595 = lshr i64 %2594, %2573
  %shft.chk590 = icmp ult i64 %2573, 64
  %2596 = select i1 %shft.chk590, i64 %2595, i64 0
  %2597 = trunc i64 %2596 to i32
  %2598 = xor i32 %2576, %2597
  %region_0_243_constant_42591 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2599 = xor i32 %2598, %region_0_243_constant_42591
  %2600 = zext i32 %2599 to i64
  %region_0_243_constant_46592 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %2601 = mul i64 %2600, %region_0_243_constant_46592
  %2602 = lshr i64 %2601, %2573
  %shft.chk593 = icmp ult i64 %2573, 64
  %2603 = select i1 %shft.chk593, i64 %2602, i64 0
  %2604 = trunc i64 %2603 to i32
  %2605 = trunc i64 %2594 to i32
  %2606 = zext i32 %2605 to i64
  %2607 = mul i64 %2606, %region_0_243_constant_46592
  %2608 = trunc i64 %2607 to i32
  %2609 = xor i32 %2604, %2608
  %region_0_243_constant_56594 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %2610 = xor i32 %2609, %region_0_243_constant_56594
  %2611 = zext i32 %2610 to i64
  %2612 = mul i64 %2611, %region_0_243_constant_18585
  %2613 = lshr i64 %2612, %2573
  %shft.chk595 = icmp ult i64 %2573, 64
  %2614 = select i1 %shft.chk595, i64 %2613, i64 0
  %2615 = trunc i64 %2614 to i32
  %2616 = lshr i64 %2607, %2573
  %shft.chk596 = icmp ult i64 %2573, 64
  %2617 = select i1 %shft.chk596, i64 %2616, i64 0
  %2618 = trunc i64 %2617 to i32
  %2619 = lshr i64 %2569, %2573
  %shft.chk597 = icmp ult i64 %2573, 64
  %2620 = select i1 %shft.chk597, i64 %2619, i64 0
  %2621 = trunc i64 %2620 to i32
  %2622 = xor i32 %2618, %2621
  %region_0_243_constant_68598 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2623 = xor i32 %2622, %region_0_243_constant_68598
  %2624 = zext i32 %2623 to i64
  %2625 = mul i64 %2624, %region_0_243_constant_18585
  %2626 = trunc i64 %2625 to i32
  %2627 = xor i32 %2615, %2626
  %region_0_243_constant_75599 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %2628 = xor i32 %2627, %region_0_243_constant_75599
  %2629 = zext i32 %2628 to i64
  %2630 = mul i64 %2629, %region_0_243_constant_46592
  %2631 = lshr i64 %2630, %2573
  %shft.chk600 = icmp ult i64 %2573, 64
  %2632 = select i1 %shft.chk600, i64 %2631, i64 0
  %2633 = trunc i64 %2632 to i32
  %2634 = lshr i64 %2625, %2573
  %shft.chk601 = icmp ult i64 %2573, 64
  %2635 = select i1 %shft.chk601, i64 %2634, i64 0
  %2636 = trunc i64 %2635 to i32
  %2637 = trunc i64 %2572 to i32
  %2638 = xor i32 %2636, %2637
  %region_0_243_constant_86602 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2639 = xor i32 %2638, %region_0_243_constant_86602
  %2640 = zext i32 %2639 to i64
  %2641 = mul i64 %2640, %region_0_243_constant_46592
  %2642 = trunc i64 %2641 to i32
  %2643 = xor i32 %2633, %2642
  %region_0_243_constant_93603 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %2644 = xor i32 %2643, %region_0_243_constant_93603
  %2645 = zext i32 %2644 to i64
  %2646 = mul i64 %2645, %region_0_243_constant_18585
  %2647 = lshr i64 %2646, %2573
  %shft.chk604 = icmp ult i64 %2573, 64
  %2648 = select i1 %shft.chk604, i64 %2647, i64 0
  %2649 = trunc i64 %2648 to i32
  %2650 = lshr i64 %2641, %2573
  %shft.chk605 = icmp ult i64 %2573, 64
  %2651 = select i1 %shft.chk605, i64 %2650, i64 0
  %2652 = trunc i64 %2651 to i32
  %2653 = trunc i64 %2601 to i32
  %2654 = xor i32 %2652, %2653
  %region_0_243_constant_104606 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2655 = xor i32 %2654, %region_0_243_constant_104606
  %2656 = zext i32 %2655 to i64
  %2657 = mul i64 %2656, %region_0_243_constant_18585
  %2658 = trunc i64 %2657 to i32
  %2659 = xor i32 %2649, %2658
  %region_0_243_constant_111607 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2660 = xor i32 %2659, %region_0_243_constant_111607
  %2661 = zext i32 %2660 to i64
  %2662 = mul i64 %2661, %region_0_243_constant_46592
  %2663 = lshr i64 %2662, %2573
  %shft.chk608 = icmp ult i64 %2573, 64
  %2664 = select i1 %shft.chk608, i64 %2663, i64 0
  %2665 = trunc i64 %2664 to i32
  %2666 = lshr i64 %2657, %2573
  %shft.chk609 = icmp ult i64 %2573, 64
  %2667 = select i1 %shft.chk609, i64 %2666, i64 0
  %2668 = trunc i64 %2667 to i32
  %2669 = trunc i64 %2612 to i32
  %2670 = xor i32 %2668, %2669
  %region_0_243_constant_122610 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %2671 = xor i32 %2670, %region_0_243_constant_122610
  %2672 = zext i32 %2671 to i64
  %2673 = mul i64 %2672, %region_0_243_constant_46592
  %2674 = trunc i64 %2673 to i32
  %2675 = xor i32 %2665, %2674
  %region_0_243_constant_129611 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %2676 = xor i32 %2675, %region_0_243_constant_129611
  %2677 = zext i32 %2676 to i64
  %2678 = mul i64 %2677, %region_0_243_constant_18585
  %2679 = lshr i64 %2678, %2573
  %shft.chk612 = icmp ult i64 %2573, 64
  %2680 = select i1 %shft.chk612, i64 %2679, i64 0
  %2681 = trunc i64 %2680 to i32
  %2682 = lshr i64 %2673, %2573
  %shft.chk613 = icmp ult i64 %2573, 64
  %2683 = select i1 %shft.chk613, i64 %2682, i64 0
  %2684 = trunc i64 %2683 to i32
  %2685 = trunc i64 %2630 to i32
  %2686 = xor i32 %2684, %2685
  %region_0_243_constant_140614 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %2687 = xor i32 %2686, %region_0_243_constant_140614
  %2688 = zext i32 %2687 to i64
  %2689 = mul i64 %2688, %region_0_243_constant_18585
  %2690 = trunc i64 %2689 to i32
  %2691 = xor i32 %2681, %2690
  %region_0_243_constant_147615 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %2692 = xor i32 %2691, %region_0_243_constant_147615
  %2693 = zext i32 %2692 to i64
  %2694 = mul i64 %2693, %region_0_243_constant_46592
  %2695 = lshr i64 %2694, %2573
  %shft.chk616 = icmp ult i64 %2573, 64
  %2696 = select i1 %shft.chk616, i64 %2695, i64 0
  %2697 = trunc i64 %2696 to i32
  %2698 = lshr i64 %2689, %2573
  %shft.chk617 = icmp ult i64 %2573, 64
  %2699 = select i1 %shft.chk617, i64 %2698, i64 0
  %2700 = trunc i64 %2699 to i32
  %2701 = trunc i64 %2646 to i32
  %2702 = xor i32 %2700, %2701
  %region_0_243_constant_158618 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2703 = xor i32 %2702, %region_0_243_constant_158618
  %2704 = zext i32 %2703 to i64
  %2705 = mul i64 %2704, %region_0_243_constant_46592
  %2706 = trunc i64 %2705 to i32
  %2707 = xor i32 %2697, %2706
  %region_0_243_constant_165619 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2708 = xor i32 %2707, %region_0_243_constant_165619
  %2709 = zext i32 %2708 to i64
  %2710 = mul i64 %2709, %region_0_243_constant_18585
  %2711 = lshr i64 %2710, %2573
  %shft.chk620 = icmp ult i64 %2573, 64
  %2712 = select i1 %shft.chk620, i64 %2711, i64 0
  %2713 = trunc i64 %2712 to i32
  %2714 = lshr i64 %2705, %2573
  %shft.chk621 = icmp ult i64 %2573, 64
  %2715 = select i1 %shft.chk621, i64 %2714, i64 0
  %2716 = trunc i64 %2715 to i32
  %2717 = trunc i64 %2662 to i32
  %2718 = xor i32 %2716, %2717
  %region_0_243_constant_176622 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %2719 = xor i32 %2718, %region_0_243_constant_176622
  %2720 = zext i32 %2719 to i64
  %2721 = mul i64 %2720, %region_0_243_constant_18585
  %2722 = trunc i64 %2721 to i32
  %2723 = xor i32 %2713, %2722
  %region_0_243_constant_183623 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2724 = xor i32 %2723, %region_0_243_constant_183623
  %2725 = zext i32 %2724 to i64
  %2726 = mul i64 %2725, %region_0_243_constant_46592
  %2727 = trunc i64 %2726 to i32
  br label %concatenate.226.merge535

concat_index_from_operand_id2624:                 ; preds = %concatenate.pivot.2.717
  %2728 = phi i32 [ 2, %concatenate.pivot.2.717 ]
  %2729 = sub nsw i32 %2358, %2728
  %2730 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2731 = load i64, i64* %2730, align 8, !invariant.load !22
  %2732 = trunc i64 %2731 to i32
  %2733 = zext i32 %2732 to i64
  %2734 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2735 = lshr i64 %2731, %2734
  %shft.chk625 = icmp ult i64 %2734, 64
  %2736 = select i1 %shft.chk625, i64 %2735, i64 0
  %2737 = trunc i64 %2736 to i32
  %2738 = zext i32 %2737 to i64
  %2739 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2740 = shl i64 %2738, %2739
  %shft.chk626 = icmp ult i64 %2739, 64
  %2741 = select i1 %shft.chk626, i64 %2740, i64 0
  %2742 = or i64 %2733, %2741
  %2743 = mul nuw nsw i32 %2359, 1
  %2744 = add nuw nsw i32 0, %2743
  %2745 = zext i32 %2744 to i64
  %2746 = add i64 %2742, %2745
  %2747 = icmp ult i64 %2746, %2742
  %2748 = zext i1 %2747 to i8
  %2749 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2750 = load i64, i64* %2749, align 8, !invariant.load !22
  %2751 = trunc i64 %2750 to i32
  %2752 = zext i32 %2751 to i64
  %2753 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2754 = lshr i64 %2750, %2753
  %shft.chk627 = icmp ult i64 %2753, 64
  %2755 = select i1 %shft.chk627, i64 %2754, i64 0
  %2756 = trunc i64 %2755 to i32
  %2757 = zext i32 %2756 to i64
  %2758 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2759 = shl i64 %2757, %2758
  %shft.chk628 = icmp ult i64 %2758, 64
  %2760 = select i1 %shft.chk628, i64 %2759, i64 0
  %2761 = or i64 %2752, %2760
  %region_0_243_constant_34629 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %2762 = add i64 %2761, %region_0_243_constant_34629
  %2763 = trunc i8 %2748 to i1
  %2764 = select i1 %2763, i64 %2762, i64 %2761
  %2765 = trunc i64 %2764 to i32
  %2766 = zext i32 %2765 to i64
  %region_0_243_constant_46630 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %2767 = mul i64 %2766, %region_0_243_constant_46630
  %2768 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2769 = lshr i64 %2767, %2768
  %shft.chk631 = icmp ult i64 %2768, 64
  %2770 = select i1 %shft.chk631, i64 %2769, i64 0
  %2771 = trunc i64 %2770 to i32
  %2772 = lshr i64 %2746, %2768
  %shft.chk632 = icmp ult i64 %2768, 64
  %2773 = select i1 %shft.chk632, i64 %2772, i64 0
  %2774 = trunc i64 %2773 to i32
  %2775 = xor i32 %2771, %2774
  %region_0_243_constant_68633 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2776 = xor i32 %2775, %region_0_243_constant_68633
  %2777 = zext i32 %2776 to i64
  %region_0_243_constant_18634 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2778 = mul i64 %2777, %region_0_243_constant_18634
  %2779 = lshr i64 %2778, %2768
  %shft.chk635 = icmp ult i64 %2768, 64
  %2780 = select i1 %shft.chk635, i64 %2779, i64 0
  %2781 = trunc i64 %2780 to i32
  %2782 = trunc i64 %2746 to i32
  %2783 = zext i32 %2782 to i64
  %2784 = mul i64 %2783, %region_0_243_constant_18634
  %2785 = trunc i64 %2784 to i32
  %2786 = xor i32 %2781, %2785
  %region_0_243_constant_86636 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2787 = xor i32 %2786, %region_0_243_constant_86636
  %2788 = zext i32 %2787 to i64
  %2789 = mul i64 %2788, %region_0_243_constant_46630
  %2790 = lshr i64 %2789, %2768
  %shft.chk637 = icmp ult i64 %2768, 64
  %2791 = select i1 %shft.chk637, i64 %2790, i64 0
  %2792 = trunc i64 %2791 to i32
  %2793 = lshr i64 %2784, %2768
  %shft.chk638 = icmp ult i64 %2768, 64
  %2794 = select i1 %shft.chk638, i64 %2793, i64 0
  %2795 = trunc i64 %2794 to i32
  %2796 = lshr i64 %2764, %2768
  %shft.chk639 = icmp ult i64 %2768, 64
  %2797 = select i1 %shft.chk639, i64 %2796, i64 0
  %2798 = trunc i64 %2797 to i32
  %2799 = xor i32 %2795, %2798
  %region_0_243_constant_42640 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2800 = xor i32 %2799, %region_0_243_constant_42640
  %2801 = zext i32 %2800 to i64
  %2802 = mul i64 %2801, %region_0_243_constant_46630
  %2803 = trunc i64 %2802 to i32
  %2804 = xor i32 %2792, %2803
  %region_0_243_constant_104641 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2805 = xor i32 %2804, %region_0_243_constant_104641
  %2806 = zext i32 %2805 to i64
  %2807 = mul i64 %2806, %region_0_243_constant_18634
  %2808 = lshr i64 %2807, %2768
  %shft.chk642 = icmp ult i64 %2768, 64
  %2809 = select i1 %shft.chk642, i64 %2808, i64 0
  %2810 = trunc i64 %2809 to i32
  %2811 = lshr i64 %2802, %2768
  %shft.chk643 = icmp ult i64 %2768, 64
  %2812 = select i1 %shft.chk643, i64 %2811, i64 0
  %2813 = trunc i64 %2812 to i32
  %2814 = trunc i64 %2767 to i32
  %2815 = xor i32 %2813, %2814
  %region_0_243_constant_56644 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %2816 = xor i32 %2815, %region_0_243_constant_56644
  %2817 = zext i32 %2816 to i64
  %2818 = mul i64 %2817, %region_0_243_constant_18634
  %2819 = trunc i64 %2818 to i32
  %2820 = xor i32 %2810, %2819
  %region_0_243_constant_122645 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %2821 = xor i32 %2820, %region_0_243_constant_122645
  %2822 = zext i32 %2821 to i64
  %2823 = mul i64 %2822, %region_0_243_constant_46630
  %2824 = lshr i64 %2823, %2768
  %shft.chk646 = icmp ult i64 %2768, 64
  %2825 = select i1 %shft.chk646, i64 %2824, i64 0
  %2826 = trunc i64 %2825 to i32
  %2827 = lshr i64 %2818, %2768
  %shft.chk647 = icmp ult i64 %2768, 64
  %2828 = select i1 %shft.chk647, i64 %2827, i64 0
  %2829 = trunc i64 %2828 to i32
  %2830 = trunc i64 %2778 to i32
  %2831 = xor i32 %2829, %2830
  %region_0_243_constant_75648 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %2832 = xor i32 %2831, %region_0_243_constant_75648
  %2833 = zext i32 %2832 to i64
  %2834 = mul i64 %2833, %region_0_243_constant_46630
  %2835 = trunc i64 %2834 to i32
  %2836 = xor i32 %2826, %2835
  %region_0_243_constant_140649 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %2837 = xor i32 %2836, %region_0_243_constant_140649
  %2838 = zext i32 %2837 to i64
  %2839 = mul i64 %2838, %region_0_243_constant_18634
  %2840 = lshr i64 %2839, %2768
  %shft.chk650 = icmp ult i64 %2768, 64
  %2841 = select i1 %shft.chk650, i64 %2840, i64 0
  %2842 = trunc i64 %2841 to i32
  %2843 = lshr i64 %2834, %2768
  %shft.chk651 = icmp ult i64 %2768, 64
  %2844 = select i1 %shft.chk651, i64 %2843, i64 0
  %2845 = trunc i64 %2844 to i32
  %2846 = trunc i64 %2789 to i32
  %2847 = xor i32 %2845, %2846
  %region_0_243_constant_93652 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %2848 = xor i32 %2847, %region_0_243_constant_93652
  %2849 = zext i32 %2848 to i64
  %2850 = mul i64 %2849, %region_0_243_constant_18634
  %2851 = trunc i64 %2850 to i32
  %2852 = xor i32 %2842, %2851
  %region_0_243_constant_158653 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2853 = xor i32 %2852, %region_0_243_constant_158653
  %2854 = zext i32 %2853 to i64
  %2855 = mul i64 %2854, %region_0_243_constant_46630
  %2856 = lshr i64 %2855, %2768
  %shft.chk654 = icmp ult i64 %2768, 64
  %2857 = select i1 %shft.chk654, i64 %2856, i64 0
  %2858 = trunc i64 %2857 to i32
  %2859 = lshr i64 %2850, %2768
  %shft.chk655 = icmp ult i64 %2768, 64
  %2860 = select i1 %shft.chk655, i64 %2859, i64 0
  %2861 = trunc i64 %2860 to i32
  %2862 = trunc i64 %2807 to i32
  %2863 = xor i32 %2861, %2862
  %region_0_243_constant_111656 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2864 = xor i32 %2863, %region_0_243_constant_111656
  %2865 = zext i32 %2864 to i64
  %2866 = mul i64 %2865, %region_0_243_constant_46630
  %2867 = trunc i64 %2866 to i32
  %2868 = xor i32 %2858, %2867
  %region_0_243_constant_176657 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %2869 = xor i32 %2868, %region_0_243_constant_176657
  %2870 = zext i32 %2869 to i64
  %2871 = mul i64 %2870, %region_0_243_constant_18634
  %2872 = lshr i64 %2871, %2768
  %shft.chk658 = icmp ult i64 %2768, 64
  %2873 = select i1 %shft.chk658, i64 %2872, i64 0
  %2874 = trunc i64 %2873 to i32
  %2875 = lshr i64 %2866, %2768
  %shft.chk659 = icmp ult i64 %2768, 64
  %2876 = select i1 %shft.chk659, i64 %2875, i64 0
  %2877 = trunc i64 %2876 to i32
  %2878 = trunc i64 %2823 to i32
  %2879 = xor i32 %2877, %2878
  %region_0_243_constant_129660 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %2880 = xor i32 %2879, %region_0_243_constant_129660
  %2881 = zext i32 %2880 to i64
  %2882 = mul i64 %2881, %region_0_243_constant_18634
  %2883 = trunc i64 %2882 to i32
  %2884 = xor i32 %2874, %2883
  %region_0_243_constant_194661 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2885 = xor i32 %2884, %region_0_243_constant_194661
  %2886 = zext i32 %2885 to i64
  %2887 = mul i64 %2886, %region_0_243_constant_46630
  %2888 = lshr i64 %2887, %2768
  %shft.chk662 = icmp ult i64 %2768, 64
  %2889 = select i1 %shft.chk662, i64 %2888, i64 0
  %2890 = trunc i64 %2889 to i32
  %2891 = lshr i64 %2882, %2768
  %shft.chk663 = icmp ult i64 %2768, 64
  %2892 = select i1 %shft.chk663, i64 %2891, i64 0
  %2893 = trunc i64 %2892 to i32
  %2894 = trunc i64 %2839 to i32
  %2895 = xor i32 %2893, %2894
  %region_0_243_constant_147664 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %2896 = xor i32 %2895, %region_0_243_constant_147664
  %2897 = zext i32 %2896 to i64
  %2898 = mul i64 %2897, %region_0_243_constant_46630
  %2899 = trunc i64 %2898 to i32
  %2900 = xor i32 %2890, %2899
  %region_0_243_constant_211665 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %2901 = xor i32 %2900, %region_0_243_constant_211665
  %2902 = zext i32 %2901 to i64
  %2903 = mul i64 %2902, %region_0_243_constant_18634
  %2904 = lshr i64 %2903, %2768
  %shft.chk666 = icmp ult i64 %2768, 64
  %2905 = select i1 %shft.chk666, i64 %2904, i64 0
  %2906 = trunc i64 %2905 to i32
  %2907 = lshr i64 %2898, %2768
  %shft.chk667 = icmp ult i64 %2768, 64
  %2908 = select i1 %shft.chk667, i64 %2907, i64 0
  %2909 = trunc i64 %2908 to i32
  %2910 = trunc i64 %2855 to i32
  %2911 = xor i32 %2909, %2910
  %region_0_243_constant_165668 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2912 = xor i32 %2911, %region_0_243_constant_165668
  %2913 = zext i32 %2912 to i64
  %2914 = mul i64 %2913, %region_0_243_constant_18634
  %2915 = trunc i64 %2914 to i32
  %2916 = xor i32 %2906, %2915
  %region_0_243_constant_220669 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2917 = xor i32 %2916, %region_0_243_constant_220669
  br label %concatenate.226.merge535

concat_index_from_operand_id3670:                 ; preds = %concatenate.pivot.3.718
  %2918 = phi i32 [ 3, %concatenate.pivot.3.718 ]
  %2919 = sub nsw i32 %2358, %2918
  %2920 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2921 = load i64, i64* %2920, align 8, !invariant.load !22
  %2922 = trunc i64 %2921 to i32
  %2923 = zext i32 %2922 to i64
  %2924 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2925 = lshr i64 %2921, %2924
  %shft.chk671 = icmp ult i64 %2924, 64
  %2926 = select i1 %shft.chk671, i64 %2925, i64 0
  %2927 = trunc i64 %2926 to i32
  %2928 = zext i32 %2927 to i64
  %2929 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2930 = shl i64 %2928, %2929
  %shft.chk672 = icmp ult i64 %2929, 64
  %2931 = select i1 %shft.chk672, i64 %2930, i64 0
  %2932 = or i64 %2923, %2931
  %2933 = mul nuw nsw i32 %2359, 1
  %2934 = add nuw nsw i32 0, %2933
  %2935 = zext i32 %2934 to i64
  %2936 = add i64 %2932, %2935
  %2937 = icmp ult i64 %2936, %2932
  %2938 = zext i1 %2937 to i8
  %2939 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2940 = load i64, i64* %2939, align 8, !invariant.load !22
  %2941 = trunc i64 %2940 to i32
  %2942 = zext i32 %2941 to i64
  %2943 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2944 = lshr i64 %2940, %2943
  %shft.chk673 = icmp ult i64 %2943, 64
  %2945 = select i1 %shft.chk673, i64 %2944, i64 0
  %2946 = trunc i64 %2945 to i32
  %2947 = zext i32 %2946 to i64
  %2948 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2949 = shl i64 %2947, %2948
  %shft.chk674 = icmp ult i64 %2948, 64
  %2950 = select i1 %shft.chk674, i64 %2949, i64 0
  %2951 = or i64 %2942, %2950
  %region_0_243_constant_34675 = load i64, i64* bitcast ([8 x i8]* @11 to i64*), align 8
  %2952 = add i64 %2951, %region_0_243_constant_34675
  %2953 = trunc i8 %2938 to i1
  %2954 = select i1 %2953, i64 %2952, i64 %2951
  %2955 = trunc i64 %2954 to i32
  %2956 = zext i32 %2955 to i64
  %region_0_243_constant_46676 = load i64, i64* bitcast ([8 x i8]* @6 to i64*), align 8
  %2957 = mul i64 %2956, %region_0_243_constant_46676
  %2958 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2959 = lshr i64 %2957, %2958
  %shft.chk677 = icmp ult i64 %2958, 64
  %2960 = select i1 %shft.chk677, i64 %2959, i64 0
  %2961 = trunc i64 %2960 to i32
  %2962 = lshr i64 %2936, %2958
  %shft.chk678 = icmp ult i64 %2958, 64
  %2963 = select i1 %shft.chk678, i64 %2962, i64 0
  %2964 = trunc i64 %2963 to i32
  %2965 = xor i32 %2961, %2964
  %region_0_243_constant_68679 = load i32, i32* bitcast ([4 x i8]* @10 to i32*), align 4
  %2966 = xor i32 %2965, %region_0_243_constant_68679
  %2967 = zext i32 %2966 to i64
  %region_0_243_constant_18680 = load i64, i64* bitcast ([8 x i8]* @4 to i64*), align 8
  %2968 = mul i64 %2967, %region_0_243_constant_18680
  %2969 = lshr i64 %2968, %2958
  %shft.chk681 = icmp ult i64 %2958, 64
  %2970 = select i1 %shft.chk681, i64 %2969, i64 0
  %2971 = trunc i64 %2970 to i32
  %2972 = trunc i64 %2936 to i32
  %2973 = zext i32 %2972 to i64
  %2974 = mul i64 %2973, %region_0_243_constant_18680
  %2975 = trunc i64 %2974 to i32
  %2976 = xor i32 %2971, %2975
  %region_0_243_constant_86682 = load i32, i32* bitcast ([4 x i8]* @16 to i32*), align 4
  %2977 = xor i32 %2976, %region_0_243_constant_86682
  %2978 = zext i32 %2977 to i64
  %2979 = mul i64 %2978, %region_0_243_constant_46676
  %2980 = lshr i64 %2979, %2958
  %shft.chk683 = icmp ult i64 %2958, 64
  %2981 = select i1 %shft.chk683, i64 %2980, i64 0
  %2982 = trunc i64 %2981 to i32
  %2983 = lshr i64 %2974, %2958
  %shft.chk684 = icmp ult i64 %2958, 64
  %2984 = select i1 %shft.chk684, i64 %2983, i64 0
  %2985 = trunc i64 %2984 to i32
  %2986 = lshr i64 %2954, %2958
  %shft.chk685 = icmp ult i64 %2958, 64
  %2987 = select i1 %shft.chk685, i64 %2986, i64 0
  %2988 = trunc i64 %2987 to i32
  %2989 = xor i32 %2985, %2988
  %region_0_243_constant_42686 = load i32, i32* bitcast ([4 x i8]* @13 to i32*), align 4
  %2990 = xor i32 %2989, %region_0_243_constant_42686
  %2991 = zext i32 %2990 to i64
  %2992 = mul i64 %2991, %region_0_243_constant_46676
  %2993 = trunc i64 %2992 to i32
  %2994 = xor i32 %2982, %2993
  %region_0_243_constant_104687 = load i32, i32* bitcast ([4 x i8]* @15 to i32*), align 4
  %2995 = xor i32 %2994, %region_0_243_constant_104687
  %2996 = zext i32 %2995 to i64
  %2997 = mul i64 %2996, %region_0_243_constant_18680
  %2998 = lshr i64 %2997, %2958
  %shft.chk688 = icmp ult i64 %2958, 64
  %2999 = select i1 %shft.chk688, i64 %2998, i64 0
  %3000 = trunc i64 %2999 to i32
  %3001 = lshr i64 %2992, %2958
  %shft.chk689 = icmp ult i64 %2958, 64
  %3002 = select i1 %shft.chk689, i64 %3001, i64 0
  %3003 = trunc i64 %3002 to i32
  %3004 = trunc i64 %2957 to i32
  %3005 = xor i32 %3003, %3004
  %region_0_243_constant_56690 = load i32, i32* bitcast ([4 x i8]* @12 to i32*), align 4
  %3006 = xor i32 %3005, %region_0_243_constant_56690
  %3007 = zext i32 %3006 to i64
  %3008 = mul i64 %3007, %region_0_243_constant_18680
  %3009 = trunc i64 %3008 to i32
  %3010 = xor i32 %3000, %3009
  %region_0_243_constant_122691 = load i32, i32* bitcast ([4 x i8]* @14 to i32*), align 4
  %3011 = xor i32 %3010, %region_0_243_constant_122691
  %3012 = zext i32 %3011 to i64
  %3013 = mul i64 %3012, %region_0_243_constant_46676
  %3014 = lshr i64 %3013, %2958
  %shft.chk692 = icmp ult i64 %2958, 64
  %3015 = select i1 %shft.chk692, i64 %3014, i64 0
  %3016 = trunc i64 %3015 to i32
  %3017 = lshr i64 %3008, %2958
  %shft.chk693 = icmp ult i64 %2958, 64
  %3018 = select i1 %shft.chk693, i64 %3017, i64 0
  %3019 = trunc i64 %3018 to i32
  %3020 = trunc i64 %2968 to i32
  %3021 = xor i32 %3019, %3020
  %region_0_243_constant_75694 = load i32, i32* bitcast ([4 x i8]* @9 to i32*), align 4
  %3022 = xor i32 %3021, %region_0_243_constant_75694
  %3023 = zext i32 %3022 to i64
  %3024 = mul i64 %3023, %region_0_243_constant_46676
  %3025 = trunc i64 %3024 to i32
  %3026 = xor i32 %3016, %3025
  %region_0_243_constant_140695 = load i32, i32* bitcast ([4 x i8]* @8 to i32*), align 4
  %3027 = xor i32 %3026, %region_0_243_constant_140695
  %3028 = zext i32 %3027 to i64
  %3029 = mul i64 %3028, %region_0_243_constant_18680
  %3030 = lshr i64 %3029, %2958
  %shft.chk696 = icmp ult i64 %2958, 64
  %3031 = select i1 %shft.chk696, i64 %3030, i64 0
  %3032 = trunc i64 %3031 to i32
  %3033 = lshr i64 %3024, %2958
  %shft.chk697 = icmp ult i64 %2958, 64
  %3034 = select i1 %shft.chk697, i64 %3033, i64 0
  %3035 = trunc i64 %3034 to i32
  %3036 = trunc i64 %2979 to i32
  %3037 = xor i32 %3035, %3036
  %region_0_243_constant_93698 = load i32, i32* bitcast ([4 x i8]* @19 to i32*), align 4
  %3038 = xor i32 %3037, %region_0_243_constant_93698
  %3039 = zext i32 %3038 to i64
  %3040 = mul i64 %3039, %region_0_243_constant_18680
  %3041 = trunc i64 %3040 to i32
  %3042 = xor i32 %3032, %3041
  %region_0_243_constant_158699 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %3043 = xor i32 %3042, %region_0_243_constant_158699
  %3044 = zext i32 %3043 to i64
  %3045 = mul i64 %3044, %region_0_243_constant_46676
  %3046 = lshr i64 %3045, %2958
  %shft.chk700 = icmp ult i64 %2958, 64
  %3047 = select i1 %shft.chk700, i64 %3046, i64 0
  %3048 = trunc i64 %3047 to i32
  %3049 = lshr i64 %3040, %2958
  %shft.chk701 = icmp ult i64 %2958, 64
  %3050 = select i1 %shft.chk701, i64 %3049, i64 0
  %3051 = trunc i64 %3050 to i32
  %3052 = trunc i64 %2997 to i32
  %3053 = xor i32 %3051, %3052
  %region_0_243_constant_111702 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %3054 = xor i32 %3053, %region_0_243_constant_111702
  %3055 = zext i32 %3054 to i64
  %3056 = mul i64 %3055, %region_0_243_constant_46676
  %3057 = trunc i64 %3056 to i32
  %3058 = xor i32 %3048, %3057
  %region_0_243_constant_176703 = load i32, i32* bitcast ([4 x i8]* @21 to i32*), align 4
  %3059 = xor i32 %3058, %region_0_243_constant_176703
  %3060 = zext i32 %3059 to i64
  %3061 = mul i64 %3060, %region_0_243_constant_18680
  %3062 = lshr i64 %3061, %2958
  %shft.chk704 = icmp ult i64 %2958, 64
  %3063 = select i1 %shft.chk704, i64 %3062, i64 0
  %3064 = trunc i64 %3063 to i32
  %3065 = lshr i64 %3056, %2958
  %shft.chk705 = icmp ult i64 %2958, 64
  %3066 = select i1 %shft.chk705, i64 %3065, i64 0
  %3067 = trunc i64 %3066 to i32
  %3068 = trunc i64 %3013 to i32
  %3069 = xor i32 %3067, %3068
  %region_0_243_constant_129706 = load i32, i32* bitcast ([4 x i8]* @17 to i32*), align 4
  %3070 = xor i32 %3069, %region_0_243_constant_129706
  %3071 = zext i32 %3070 to i64
  %3072 = mul i64 %3071, %region_0_243_constant_18680
  %3073 = trunc i64 %3072 to i32
  %3074 = xor i32 %3064, %3073
  %region_0_243_constant_194707 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %3075 = xor i32 %3074, %region_0_243_constant_194707
  %3076 = zext i32 %3075 to i64
  %3077 = mul i64 %3076, %region_0_243_constant_46676
  %3078 = lshr i64 %3077, %2958
  %shft.chk708 = icmp ult i64 %2958, 64
  %3079 = select i1 %shft.chk708, i64 %3078, i64 0
  %3080 = trunc i64 %3079 to i32
  %3081 = lshr i64 %3072, %2958
  %shft.chk709 = icmp ult i64 %2958, 64
  %3082 = select i1 %shft.chk709, i64 %3081, i64 0
  %3083 = trunc i64 %3082 to i32
  %3084 = trunc i64 %3029 to i32
  %3085 = xor i32 %3083, %3084
  %region_0_243_constant_147710 = load i32, i32* bitcast ([4 x i8]* @7 to i32*), align 4
  %3086 = xor i32 %3085, %region_0_243_constant_147710
  %3087 = zext i32 %3086 to i64
  %3088 = mul i64 %3087, %region_0_243_constant_46676
  %3089 = trunc i64 %3088 to i32
  %3090 = xor i32 %3080, %3089
  %region_0_243_constant_211711 = load i32, i32* bitcast ([4 x i8]* @5 to i32*), align 4
  %3091 = xor i32 %3090, %region_0_243_constant_211711
  %3092 = zext i32 %3091 to i64
  %3093 = mul i64 %3092, %region_0_243_constant_18680
  %3094 = trunc i64 %3093 to i32
  br label %concatenate.226.merge535

concatenate.pivot.2.712:                          ; preds = %concatenate.226.merge344
  %3095 = icmp ult i32 %2358, 2
  br i1 %3095, label %concatenate.pivot.1.713, label %concatenate.pivot.3.716

concatenate.pivot.1.713:                          ; preds = %concatenate.pivot.2.712
  %3096 = icmp ult i32 %2358, 1
  br i1 %3096, label %concatenate.pivot.0.714, label %concatenate.pivot.1.715

concatenate.pivot.0.714:                          ; preds = %concatenate.pivot.1.713
  br label %concat_index_from_operand_id0536

concatenate.pivot.1.715:                          ; preds = %concatenate.pivot.1.713
  br label %concat_index_from_operand_id1582

concatenate.pivot.3.716:                          ; preds = %concatenate.pivot.2.712
  %3097 = icmp ult i32 %2358, 3
  br i1 %3097, label %concatenate.pivot.2.717, label %concatenate.pivot.3.718

concatenate.pivot.2.717:                          ; preds = %concatenate.pivot.3.716
  br label %concat_index_from_operand_id2624

concatenate.pivot.3.718:                          ; preds = %concatenate.pivot.3.716
  br label %concat_index_from_operand_id3670

concatenate.226.merge535:                         ; preds = %concat_index_from_operand_id3670, %concat_index_from_operand_id2624, %concat_index_from_operand_id1582, %concat_index_from_operand_id0536
  %3098 = phi i32 [ %2550, %concat_index_from_operand_id0536 ], [ %2727, %concat_index_from_operand_id1582 ], [ %2917, %concat_index_from_operand_id2624 ], [ %3094, %concat_index_from_operand_id3670 ]
  %region_0_243_constant_227719 = load i32, i32* bitcast ([4 x i8]* @3 to i32*), align 4
  %3099 = lshr i32 %3098, %region_0_243_constant_227719
  %shft.chk720 = icmp ult i32 %region_0_243_constant_227719, 32
  %3100 = select i1 %shft.chk720, i32 %3099, i32 0
  %3101 = uitofp i32 %3100 to float
  %region_0_243_constant_231721 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %multiply.233722 = fmul float %3101, %region_0_243_constant_231721
  %region_0_243_constant_234723 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %compare.236724 = fcmp olt float %multiply.233722, %region_0_243_constant_234723
  %3102 = zext i1 %compare.236724 to i8
  %3103 = mul nuw nsw i32 %2345, 1
  %3104 = add nuw nsw i32 0, %3103
  %3105 = udiv i32 %3104, 16
  %3106 = mul nuw nsw i32 %2349, 1
  %3107 = add nuw nsw i32 0, %3106
  %3108 = mul nuw nsw i32 %2350, 32
  %3109 = add nuw nsw i32 %3107, %3108
  %3110 = udiv i32 %3109, 256
  %3111 = bitcast [256 x [16 x float]]* %1 to float*
  %3112 = getelementptr inbounds float, float* %3111, i32 %linear_index3
  %3113 = load float, float* %3112, align 4, !invariant.load !22
  %region_0_243_constant_239725 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %3114 = trunc i8 %3102 to i1
  %3115 = select i1 %3114, float %3113, float %region_0_243_constant_239725
  %3116 = bitcast [256 x [16 x float]]* %5 to float*
  %3117 = getelementptr inbounds float, float* %3116, i32 %linear_index3
  store float %3115, float* %3117, align 4
  br label %fusion_5.in_bounds-after
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() #0

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() #0

; Function Attrs: inaccessiblememonly nocallback nofree nosync nounwind willreturn
declare void @llvm.assume(i1 noundef) #1

define void @fusion_4(i8* noalias align 16 dereferenceable(16384) %alloc4, i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %alloc4, i64 0
  %1 = bitcast i8* %0 to [8 x [32 x [16 x float]]]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 32768
  %3 = bitcast i8* %2 to [256 x [16 x float]]*
  %4 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %5 = bitcast i8* %4 to [8 x [32 x [16 x float]]]*
  %6 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !20
  %7 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !21
  %8 = mul nuw nsw i32 %6, 256
  %linear_index = add nuw nsw i32 %8, %7
  %linear_index_in_range = icmp ult i32 %linear_index, 1024
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %9 = udiv i32 %linear_index_base, 1
  %10 = urem i32 %9, 16
  %11 = udiv i32 %linear_index_base, 16
  %12 = urem i32 %11, 32
  %13 = udiv i32 %linear_index_base, 512
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %14 = udiv i32 %linear_index1, 1
  %15 = urem i32 %14, 16
  %16 = udiv i32 %linear_index1, 16
  %17 = urem i32 %16, 32
  %18 = udiv i32 %linear_index1, 512
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %19 = udiv i32 %linear_index2, 1
  %20 = urem i32 %19, 16
  %21 = udiv i32 %linear_index2, 16
  %22 = urem i32 %21, 32
  %23 = udiv i32 %linear_index2, 512
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %24 = udiv i32 %linear_index3, 1
  %25 = urem i32 %24, 16
  %26 = udiv i32 %linear_index3, 16
  %27 = urem i32 %26, 32
  %28 = udiv i32 %linear_index3, 512
  %29 = icmp ult i32 %linear_index_base, 4096
  br i1 %29, label %fusion_4.in_bounds-true, label %fusion_4.in_bounds-after

fusion_4.in_bounds-after:                         ; preds = %fusion_4.in_bounds-true, %entry
  ret void

fusion_4.in_bounds-true:                          ; preds = %entry
  %30 = mul nuw nsw i32 %10, 1
  %31 = add nuw nsw i32 0, %30
  %32 = udiv i32 %31, 16
  %33 = mul nuw nsw i32 %12, 1
  %34 = add nuw nsw i32 0, %33
  %35 = mul nuw nsw i32 %13, 32
  %36 = add nuw nsw i32 %34, %35
  %37 = udiv i32 %36, 256
  %38 = bitcast [256 x [16 x float]]* %3 to float*
  %39 = getelementptr inbounds float, float* %38, i32 %linear_index_base
  %40 = load float, float* %39, align 4, !invariant.load !22
  %41 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %42 = getelementptr inbounds float, float* %41, i32 %linear_index_base
  %43 = load float, float* %42, align 4, !invariant.load !22
  %subtract.4 = fsub float %40, %43
  %region_0_8_constant_5 = load float, float* bitcast ([4 x i8]* @27 to float*), align 4
  %multiply.7 = fmul float %subtract.4, %region_0_8_constant_5
  %44 = bitcast [8 x [32 x [16 x float]]]* %5 to float*
  %45 = getelementptr inbounds float, float* %44, i32 %linear_index_base
  store float %multiply.7, float* %45, align 4
  %46 = mul nuw nsw i32 %15, 1
  %47 = add nuw nsw i32 0, %46
  %48 = udiv i32 %47, 16
  %49 = mul nuw nsw i32 %17, 1
  %50 = add nuw nsw i32 0, %49
  %51 = mul nuw nsw i32 %18, 32
  %52 = add nuw nsw i32 %50, %51
  %53 = udiv i32 %52, 256
  %54 = bitcast [256 x [16 x float]]* %3 to float*
  %55 = getelementptr inbounds float, float* %54, i32 %linear_index1
  %56 = load float, float* %55, align 4, !invariant.load !22
  %57 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %58 = getelementptr inbounds float, float* %57, i32 %linear_index1
  %59 = load float, float* %58, align 4, !invariant.load !22
  %subtract.41 = fsub float %56, %59
  %region_0_8_constant_52 = load float, float* bitcast ([4 x i8]* @27 to float*), align 4
  %multiply.73 = fmul float %subtract.41, %region_0_8_constant_52
  %60 = bitcast [8 x [32 x [16 x float]]]* %5 to float*
  %61 = getelementptr inbounds float, float* %60, i32 %linear_index1
  store float %multiply.73, float* %61, align 4
  %62 = mul nuw nsw i32 %20, 1
  %63 = add nuw nsw i32 0, %62
  %64 = udiv i32 %63, 16
  %65 = mul nuw nsw i32 %22, 1
  %66 = add nuw nsw i32 0, %65
  %67 = mul nuw nsw i32 %23, 32
  %68 = add nuw nsw i32 %66, %67
  %69 = udiv i32 %68, 256
  %70 = bitcast [256 x [16 x float]]* %3 to float*
  %71 = getelementptr inbounds float, float* %70, i32 %linear_index2
  %72 = load float, float* %71, align 4, !invariant.load !22
  %73 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %74 = getelementptr inbounds float, float* %73, i32 %linear_index2
  %75 = load float, float* %74, align 4, !invariant.load !22
  %subtract.44 = fsub float %72, %75
  %region_0_8_constant_55 = load float, float* bitcast ([4 x i8]* @27 to float*), align 4
  %multiply.76 = fmul float %subtract.44, %region_0_8_constant_55
  %76 = bitcast [8 x [32 x [16 x float]]]* %5 to float*
  %77 = getelementptr inbounds float, float* %76, i32 %linear_index2
  store float %multiply.76, float* %77, align 4
  %78 = mul nuw nsw i32 %25, 1
  %79 = add nuw nsw i32 0, %78
  %80 = udiv i32 %79, 16
  %81 = mul nuw nsw i32 %27, 1
  %82 = add nuw nsw i32 0, %81
  %83 = mul nuw nsw i32 %28, 32
  %84 = add nuw nsw i32 %82, %83
  %85 = udiv i32 %84, 256
  %86 = bitcast [256 x [16 x float]]* %3 to float*
  %87 = getelementptr inbounds float, float* %86, i32 %linear_index3
  %88 = load float, float* %87, align 4, !invariant.load !22
  %89 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %90 = getelementptr inbounds float, float* %89, i32 %linear_index3
  %91 = load float, float* %90, align 4, !invariant.load !22
  %subtract.47 = fsub float %88, %91
  %region_0_8_constant_58 = load float, float* bitcast ([4 x i8]* @27 to float*), align 4
  %multiply.79 = fmul float %subtract.47, %region_0_8_constant_58
  %92 = bitcast [8 x [32 x [16 x float]]]* %5 to float*
  %93 = getelementptr inbounds float, float* %92, i32 %linear_index3
  store float %multiply.79, float* %93, align 4
  br label %fusion_4.in_bounds-after
}

define void @fusion_1(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [8 x [32 x [16 x float]]]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 32768
  %3 = bitcast i8* %2 to [256 x [16 x float]]*
  %4 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !20
  %5 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !21
  %6 = mul nuw nsw i32 %4, 256
  %linear_index = add nuw nsw i32 %6, %5
  %linear_index_in_range = icmp ult i32 %linear_index, 1024
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %7 = udiv i32 %linear_index_base, 1
  %8 = urem i32 %7, 16
  %9 = udiv i32 %linear_index_base, 16
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %10 = udiv i32 %linear_index1, 1
  %11 = urem i32 %10, 16
  %12 = udiv i32 %linear_index1, 16
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %13 = udiv i32 %linear_index2, 1
  %14 = urem i32 %13, 16
  %15 = udiv i32 %linear_index2, 16
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %16 = udiv i32 %linear_index3, 1
  %17 = urem i32 %16, 16
  %18 = udiv i32 %linear_index3, 16
  %19 = icmp ult i32 %linear_index_base, 4096
  br i1 %19, label %fusion_1.in_bounds-true, label %fusion_1.in_bounds-after

fusion_1.in_bounds-after:                         ; preds = %fusion_1.in_bounds-true, %entry
  ret void

fusion_1.in_bounds-true:                          ; preds = %entry
  %20 = mul nuw nsw i32 %9, 1
  %21 = add nuw nsw i32 0, %20
  %22 = urem i32 %21, 32
  %23 = udiv i32 %21, 32
  %24 = udiv i32 %23, 8
  %25 = mul nuw nsw i32 %8, 1
  %26 = add nuw nsw i32 0, %25
  %27 = udiv i32 %26, 16
  %28 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %29 = getelementptr inbounds float, float* %28, i32 %linear_index_base
  %30 = load float, float* %29, align 4, !invariant.load !22
  %31 = bitcast [256 x [16 x float]]* %3 to float*
  %32 = getelementptr inbounds float, float* %31, i32 %linear_index_base
  store float %30, float* %32, align 4
  %33 = mul nuw nsw i32 %12, 1
  %34 = add nuw nsw i32 0, %33
  %35 = urem i32 %34, 32
  %36 = udiv i32 %34, 32
  %37 = udiv i32 %36, 8
  %38 = mul nuw nsw i32 %11, 1
  %39 = add nuw nsw i32 0, %38
  %40 = udiv i32 %39, 16
  %41 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %42 = getelementptr inbounds float, float* %41, i32 %linear_index1
  %43 = load float, float* %42, align 4, !invariant.load !22
  %44 = bitcast [256 x [16 x float]]* %3 to float*
  %45 = getelementptr inbounds float, float* %44, i32 %linear_index1
  store float %43, float* %45, align 4
  %46 = mul nuw nsw i32 %15, 1
  %47 = add nuw nsw i32 0, %46
  %48 = urem i32 %47, 32
  %49 = udiv i32 %47, 32
  %50 = udiv i32 %49, 8
  %51 = mul nuw nsw i32 %14, 1
  %52 = add nuw nsw i32 0, %51
  %53 = udiv i32 %52, 16
  %54 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %55 = getelementptr inbounds float, float* %54, i32 %linear_index2
  %56 = load float, float* %55, align 4, !invariant.load !22
  %57 = bitcast [256 x [16 x float]]* %3 to float*
  %58 = getelementptr inbounds float, float* %57, i32 %linear_index2
  store float %56, float* %58, align 4
  %59 = mul nuw nsw i32 %18, 1
  %60 = add nuw nsw i32 0, %59
  %61 = urem i32 %60, 32
  %62 = udiv i32 %60, 32
  %63 = udiv i32 %62, 8
  %64 = mul nuw nsw i32 %17, 1
  %65 = add nuw nsw i32 0, %64
  %66 = udiv i32 %65, 16
  %67 = bitcast [8 x [32 x [16 x float]]]* %1 to float*
  %68 = getelementptr inbounds float, float* %67, i32 %linear_index3
  %69 = load float, float* %68, align 4, !invariant.load !22
  %70 = bitcast [256 x [16 x float]]* %3 to float*
  %71 = getelementptr inbounds float, float* %70, i32 %linear_index3
  store float %69, float* %71, align 4
  br label %fusion_1.in_bounds-after
}

define void @fusion_3(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 16384
  %1 = bitcast i8* %0 to [256 x [16 x float]]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 50176
  %3 = bitcast i8* %2 to [2 x i64]*
  %4 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %5 = bitcast i8* %4 to [256 x [16 x float]]*
  %6 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !20
  %7 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !21
  %8 = mul nuw nsw i32 %6, 1024
  %linear_index = add nuw nsw i32 %8, %7
  %linear_index_in_range = icmp ult i32 %linear_index, 4096
  call void @llvm.assume(i1 %linear_index_in_range)
  %9 = urem i32 %linear_index, 256
  %10 = udiv i32 %linear_index, 256
  %11 = urem i32 %10, 16
  %12 = udiv i32 %10, 16
  %linear_index1 = add nuw nsw i32 %linear_index, 256
  %13 = urem i32 %linear_index1, 256
  %14 = udiv i32 %linear_index1, 256
  %15 = urem i32 %14, 16
  %16 = udiv i32 %14, 16
  %linear_index2 = add nuw nsw i32 %linear_index1, 256
  %17 = urem i32 %linear_index2, 256
  %18 = udiv i32 %linear_index2, 256
  %19 = urem i32 %18, 16
  %20 = udiv i32 %18, 16
  %linear_index3 = add nuw nsw i32 %linear_index2, 256
  %21 = urem i32 %linear_index3, 256
  %22 = udiv i32 %linear_index3, 256
  %23 = urem i32 %22, 16
  %24 = udiv i32 %22, 16
  br label %fusion-body

after-fusion-body:                                ; preds = %concatenate.226.merge541
  ret void

fusion-body:                                      ; preds = %entry
  %25 = mul nuw nsw i32 %9, 1
  %26 = add nuw nsw i32 0, %25
  %27 = urem i32 %26, 32
  %28 = udiv i32 %26, 32
  %29 = udiv i32 %28, 8
  %30 = mul nuw nsw i32 %11, 1
  %31 = add nuw nsw i32 0, %30
  %32 = udiv i32 %31, 16
  %33 = mul nuw nsw i32 %31, 1
  %34 = add nuw nsw i32 0, %33
  %35 = mul nuw nsw i32 %27, 16
  %36 = add nuw nsw i32 %34, %35
  %37 = mul nuw nsw i32 %28, 512
  %38 = add nuw nsw i32 %36, %37
  %39 = urem i32 %38, 4
  %40 = udiv i32 %38, 4
  %41 = udiv i32 %40, 1024
  br label %concatenate.pivot.2.

concat_index_from_operand_id0:                    ; preds = %concatenate.pivot.0.
  %42 = phi i32 [ 0, %concatenate.pivot.0. ]
  %43 = sub nsw i32 %39, %42
  %44 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %45 = load i64, i64* %44, align 8, !invariant.load !22
  %46 = trunc i64 %45 to i32
  %47 = zext i32 %46 to i64
  %48 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %49 = lshr i64 %45, %48
  %shft.chk = icmp ult i64 %48, 64
  %50 = select i1 %shft.chk, i64 %49, i64 0
  %51 = trunc i64 %50 to i32
  %52 = zext i32 %51 to i64
  %53 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %54 = shl i64 %52, %53
  %shft.chk1 = icmp ult i64 %53, 64
  %55 = select i1 %shft.chk1, i64 %54, i64 0
  %56 = or i64 %47, %55
  %57 = mul nuw nsw i32 %40, 1
  %58 = add nuw nsw i32 0, %57
  %59 = zext i32 %58 to i64
  %60 = add i64 %56, %59
  %61 = trunc i64 %60 to i32
  %62 = zext i32 %61 to i64
  %region_0_250_constant_18 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %63 = mul i64 %62, %region_0_250_constant_18
  %64 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %65 = lshr i64 %63, %64
  %shft.chk2 = icmp ult i64 %64, 64
  %66 = select i1 %shft.chk2, i64 %65, i64 0
  %67 = trunc i64 %66 to i32
  %68 = icmp ult i64 %60, %56
  %69 = zext i1 %68 to i8
  %70 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %71 = load i64, i64* %70, align 8, !invariant.load !22
  %72 = trunc i64 %71 to i32
  %73 = zext i32 %72 to i64
  %74 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %75 = lshr i64 %71, %74
  %shft.chk3 = icmp ult i64 %74, 64
  %76 = select i1 %shft.chk3, i64 %75, i64 0
  %77 = trunc i64 %76 to i32
  %78 = zext i32 %77 to i64
  %79 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %80 = shl i64 %78, %79
  %shft.chk4 = icmp ult i64 %79, 64
  %81 = select i1 %shft.chk4, i64 %80, i64 0
  %82 = or i64 %73, %81
  %region_0_250_constant_34 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %83 = add i64 %82, %region_0_250_constant_34
  %84 = trunc i8 %69 to i1
  %85 = select i1 %84, i64 %83, i64 %82
  %86 = lshr i64 %85, %64
  %shft.chk5 = icmp ult i64 %64, 64
  %87 = select i1 %shft.chk5, i64 %86, i64 0
  %88 = trunc i64 %87 to i32
  %89 = xor i32 %67, %88
  %region_0_250_constant_42 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %90 = xor i32 %89, %region_0_250_constant_42
  %91 = zext i32 %90 to i64
  %region_0_250_constant_46 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %92 = mul i64 %91, %region_0_250_constant_46
  %93 = lshr i64 %92, %64
  %shft.chk6 = icmp ult i64 %64, 64
  %94 = select i1 %shft.chk6, i64 %93, i64 0
  %95 = trunc i64 %94 to i32
  %96 = trunc i64 %85 to i32
  %97 = zext i32 %96 to i64
  %98 = mul i64 %97, %region_0_250_constant_46
  %99 = trunc i64 %98 to i32
  %100 = xor i32 %95, %99
  %region_0_250_constant_56 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %101 = xor i32 %100, %region_0_250_constant_56
  %102 = zext i32 %101 to i64
  %103 = mul i64 %102, %region_0_250_constant_18
  %104 = lshr i64 %103, %64
  %shft.chk7 = icmp ult i64 %64, 64
  %105 = select i1 %shft.chk7, i64 %104, i64 0
  %106 = trunc i64 %105 to i32
  %107 = lshr i64 %98, %64
  %shft.chk8 = icmp ult i64 %64, 64
  %108 = select i1 %shft.chk8, i64 %107, i64 0
  %109 = trunc i64 %108 to i32
  %110 = lshr i64 %60, %64
  %shft.chk9 = icmp ult i64 %64, 64
  %111 = select i1 %shft.chk9, i64 %110, i64 0
  %112 = trunc i64 %111 to i32
  %113 = xor i32 %109, %112
  %region_0_250_constant_68 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %114 = xor i32 %113, %region_0_250_constant_68
  %115 = zext i32 %114 to i64
  %116 = mul i64 %115, %region_0_250_constant_18
  %117 = trunc i64 %116 to i32
  %118 = xor i32 %106, %117
  %region_0_250_constant_75 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %119 = xor i32 %118, %region_0_250_constant_75
  %120 = zext i32 %119 to i64
  %121 = mul i64 %120, %region_0_250_constant_46
  %122 = lshr i64 %121, %64
  %shft.chk10 = icmp ult i64 %64, 64
  %123 = select i1 %shft.chk10, i64 %122, i64 0
  %124 = trunc i64 %123 to i32
  %125 = lshr i64 %116, %64
  %shft.chk11 = icmp ult i64 %64, 64
  %126 = select i1 %shft.chk11, i64 %125, i64 0
  %127 = trunc i64 %126 to i32
  %128 = trunc i64 %63 to i32
  %129 = xor i32 %127, %128
  %region_0_250_constant_86 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %130 = xor i32 %129, %region_0_250_constant_86
  %131 = zext i32 %130 to i64
  %132 = mul i64 %131, %region_0_250_constant_46
  %133 = trunc i64 %132 to i32
  %134 = xor i32 %124, %133
  %region_0_250_constant_93 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %135 = xor i32 %134, %region_0_250_constant_93
  %136 = zext i32 %135 to i64
  %137 = mul i64 %136, %region_0_250_constant_18
  %138 = lshr i64 %137, %64
  %shft.chk12 = icmp ult i64 %64, 64
  %139 = select i1 %shft.chk12, i64 %138, i64 0
  %140 = trunc i64 %139 to i32
  %141 = lshr i64 %132, %64
  %shft.chk13 = icmp ult i64 %64, 64
  %142 = select i1 %shft.chk13, i64 %141, i64 0
  %143 = trunc i64 %142 to i32
  %144 = trunc i64 %92 to i32
  %145 = xor i32 %143, %144
  %region_0_250_constant_104 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %146 = xor i32 %145, %region_0_250_constant_104
  %147 = zext i32 %146 to i64
  %148 = mul i64 %147, %region_0_250_constant_18
  %149 = trunc i64 %148 to i32
  %150 = xor i32 %140, %149
  %region_0_250_constant_111 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %151 = xor i32 %150, %region_0_250_constant_111
  %152 = zext i32 %151 to i64
  %153 = mul i64 %152, %region_0_250_constant_46
  %154 = lshr i64 %153, %64
  %shft.chk14 = icmp ult i64 %64, 64
  %155 = select i1 %shft.chk14, i64 %154, i64 0
  %156 = trunc i64 %155 to i32
  %157 = lshr i64 %148, %64
  %shft.chk15 = icmp ult i64 %64, 64
  %158 = select i1 %shft.chk15, i64 %157, i64 0
  %159 = trunc i64 %158 to i32
  %160 = trunc i64 %103 to i32
  %161 = xor i32 %159, %160
  %region_0_250_constant_122 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %162 = xor i32 %161, %region_0_250_constant_122
  %163 = zext i32 %162 to i64
  %164 = mul i64 %163, %region_0_250_constant_46
  %165 = trunc i64 %164 to i32
  %166 = xor i32 %156, %165
  %region_0_250_constant_129 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %167 = xor i32 %166, %region_0_250_constant_129
  %168 = zext i32 %167 to i64
  %169 = mul i64 %168, %region_0_250_constant_18
  %170 = lshr i64 %169, %64
  %shft.chk16 = icmp ult i64 %64, 64
  %171 = select i1 %shft.chk16, i64 %170, i64 0
  %172 = trunc i64 %171 to i32
  %173 = lshr i64 %164, %64
  %shft.chk17 = icmp ult i64 %64, 64
  %174 = select i1 %shft.chk17, i64 %173, i64 0
  %175 = trunc i64 %174 to i32
  %176 = trunc i64 %121 to i32
  %177 = xor i32 %175, %176
  %region_0_250_constant_140 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %178 = xor i32 %177, %region_0_250_constant_140
  %179 = zext i32 %178 to i64
  %180 = mul i64 %179, %region_0_250_constant_18
  %181 = trunc i64 %180 to i32
  %182 = xor i32 %172, %181
  %region_0_250_constant_147 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %183 = xor i32 %182, %region_0_250_constant_147
  %184 = zext i32 %183 to i64
  %185 = mul i64 %184, %region_0_250_constant_46
  %186 = lshr i64 %185, %64
  %shft.chk18 = icmp ult i64 %64, 64
  %187 = select i1 %shft.chk18, i64 %186, i64 0
  %188 = trunc i64 %187 to i32
  %189 = lshr i64 %180, %64
  %shft.chk19 = icmp ult i64 %64, 64
  %190 = select i1 %shft.chk19, i64 %189, i64 0
  %191 = trunc i64 %190 to i32
  %192 = trunc i64 %137 to i32
  %193 = xor i32 %191, %192
  %region_0_250_constant_158 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %194 = xor i32 %193, %region_0_250_constant_158
  %195 = zext i32 %194 to i64
  %196 = mul i64 %195, %region_0_250_constant_46
  %197 = trunc i64 %196 to i32
  %198 = xor i32 %188, %197
  %region_0_250_constant_165 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %199 = xor i32 %198, %region_0_250_constant_165
  %200 = zext i32 %199 to i64
  %201 = mul i64 %200, %region_0_250_constant_18
  %202 = lshr i64 %201, %64
  %shft.chk20 = icmp ult i64 %64, 64
  %203 = select i1 %shft.chk20, i64 %202, i64 0
  %204 = trunc i64 %203 to i32
  %205 = lshr i64 %196, %64
  %shft.chk21 = icmp ult i64 %64, 64
  %206 = select i1 %shft.chk21, i64 %205, i64 0
  %207 = trunc i64 %206 to i32
  %208 = trunc i64 %153 to i32
  %209 = xor i32 %207, %208
  %region_0_250_constant_176 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %210 = xor i32 %209, %region_0_250_constant_176
  %211 = zext i32 %210 to i64
  %212 = mul i64 %211, %region_0_250_constant_18
  %213 = trunc i64 %212 to i32
  %214 = xor i32 %204, %213
  %region_0_250_constant_183 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %215 = xor i32 %214, %region_0_250_constant_183
  %216 = zext i32 %215 to i64
  %217 = mul i64 %216, %region_0_250_constant_46
  %218 = lshr i64 %217, %64
  %shft.chk22 = icmp ult i64 %64, 64
  %219 = select i1 %shft.chk22, i64 %218, i64 0
  %220 = trunc i64 %219 to i32
  %221 = lshr i64 %212, %64
  %shft.chk23 = icmp ult i64 %64, 64
  %222 = select i1 %shft.chk23, i64 %221, i64 0
  %223 = trunc i64 %222 to i32
  %224 = trunc i64 %169 to i32
  %225 = xor i32 %223, %224
  %region_0_250_constant_194 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %226 = xor i32 %225, %region_0_250_constant_194
  %227 = zext i32 %226 to i64
  %228 = mul i64 %227, %region_0_250_constant_46
  %229 = trunc i64 %228 to i32
  %230 = xor i32 %220, %229
  %region_0_250_constant_201 = load i32, i32* bitcast ([4 x i8]* @56 to i32*), align 4
  %231 = xor i32 %230, %region_0_250_constant_201
  br label %concatenate.226.merge

concat_index_from_operand_id1:                    ; preds = %concatenate.pivot.1.149
  %232 = phi i32 [ 1, %concatenate.pivot.1.149 ]
  %233 = sub nsw i32 %39, %232
  %234 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %235 = load i64, i64* %234, align 8, !invariant.load !22
  %236 = trunc i64 %235 to i32
  %237 = zext i32 %236 to i64
  %238 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %239 = lshr i64 %235, %238
  %shft.chk24 = icmp ult i64 %238, 64
  %240 = select i1 %shft.chk24, i64 %239, i64 0
  %241 = trunc i64 %240 to i32
  %242 = zext i32 %241 to i64
  %243 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %244 = shl i64 %242, %243
  %shft.chk25 = icmp ult i64 %243, 64
  %245 = select i1 %shft.chk25, i64 %244, i64 0
  %246 = or i64 %237, %245
  %247 = mul nuw nsw i32 %40, 1
  %248 = add nuw nsw i32 0, %247
  %249 = zext i32 %248 to i64
  %250 = add i64 %246, %249
  %251 = trunc i64 %250 to i32
  %252 = zext i32 %251 to i64
  %region_0_250_constant_1826 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %253 = mul i64 %252, %region_0_250_constant_1826
  %254 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %255 = lshr i64 %253, %254
  %shft.chk27 = icmp ult i64 %254, 64
  %256 = select i1 %shft.chk27, i64 %255, i64 0
  %257 = trunc i64 %256 to i32
  %258 = icmp ult i64 %250, %246
  %259 = zext i1 %258 to i8
  %260 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %261 = load i64, i64* %260, align 8, !invariant.load !22
  %262 = trunc i64 %261 to i32
  %263 = zext i32 %262 to i64
  %264 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %265 = lshr i64 %261, %264
  %shft.chk28 = icmp ult i64 %264, 64
  %266 = select i1 %shft.chk28, i64 %265, i64 0
  %267 = trunc i64 %266 to i32
  %268 = zext i32 %267 to i64
  %269 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %270 = shl i64 %268, %269
  %shft.chk29 = icmp ult i64 %269, 64
  %271 = select i1 %shft.chk29, i64 %270, i64 0
  %272 = or i64 %263, %271
  %region_0_250_constant_3430 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %273 = add i64 %272, %region_0_250_constant_3430
  %274 = trunc i8 %259 to i1
  %275 = select i1 %274, i64 %273, i64 %272
  %276 = lshr i64 %275, %254
  %shft.chk31 = icmp ult i64 %254, 64
  %277 = select i1 %shft.chk31, i64 %276, i64 0
  %278 = trunc i64 %277 to i32
  %279 = xor i32 %257, %278
  %region_0_250_constant_4232 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %280 = xor i32 %279, %region_0_250_constant_4232
  %281 = zext i32 %280 to i64
  %region_0_250_constant_4633 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %282 = mul i64 %281, %region_0_250_constant_4633
  %283 = lshr i64 %282, %254
  %shft.chk34 = icmp ult i64 %254, 64
  %284 = select i1 %shft.chk34, i64 %283, i64 0
  %285 = trunc i64 %284 to i32
  %286 = trunc i64 %275 to i32
  %287 = zext i32 %286 to i64
  %288 = mul i64 %287, %region_0_250_constant_4633
  %289 = trunc i64 %288 to i32
  %290 = xor i32 %285, %289
  %region_0_250_constant_5635 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %291 = xor i32 %290, %region_0_250_constant_5635
  %292 = zext i32 %291 to i64
  %293 = mul i64 %292, %region_0_250_constant_1826
  %294 = lshr i64 %293, %254
  %shft.chk36 = icmp ult i64 %254, 64
  %295 = select i1 %shft.chk36, i64 %294, i64 0
  %296 = trunc i64 %295 to i32
  %297 = lshr i64 %288, %254
  %shft.chk37 = icmp ult i64 %254, 64
  %298 = select i1 %shft.chk37, i64 %297, i64 0
  %299 = trunc i64 %298 to i32
  %300 = lshr i64 %250, %254
  %shft.chk38 = icmp ult i64 %254, 64
  %301 = select i1 %shft.chk38, i64 %300, i64 0
  %302 = trunc i64 %301 to i32
  %303 = xor i32 %299, %302
  %region_0_250_constant_6839 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %304 = xor i32 %303, %region_0_250_constant_6839
  %305 = zext i32 %304 to i64
  %306 = mul i64 %305, %region_0_250_constant_1826
  %307 = trunc i64 %306 to i32
  %308 = xor i32 %296, %307
  %region_0_250_constant_7540 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %309 = xor i32 %308, %region_0_250_constant_7540
  %310 = zext i32 %309 to i64
  %311 = mul i64 %310, %region_0_250_constant_4633
  %312 = lshr i64 %311, %254
  %shft.chk41 = icmp ult i64 %254, 64
  %313 = select i1 %shft.chk41, i64 %312, i64 0
  %314 = trunc i64 %313 to i32
  %315 = lshr i64 %306, %254
  %shft.chk42 = icmp ult i64 %254, 64
  %316 = select i1 %shft.chk42, i64 %315, i64 0
  %317 = trunc i64 %316 to i32
  %318 = trunc i64 %253 to i32
  %319 = xor i32 %317, %318
  %region_0_250_constant_8643 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %320 = xor i32 %319, %region_0_250_constant_8643
  %321 = zext i32 %320 to i64
  %322 = mul i64 %321, %region_0_250_constant_4633
  %323 = trunc i64 %322 to i32
  %324 = xor i32 %314, %323
  %region_0_250_constant_9344 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %325 = xor i32 %324, %region_0_250_constant_9344
  %326 = zext i32 %325 to i64
  %327 = mul i64 %326, %region_0_250_constant_1826
  %328 = lshr i64 %327, %254
  %shft.chk45 = icmp ult i64 %254, 64
  %329 = select i1 %shft.chk45, i64 %328, i64 0
  %330 = trunc i64 %329 to i32
  %331 = lshr i64 %322, %254
  %shft.chk46 = icmp ult i64 %254, 64
  %332 = select i1 %shft.chk46, i64 %331, i64 0
  %333 = trunc i64 %332 to i32
  %334 = trunc i64 %282 to i32
  %335 = xor i32 %333, %334
  %region_0_250_constant_10447 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %336 = xor i32 %335, %region_0_250_constant_10447
  %337 = zext i32 %336 to i64
  %338 = mul i64 %337, %region_0_250_constant_1826
  %339 = trunc i64 %338 to i32
  %340 = xor i32 %330, %339
  %region_0_250_constant_11148 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %341 = xor i32 %340, %region_0_250_constant_11148
  %342 = zext i32 %341 to i64
  %343 = mul i64 %342, %region_0_250_constant_4633
  %344 = lshr i64 %343, %254
  %shft.chk49 = icmp ult i64 %254, 64
  %345 = select i1 %shft.chk49, i64 %344, i64 0
  %346 = trunc i64 %345 to i32
  %347 = lshr i64 %338, %254
  %shft.chk50 = icmp ult i64 %254, 64
  %348 = select i1 %shft.chk50, i64 %347, i64 0
  %349 = trunc i64 %348 to i32
  %350 = trunc i64 %293 to i32
  %351 = xor i32 %349, %350
  %region_0_250_constant_12251 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %352 = xor i32 %351, %region_0_250_constant_12251
  %353 = zext i32 %352 to i64
  %354 = mul i64 %353, %region_0_250_constant_4633
  %355 = trunc i64 %354 to i32
  %356 = xor i32 %346, %355
  %region_0_250_constant_12952 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %357 = xor i32 %356, %region_0_250_constant_12952
  %358 = zext i32 %357 to i64
  %359 = mul i64 %358, %region_0_250_constant_1826
  %360 = lshr i64 %359, %254
  %shft.chk53 = icmp ult i64 %254, 64
  %361 = select i1 %shft.chk53, i64 %360, i64 0
  %362 = trunc i64 %361 to i32
  %363 = lshr i64 %354, %254
  %shft.chk54 = icmp ult i64 %254, 64
  %364 = select i1 %shft.chk54, i64 %363, i64 0
  %365 = trunc i64 %364 to i32
  %366 = trunc i64 %311 to i32
  %367 = xor i32 %365, %366
  %region_0_250_constant_14055 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %368 = xor i32 %367, %region_0_250_constant_14055
  %369 = zext i32 %368 to i64
  %370 = mul i64 %369, %region_0_250_constant_1826
  %371 = trunc i64 %370 to i32
  %372 = xor i32 %362, %371
  %region_0_250_constant_14756 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %373 = xor i32 %372, %region_0_250_constant_14756
  %374 = zext i32 %373 to i64
  %375 = mul i64 %374, %region_0_250_constant_4633
  %376 = lshr i64 %375, %254
  %shft.chk57 = icmp ult i64 %254, 64
  %377 = select i1 %shft.chk57, i64 %376, i64 0
  %378 = trunc i64 %377 to i32
  %379 = lshr i64 %370, %254
  %shft.chk58 = icmp ult i64 %254, 64
  %380 = select i1 %shft.chk58, i64 %379, i64 0
  %381 = trunc i64 %380 to i32
  %382 = trunc i64 %327 to i32
  %383 = xor i32 %381, %382
  %region_0_250_constant_15859 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %384 = xor i32 %383, %region_0_250_constant_15859
  %385 = zext i32 %384 to i64
  %386 = mul i64 %385, %region_0_250_constant_4633
  %387 = trunc i64 %386 to i32
  %388 = xor i32 %378, %387
  %region_0_250_constant_16560 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %389 = xor i32 %388, %region_0_250_constant_16560
  %390 = zext i32 %389 to i64
  %391 = mul i64 %390, %region_0_250_constant_1826
  %392 = lshr i64 %391, %254
  %shft.chk61 = icmp ult i64 %254, 64
  %393 = select i1 %shft.chk61, i64 %392, i64 0
  %394 = trunc i64 %393 to i32
  %395 = lshr i64 %386, %254
  %shft.chk62 = icmp ult i64 %254, 64
  %396 = select i1 %shft.chk62, i64 %395, i64 0
  %397 = trunc i64 %396 to i32
  %398 = trunc i64 %343 to i32
  %399 = xor i32 %397, %398
  %region_0_250_constant_17663 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %400 = xor i32 %399, %region_0_250_constant_17663
  %401 = zext i32 %400 to i64
  %402 = mul i64 %401, %region_0_250_constant_1826
  %403 = trunc i64 %402 to i32
  %404 = xor i32 %394, %403
  %region_0_250_constant_18364 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %405 = xor i32 %404, %region_0_250_constant_18364
  %406 = zext i32 %405 to i64
  %407 = mul i64 %406, %region_0_250_constant_4633
  %408 = trunc i64 %407 to i32
  br label %concatenate.226.merge

concat_index_from_operand_id2:                    ; preds = %concatenate.pivot.2.150
  %409 = phi i32 [ 2, %concatenate.pivot.2.150 ]
  %410 = sub nsw i32 %39, %409
  %411 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %412 = load i64, i64* %411, align 8, !invariant.load !22
  %413 = trunc i64 %412 to i32
  %414 = zext i32 %413 to i64
  %415 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %416 = lshr i64 %412, %415
  %shft.chk65 = icmp ult i64 %415, 64
  %417 = select i1 %shft.chk65, i64 %416, i64 0
  %418 = trunc i64 %417 to i32
  %419 = zext i32 %418 to i64
  %420 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %421 = shl i64 %419, %420
  %shft.chk66 = icmp ult i64 %420, 64
  %422 = select i1 %shft.chk66, i64 %421, i64 0
  %423 = or i64 %414, %422
  %424 = mul nuw nsw i32 %40, 1
  %425 = add nuw nsw i32 0, %424
  %426 = zext i32 %425 to i64
  %427 = add i64 %423, %426
  %428 = icmp ult i64 %427, %423
  %429 = zext i1 %428 to i8
  %430 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %431 = load i64, i64* %430, align 8, !invariant.load !22
  %432 = trunc i64 %431 to i32
  %433 = zext i32 %432 to i64
  %434 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %435 = lshr i64 %431, %434
  %shft.chk67 = icmp ult i64 %434, 64
  %436 = select i1 %shft.chk67, i64 %435, i64 0
  %437 = trunc i64 %436 to i32
  %438 = zext i32 %437 to i64
  %439 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %440 = shl i64 %438, %439
  %shft.chk68 = icmp ult i64 %439, 64
  %441 = select i1 %shft.chk68, i64 %440, i64 0
  %442 = or i64 %433, %441
  %region_0_250_constant_3469 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %443 = add i64 %442, %region_0_250_constant_3469
  %444 = trunc i8 %429 to i1
  %445 = select i1 %444, i64 %443, i64 %442
  %446 = trunc i64 %445 to i32
  %447 = zext i32 %446 to i64
  %region_0_250_constant_4670 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %448 = mul i64 %447, %region_0_250_constant_4670
  %449 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %450 = lshr i64 %448, %449
  %shft.chk71 = icmp ult i64 %449, 64
  %451 = select i1 %shft.chk71, i64 %450, i64 0
  %452 = trunc i64 %451 to i32
  %453 = lshr i64 %427, %449
  %shft.chk72 = icmp ult i64 %449, 64
  %454 = select i1 %shft.chk72, i64 %453, i64 0
  %455 = trunc i64 %454 to i32
  %456 = xor i32 %452, %455
  %region_0_250_constant_6873 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %457 = xor i32 %456, %region_0_250_constant_6873
  %458 = zext i32 %457 to i64
  %region_0_250_constant_1874 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %459 = mul i64 %458, %region_0_250_constant_1874
  %460 = lshr i64 %459, %449
  %shft.chk75 = icmp ult i64 %449, 64
  %461 = select i1 %shft.chk75, i64 %460, i64 0
  %462 = trunc i64 %461 to i32
  %463 = trunc i64 %427 to i32
  %464 = zext i32 %463 to i64
  %465 = mul i64 %464, %region_0_250_constant_1874
  %466 = trunc i64 %465 to i32
  %467 = xor i32 %462, %466
  %region_0_250_constant_8676 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %468 = xor i32 %467, %region_0_250_constant_8676
  %469 = zext i32 %468 to i64
  %470 = mul i64 %469, %region_0_250_constant_4670
  %471 = lshr i64 %470, %449
  %shft.chk77 = icmp ult i64 %449, 64
  %472 = select i1 %shft.chk77, i64 %471, i64 0
  %473 = trunc i64 %472 to i32
  %474 = lshr i64 %465, %449
  %shft.chk78 = icmp ult i64 %449, 64
  %475 = select i1 %shft.chk78, i64 %474, i64 0
  %476 = trunc i64 %475 to i32
  %477 = lshr i64 %445, %449
  %shft.chk79 = icmp ult i64 %449, 64
  %478 = select i1 %shft.chk79, i64 %477, i64 0
  %479 = trunc i64 %478 to i32
  %480 = xor i32 %476, %479
  %region_0_250_constant_4280 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %481 = xor i32 %480, %region_0_250_constant_4280
  %482 = zext i32 %481 to i64
  %483 = mul i64 %482, %region_0_250_constant_4670
  %484 = trunc i64 %483 to i32
  %485 = xor i32 %473, %484
  %region_0_250_constant_10481 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %486 = xor i32 %485, %region_0_250_constant_10481
  %487 = zext i32 %486 to i64
  %488 = mul i64 %487, %region_0_250_constant_1874
  %489 = lshr i64 %488, %449
  %shft.chk82 = icmp ult i64 %449, 64
  %490 = select i1 %shft.chk82, i64 %489, i64 0
  %491 = trunc i64 %490 to i32
  %492 = lshr i64 %483, %449
  %shft.chk83 = icmp ult i64 %449, 64
  %493 = select i1 %shft.chk83, i64 %492, i64 0
  %494 = trunc i64 %493 to i32
  %495 = trunc i64 %448 to i32
  %496 = xor i32 %494, %495
  %region_0_250_constant_5684 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %497 = xor i32 %496, %region_0_250_constant_5684
  %498 = zext i32 %497 to i64
  %499 = mul i64 %498, %region_0_250_constant_1874
  %500 = trunc i64 %499 to i32
  %501 = xor i32 %491, %500
  %region_0_250_constant_12285 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %502 = xor i32 %501, %region_0_250_constant_12285
  %503 = zext i32 %502 to i64
  %504 = mul i64 %503, %region_0_250_constant_4670
  %505 = lshr i64 %504, %449
  %shft.chk86 = icmp ult i64 %449, 64
  %506 = select i1 %shft.chk86, i64 %505, i64 0
  %507 = trunc i64 %506 to i32
  %508 = lshr i64 %499, %449
  %shft.chk87 = icmp ult i64 %449, 64
  %509 = select i1 %shft.chk87, i64 %508, i64 0
  %510 = trunc i64 %509 to i32
  %511 = trunc i64 %459 to i32
  %512 = xor i32 %510, %511
  %region_0_250_constant_7588 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %513 = xor i32 %512, %region_0_250_constant_7588
  %514 = zext i32 %513 to i64
  %515 = mul i64 %514, %region_0_250_constant_4670
  %516 = trunc i64 %515 to i32
  %517 = xor i32 %507, %516
  %region_0_250_constant_14089 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %518 = xor i32 %517, %region_0_250_constant_14089
  %519 = zext i32 %518 to i64
  %520 = mul i64 %519, %region_0_250_constant_1874
  %521 = lshr i64 %520, %449
  %shft.chk90 = icmp ult i64 %449, 64
  %522 = select i1 %shft.chk90, i64 %521, i64 0
  %523 = trunc i64 %522 to i32
  %524 = lshr i64 %515, %449
  %shft.chk91 = icmp ult i64 %449, 64
  %525 = select i1 %shft.chk91, i64 %524, i64 0
  %526 = trunc i64 %525 to i32
  %527 = trunc i64 %470 to i32
  %528 = xor i32 %526, %527
  %region_0_250_constant_9392 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %529 = xor i32 %528, %region_0_250_constant_9392
  %530 = zext i32 %529 to i64
  %531 = mul i64 %530, %region_0_250_constant_1874
  %532 = trunc i64 %531 to i32
  %533 = xor i32 %523, %532
  %region_0_250_constant_15893 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %534 = xor i32 %533, %region_0_250_constant_15893
  %535 = zext i32 %534 to i64
  %536 = mul i64 %535, %region_0_250_constant_4670
  %537 = lshr i64 %536, %449
  %shft.chk94 = icmp ult i64 %449, 64
  %538 = select i1 %shft.chk94, i64 %537, i64 0
  %539 = trunc i64 %538 to i32
  %540 = lshr i64 %531, %449
  %shft.chk95 = icmp ult i64 %449, 64
  %541 = select i1 %shft.chk95, i64 %540, i64 0
  %542 = trunc i64 %541 to i32
  %543 = trunc i64 %488 to i32
  %544 = xor i32 %542, %543
  %region_0_250_constant_11196 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %545 = xor i32 %544, %region_0_250_constant_11196
  %546 = zext i32 %545 to i64
  %547 = mul i64 %546, %region_0_250_constant_4670
  %548 = trunc i64 %547 to i32
  %549 = xor i32 %539, %548
  %region_0_250_constant_17697 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %550 = xor i32 %549, %region_0_250_constant_17697
  %551 = zext i32 %550 to i64
  %552 = mul i64 %551, %region_0_250_constant_1874
  %553 = lshr i64 %552, %449
  %shft.chk98 = icmp ult i64 %449, 64
  %554 = select i1 %shft.chk98, i64 %553, i64 0
  %555 = trunc i64 %554 to i32
  %556 = lshr i64 %547, %449
  %shft.chk99 = icmp ult i64 %449, 64
  %557 = select i1 %shft.chk99, i64 %556, i64 0
  %558 = trunc i64 %557 to i32
  %559 = trunc i64 %504 to i32
  %560 = xor i32 %558, %559
  %region_0_250_constant_129100 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %561 = xor i32 %560, %region_0_250_constant_129100
  %562 = zext i32 %561 to i64
  %563 = mul i64 %562, %region_0_250_constant_1874
  %564 = trunc i64 %563 to i32
  %565 = xor i32 %555, %564
  %region_0_250_constant_194101 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %566 = xor i32 %565, %region_0_250_constant_194101
  %567 = zext i32 %566 to i64
  %568 = mul i64 %567, %region_0_250_constant_4670
  %569 = lshr i64 %568, %449
  %shft.chk102 = icmp ult i64 %449, 64
  %570 = select i1 %shft.chk102, i64 %569, i64 0
  %571 = trunc i64 %570 to i32
  %572 = lshr i64 %563, %449
  %shft.chk103 = icmp ult i64 %449, 64
  %573 = select i1 %shft.chk103, i64 %572, i64 0
  %574 = trunc i64 %573 to i32
  %575 = trunc i64 %520 to i32
  %576 = xor i32 %574, %575
  %region_0_250_constant_147104 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %577 = xor i32 %576, %region_0_250_constant_147104
  %578 = zext i32 %577 to i64
  %579 = mul i64 %578, %region_0_250_constant_4670
  %580 = trunc i64 %579 to i32
  %581 = xor i32 %571, %580
  %region_0_250_constant_211 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %582 = xor i32 %581, %region_0_250_constant_211
  %583 = zext i32 %582 to i64
  %584 = mul i64 %583, %region_0_250_constant_1874
  %585 = lshr i64 %584, %449
  %shft.chk105 = icmp ult i64 %449, 64
  %586 = select i1 %shft.chk105, i64 %585, i64 0
  %587 = trunc i64 %586 to i32
  %588 = lshr i64 %579, %449
  %shft.chk106 = icmp ult i64 %449, 64
  %589 = select i1 %shft.chk106, i64 %588, i64 0
  %590 = trunc i64 %589 to i32
  %591 = trunc i64 %536 to i32
  %592 = xor i32 %590, %591
  %region_0_250_constant_165107 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %593 = xor i32 %592, %region_0_250_constant_165107
  %594 = zext i32 %593 to i64
  %595 = mul i64 %594, %region_0_250_constant_1874
  %596 = trunc i64 %595 to i32
  %597 = xor i32 %587, %596
  %region_0_250_constant_220 = load i32, i32* bitcast ([4 x i8]* @53 to i32*), align 4
  %598 = xor i32 %597, %region_0_250_constant_220
  br label %concatenate.226.merge

concat_index_from_operand_id3:                    ; preds = %concatenate.pivot.3.151
  %599 = phi i32 [ 3, %concatenate.pivot.3.151 ]
  %600 = sub nsw i32 %39, %599
  %601 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %602 = load i64, i64* %601, align 8, !invariant.load !22
  %603 = trunc i64 %602 to i32
  %604 = zext i32 %603 to i64
  %605 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %606 = lshr i64 %602, %605
  %shft.chk108 = icmp ult i64 %605, 64
  %607 = select i1 %shft.chk108, i64 %606, i64 0
  %608 = trunc i64 %607 to i32
  %609 = zext i32 %608 to i64
  %610 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %611 = shl i64 %609, %610
  %shft.chk109 = icmp ult i64 %610, 64
  %612 = select i1 %shft.chk109, i64 %611, i64 0
  %613 = or i64 %604, %612
  %614 = mul nuw nsw i32 %40, 1
  %615 = add nuw nsw i32 0, %614
  %616 = zext i32 %615 to i64
  %617 = add i64 %613, %616
  %618 = icmp ult i64 %617, %613
  %619 = zext i1 %618 to i8
  %620 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %621 = load i64, i64* %620, align 8, !invariant.load !22
  %622 = trunc i64 %621 to i32
  %623 = zext i32 %622 to i64
  %624 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %625 = lshr i64 %621, %624
  %shft.chk110 = icmp ult i64 %624, 64
  %626 = select i1 %shft.chk110, i64 %625, i64 0
  %627 = trunc i64 %626 to i32
  %628 = zext i32 %627 to i64
  %629 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %630 = shl i64 %628, %629
  %shft.chk111 = icmp ult i64 %629, 64
  %631 = select i1 %shft.chk111, i64 %630, i64 0
  %632 = or i64 %623, %631
  %region_0_250_constant_34112 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %633 = add i64 %632, %region_0_250_constant_34112
  %634 = trunc i8 %619 to i1
  %635 = select i1 %634, i64 %633, i64 %632
  %636 = trunc i64 %635 to i32
  %637 = zext i32 %636 to i64
  %region_0_250_constant_46113 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %638 = mul i64 %637, %region_0_250_constant_46113
  %639 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %640 = lshr i64 %638, %639
  %shft.chk114 = icmp ult i64 %639, 64
  %641 = select i1 %shft.chk114, i64 %640, i64 0
  %642 = trunc i64 %641 to i32
  %643 = lshr i64 %617, %639
  %shft.chk115 = icmp ult i64 %639, 64
  %644 = select i1 %shft.chk115, i64 %643, i64 0
  %645 = trunc i64 %644 to i32
  %646 = xor i32 %642, %645
  %region_0_250_constant_68116 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %647 = xor i32 %646, %region_0_250_constant_68116
  %648 = zext i32 %647 to i64
  %region_0_250_constant_18117 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %649 = mul i64 %648, %region_0_250_constant_18117
  %650 = lshr i64 %649, %639
  %shft.chk118 = icmp ult i64 %639, 64
  %651 = select i1 %shft.chk118, i64 %650, i64 0
  %652 = trunc i64 %651 to i32
  %653 = trunc i64 %617 to i32
  %654 = zext i32 %653 to i64
  %655 = mul i64 %654, %region_0_250_constant_18117
  %656 = trunc i64 %655 to i32
  %657 = xor i32 %652, %656
  %region_0_250_constant_86119 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %658 = xor i32 %657, %region_0_250_constant_86119
  %659 = zext i32 %658 to i64
  %660 = mul i64 %659, %region_0_250_constant_46113
  %661 = lshr i64 %660, %639
  %shft.chk120 = icmp ult i64 %639, 64
  %662 = select i1 %shft.chk120, i64 %661, i64 0
  %663 = trunc i64 %662 to i32
  %664 = lshr i64 %655, %639
  %shft.chk121 = icmp ult i64 %639, 64
  %665 = select i1 %shft.chk121, i64 %664, i64 0
  %666 = trunc i64 %665 to i32
  %667 = lshr i64 %635, %639
  %shft.chk122 = icmp ult i64 %639, 64
  %668 = select i1 %shft.chk122, i64 %667, i64 0
  %669 = trunc i64 %668 to i32
  %670 = xor i32 %666, %669
  %region_0_250_constant_42123 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %671 = xor i32 %670, %region_0_250_constant_42123
  %672 = zext i32 %671 to i64
  %673 = mul i64 %672, %region_0_250_constant_46113
  %674 = trunc i64 %673 to i32
  %675 = xor i32 %663, %674
  %region_0_250_constant_104124 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %676 = xor i32 %675, %region_0_250_constant_104124
  %677 = zext i32 %676 to i64
  %678 = mul i64 %677, %region_0_250_constant_18117
  %679 = lshr i64 %678, %639
  %shft.chk125 = icmp ult i64 %639, 64
  %680 = select i1 %shft.chk125, i64 %679, i64 0
  %681 = trunc i64 %680 to i32
  %682 = lshr i64 %673, %639
  %shft.chk126 = icmp ult i64 %639, 64
  %683 = select i1 %shft.chk126, i64 %682, i64 0
  %684 = trunc i64 %683 to i32
  %685 = trunc i64 %638 to i32
  %686 = xor i32 %684, %685
  %region_0_250_constant_56127 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %687 = xor i32 %686, %region_0_250_constant_56127
  %688 = zext i32 %687 to i64
  %689 = mul i64 %688, %region_0_250_constant_18117
  %690 = trunc i64 %689 to i32
  %691 = xor i32 %681, %690
  %region_0_250_constant_122128 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %692 = xor i32 %691, %region_0_250_constant_122128
  %693 = zext i32 %692 to i64
  %694 = mul i64 %693, %region_0_250_constant_46113
  %695 = lshr i64 %694, %639
  %shft.chk129 = icmp ult i64 %639, 64
  %696 = select i1 %shft.chk129, i64 %695, i64 0
  %697 = trunc i64 %696 to i32
  %698 = lshr i64 %689, %639
  %shft.chk130 = icmp ult i64 %639, 64
  %699 = select i1 %shft.chk130, i64 %698, i64 0
  %700 = trunc i64 %699 to i32
  %701 = trunc i64 %649 to i32
  %702 = xor i32 %700, %701
  %region_0_250_constant_75131 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %703 = xor i32 %702, %region_0_250_constant_75131
  %704 = zext i32 %703 to i64
  %705 = mul i64 %704, %region_0_250_constant_46113
  %706 = trunc i64 %705 to i32
  %707 = xor i32 %697, %706
  %region_0_250_constant_140132 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %708 = xor i32 %707, %region_0_250_constant_140132
  %709 = zext i32 %708 to i64
  %710 = mul i64 %709, %region_0_250_constant_18117
  %711 = lshr i64 %710, %639
  %shft.chk133 = icmp ult i64 %639, 64
  %712 = select i1 %shft.chk133, i64 %711, i64 0
  %713 = trunc i64 %712 to i32
  %714 = lshr i64 %705, %639
  %shft.chk134 = icmp ult i64 %639, 64
  %715 = select i1 %shft.chk134, i64 %714, i64 0
  %716 = trunc i64 %715 to i32
  %717 = trunc i64 %660 to i32
  %718 = xor i32 %716, %717
  %region_0_250_constant_93135 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %719 = xor i32 %718, %region_0_250_constant_93135
  %720 = zext i32 %719 to i64
  %721 = mul i64 %720, %region_0_250_constant_18117
  %722 = trunc i64 %721 to i32
  %723 = xor i32 %713, %722
  %region_0_250_constant_158136 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %724 = xor i32 %723, %region_0_250_constant_158136
  %725 = zext i32 %724 to i64
  %726 = mul i64 %725, %region_0_250_constant_46113
  %727 = lshr i64 %726, %639
  %shft.chk137 = icmp ult i64 %639, 64
  %728 = select i1 %shft.chk137, i64 %727, i64 0
  %729 = trunc i64 %728 to i32
  %730 = lshr i64 %721, %639
  %shft.chk138 = icmp ult i64 %639, 64
  %731 = select i1 %shft.chk138, i64 %730, i64 0
  %732 = trunc i64 %731 to i32
  %733 = trunc i64 %678 to i32
  %734 = xor i32 %732, %733
  %region_0_250_constant_111139 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %735 = xor i32 %734, %region_0_250_constant_111139
  %736 = zext i32 %735 to i64
  %737 = mul i64 %736, %region_0_250_constant_46113
  %738 = trunc i64 %737 to i32
  %739 = xor i32 %729, %738
  %region_0_250_constant_176140 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %740 = xor i32 %739, %region_0_250_constant_176140
  %741 = zext i32 %740 to i64
  %742 = mul i64 %741, %region_0_250_constant_18117
  %743 = lshr i64 %742, %639
  %shft.chk141 = icmp ult i64 %639, 64
  %744 = select i1 %shft.chk141, i64 %743, i64 0
  %745 = trunc i64 %744 to i32
  %746 = lshr i64 %737, %639
  %shft.chk142 = icmp ult i64 %639, 64
  %747 = select i1 %shft.chk142, i64 %746, i64 0
  %748 = trunc i64 %747 to i32
  %749 = trunc i64 %694 to i32
  %750 = xor i32 %748, %749
  %region_0_250_constant_129143 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %751 = xor i32 %750, %region_0_250_constant_129143
  %752 = zext i32 %751 to i64
  %753 = mul i64 %752, %region_0_250_constant_18117
  %754 = trunc i64 %753 to i32
  %755 = xor i32 %745, %754
  %region_0_250_constant_194144 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %756 = xor i32 %755, %region_0_250_constant_194144
  %757 = zext i32 %756 to i64
  %758 = mul i64 %757, %region_0_250_constant_46113
  %759 = lshr i64 %758, %639
  %shft.chk145 = icmp ult i64 %639, 64
  %760 = select i1 %shft.chk145, i64 %759, i64 0
  %761 = trunc i64 %760 to i32
  %762 = lshr i64 %753, %639
  %shft.chk146 = icmp ult i64 %639, 64
  %763 = select i1 %shft.chk146, i64 %762, i64 0
  %764 = trunc i64 %763 to i32
  %765 = trunc i64 %710 to i32
  %766 = xor i32 %764, %765
  %region_0_250_constant_147147 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %767 = xor i32 %766, %region_0_250_constant_147147
  %768 = zext i32 %767 to i64
  %769 = mul i64 %768, %region_0_250_constant_46113
  %770 = trunc i64 %769 to i32
  %771 = xor i32 %761, %770
  %region_0_250_constant_211148 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %772 = xor i32 %771, %region_0_250_constant_211148
  %773 = zext i32 %772 to i64
  %774 = mul i64 %773, %region_0_250_constant_18117
  %775 = trunc i64 %774 to i32
  br label %concatenate.226.merge

concatenate.pivot.2.:                             ; preds = %fusion-body
  %776 = icmp ult i32 %39, 2
  br i1 %776, label %concatenate.pivot.1., label %concatenate.pivot.3.

concatenate.pivot.1.:                             ; preds = %concatenate.pivot.2.
  %777 = icmp ult i32 %39, 1
  br i1 %777, label %concatenate.pivot.0., label %concatenate.pivot.1.149

concatenate.pivot.0.:                             ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id0

concatenate.pivot.1.149:                          ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id1

concatenate.pivot.3.:                             ; preds = %concatenate.pivot.2.
  %778 = icmp ult i32 %39, 3
  br i1 %778, label %concatenate.pivot.2.150, label %concatenate.pivot.3.151

concatenate.pivot.2.150:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id2

concatenate.pivot.3.151:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id3

concatenate.226.merge:                            ; preds = %concat_index_from_operand_id3, %concat_index_from_operand_id2, %concat_index_from_operand_id1, %concat_index_from_operand_id0
  %779 = phi i32 [ %231, %concat_index_from_operand_id0 ], [ %408, %concat_index_from_operand_id1 ], [ %598, %concat_index_from_operand_id2 ], [ %775, %concat_index_from_operand_id3 ]
  %region_0_250_constant_227 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %780 = lshr i32 %779, %region_0_250_constant_227
  %shft.chk152 = icmp ult i32 %region_0_250_constant_227, 32
  %781 = select i1 %shft.chk152, i32 %780, i32 0
  %782 = uitofp i32 %781 to float
  %region_0_250_constant_231 = load float, float* bitcast ([4 x i8]* @32 to float*), align 4
  %multiply.233 = fmul float %782, %region_0_250_constant_231
  %region_0_250_constant_234 = load float, float* bitcast ([4 x i8]* @31 to float*), align 4
  %compare.236 = fcmp olt float %multiply.233, %region_0_250_constant_234
  %783 = zext i1 %compare.236 to i8
  %region_0_250_constant_238 = load i8, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @30, i32 0, i32 0), align 1
  %784 = icmp eq i8 %783, %region_0_250_constant_238
  %785 = zext i1 %784 to i8
  %786 = mul nuw nsw i32 %31, 1
  %787 = add nuw nsw i32 0, %786
  %788 = udiv i32 %787, 16
  %789 = mul nuw nsw i32 %27, 1
  %790 = add nuw nsw i32 0, %789
  %791 = mul nuw nsw i32 %28, 32
  %792 = add nuw nsw i32 %790, %791
  %793 = udiv i32 %792, 256
  %794 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %1, i32 0, i32 %792, i32 %787
  %795 = load float, float* %794, align 4, !invariant.load !22
  %region_0_250_constant_242 = load float, float* bitcast ([4 x i8]* @29 to float*), align 4
  %796 = trunc i8 %785 to i1
  %797 = select i1 %796, float %795, float %region_0_250_constant_242
  %region_0_250_constant_245 = load float, float* bitcast ([4 x i8]* @28 to float*), align 4
  %multiply.247 = fmul float %797, %region_0_250_constant_245
  %798 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %5, i32 0, i32 %9, i32 %11
  store float %multiply.247, float* %798, align 4
  %799 = mul nuw nsw i32 %13, 1
  %800 = add nuw nsw i32 0, %799
  %801 = urem i32 %800, 32
  %802 = udiv i32 %800, 32
  %803 = udiv i32 %802, 8
  %804 = mul nuw nsw i32 %15, 1
  %805 = add nuw nsw i32 0, %804
  %806 = udiv i32 %805, 16
  %807 = mul nuw nsw i32 %805, 1
  %808 = add nuw nsw i32 0, %807
  %809 = mul nuw nsw i32 %801, 16
  %810 = add nuw nsw i32 %808, %809
  %811 = mul nuw nsw i32 %802, 512
  %812 = add nuw nsw i32 %810, %811
  %813 = urem i32 %812, 4
  %814 = udiv i32 %812, 4
  %815 = udiv i32 %814, 1024
  br label %concatenate.pivot.2.330

concat_index_from_operand_id0154:                 ; preds = %concatenate.pivot.0.332
  %816 = phi i32 [ 0, %concatenate.pivot.0.332 ]
  %817 = sub nsw i32 %813, %816
  %818 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %819 = load i64, i64* %818, align 8, !invariant.load !22
  %820 = trunc i64 %819 to i32
  %821 = zext i32 %820 to i64
  %822 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %823 = lshr i64 %819, %822
  %shft.chk155 = icmp ult i64 %822, 64
  %824 = select i1 %shft.chk155, i64 %823, i64 0
  %825 = trunc i64 %824 to i32
  %826 = zext i32 %825 to i64
  %827 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %828 = shl i64 %826, %827
  %shft.chk156 = icmp ult i64 %827, 64
  %829 = select i1 %shft.chk156, i64 %828, i64 0
  %830 = or i64 %821, %829
  %831 = mul nuw nsw i32 %814, 1
  %832 = add nuw nsw i32 0, %831
  %833 = zext i32 %832 to i64
  %834 = add i64 %830, %833
  %835 = trunc i64 %834 to i32
  %836 = zext i32 %835 to i64
  %region_0_250_constant_18157 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %837 = mul i64 %836, %region_0_250_constant_18157
  %838 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %839 = lshr i64 %837, %838
  %shft.chk158 = icmp ult i64 %838, 64
  %840 = select i1 %shft.chk158, i64 %839, i64 0
  %841 = trunc i64 %840 to i32
  %842 = icmp ult i64 %834, %830
  %843 = zext i1 %842 to i8
  %844 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %845 = load i64, i64* %844, align 8, !invariant.load !22
  %846 = trunc i64 %845 to i32
  %847 = zext i32 %846 to i64
  %848 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %849 = lshr i64 %845, %848
  %shft.chk159 = icmp ult i64 %848, 64
  %850 = select i1 %shft.chk159, i64 %849, i64 0
  %851 = trunc i64 %850 to i32
  %852 = zext i32 %851 to i64
  %853 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %854 = shl i64 %852, %853
  %shft.chk160 = icmp ult i64 %853, 64
  %855 = select i1 %shft.chk160, i64 %854, i64 0
  %856 = or i64 %847, %855
  %region_0_250_constant_34161 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %857 = add i64 %856, %region_0_250_constant_34161
  %858 = trunc i8 %843 to i1
  %859 = select i1 %858, i64 %857, i64 %856
  %860 = lshr i64 %859, %838
  %shft.chk162 = icmp ult i64 %838, 64
  %861 = select i1 %shft.chk162, i64 %860, i64 0
  %862 = trunc i64 %861 to i32
  %863 = xor i32 %841, %862
  %region_0_250_constant_42163 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %864 = xor i32 %863, %region_0_250_constant_42163
  %865 = zext i32 %864 to i64
  %region_0_250_constant_46164 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %866 = mul i64 %865, %region_0_250_constant_46164
  %867 = lshr i64 %866, %838
  %shft.chk165 = icmp ult i64 %838, 64
  %868 = select i1 %shft.chk165, i64 %867, i64 0
  %869 = trunc i64 %868 to i32
  %870 = trunc i64 %859 to i32
  %871 = zext i32 %870 to i64
  %872 = mul i64 %871, %region_0_250_constant_46164
  %873 = trunc i64 %872 to i32
  %874 = xor i32 %869, %873
  %region_0_250_constant_56166 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %875 = xor i32 %874, %region_0_250_constant_56166
  %876 = zext i32 %875 to i64
  %877 = mul i64 %876, %region_0_250_constant_18157
  %878 = lshr i64 %877, %838
  %shft.chk167 = icmp ult i64 %838, 64
  %879 = select i1 %shft.chk167, i64 %878, i64 0
  %880 = trunc i64 %879 to i32
  %881 = lshr i64 %872, %838
  %shft.chk168 = icmp ult i64 %838, 64
  %882 = select i1 %shft.chk168, i64 %881, i64 0
  %883 = trunc i64 %882 to i32
  %884 = lshr i64 %834, %838
  %shft.chk169 = icmp ult i64 %838, 64
  %885 = select i1 %shft.chk169, i64 %884, i64 0
  %886 = trunc i64 %885 to i32
  %887 = xor i32 %883, %886
  %region_0_250_constant_68170 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %888 = xor i32 %887, %region_0_250_constant_68170
  %889 = zext i32 %888 to i64
  %890 = mul i64 %889, %region_0_250_constant_18157
  %891 = trunc i64 %890 to i32
  %892 = xor i32 %880, %891
  %region_0_250_constant_75171 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %893 = xor i32 %892, %region_0_250_constant_75171
  %894 = zext i32 %893 to i64
  %895 = mul i64 %894, %region_0_250_constant_46164
  %896 = lshr i64 %895, %838
  %shft.chk172 = icmp ult i64 %838, 64
  %897 = select i1 %shft.chk172, i64 %896, i64 0
  %898 = trunc i64 %897 to i32
  %899 = lshr i64 %890, %838
  %shft.chk173 = icmp ult i64 %838, 64
  %900 = select i1 %shft.chk173, i64 %899, i64 0
  %901 = trunc i64 %900 to i32
  %902 = trunc i64 %837 to i32
  %903 = xor i32 %901, %902
  %region_0_250_constant_86174 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %904 = xor i32 %903, %region_0_250_constant_86174
  %905 = zext i32 %904 to i64
  %906 = mul i64 %905, %region_0_250_constant_46164
  %907 = trunc i64 %906 to i32
  %908 = xor i32 %898, %907
  %region_0_250_constant_93175 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %909 = xor i32 %908, %region_0_250_constant_93175
  %910 = zext i32 %909 to i64
  %911 = mul i64 %910, %region_0_250_constant_18157
  %912 = lshr i64 %911, %838
  %shft.chk176 = icmp ult i64 %838, 64
  %913 = select i1 %shft.chk176, i64 %912, i64 0
  %914 = trunc i64 %913 to i32
  %915 = lshr i64 %906, %838
  %shft.chk177 = icmp ult i64 %838, 64
  %916 = select i1 %shft.chk177, i64 %915, i64 0
  %917 = trunc i64 %916 to i32
  %918 = trunc i64 %866 to i32
  %919 = xor i32 %917, %918
  %region_0_250_constant_104178 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %920 = xor i32 %919, %region_0_250_constant_104178
  %921 = zext i32 %920 to i64
  %922 = mul i64 %921, %region_0_250_constant_18157
  %923 = trunc i64 %922 to i32
  %924 = xor i32 %914, %923
  %region_0_250_constant_111179 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %925 = xor i32 %924, %region_0_250_constant_111179
  %926 = zext i32 %925 to i64
  %927 = mul i64 %926, %region_0_250_constant_46164
  %928 = lshr i64 %927, %838
  %shft.chk180 = icmp ult i64 %838, 64
  %929 = select i1 %shft.chk180, i64 %928, i64 0
  %930 = trunc i64 %929 to i32
  %931 = lshr i64 %922, %838
  %shft.chk181 = icmp ult i64 %838, 64
  %932 = select i1 %shft.chk181, i64 %931, i64 0
  %933 = trunc i64 %932 to i32
  %934 = trunc i64 %877 to i32
  %935 = xor i32 %933, %934
  %region_0_250_constant_122182 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %936 = xor i32 %935, %region_0_250_constant_122182
  %937 = zext i32 %936 to i64
  %938 = mul i64 %937, %region_0_250_constant_46164
  %939 = trunc i64 %938 to i32
  %940 = xor i32 %930, %939
  %region_0_250_constant_129183 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %941 = xor i32 %940, %region_0_250_constant_129183
  %942 = zext i32 %941 to i64
  %943 = mul i64 %942, %region_0_250_constant_18157
  %944 = lshr i64 %943, %838
  %shft.chk184 = icmp ult i64 %838, 64
  %945 = select i1 %shft.chk184, i64 %944, i64 0
  %946 = trunc i64 %945 to i32
  %947 = lshr i64 %938, %838
  %shft.chk185 = icmp ult i64 %838, 64
  %948 = select i1 %shft.chk185, i64 %947, i64 0
  %949 = trunc i64 %948 to i32
  %950 = trunc i64 %895 to i32
  %951 = xor i32 %949, %950
  %region_0_250_constant_140186 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %952 = xor i32 %951, %region_0_250_constant_140186
  %953 = zext i32 %952 to i64
  %954 = mul i64 %953, %region_0_250_constant_18157
  %955 = trunc i64 %954 to i32
  %956 = xor i32 %946, %955
  %region_0_250_constant_147187 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %957 = xor i32 %956, %region_0_250_constant_147187
  %958 = zext i32 %957 to i64
  %959 = mul i64 %958, %region_0_250_constant_46164
  %960 = lshr i64 %959, %838
  %shft.chk188 = icmp ult i64 %838, 64
  %961 = select i1 %shft.chk188, i64 %960, i64 0
  %962 = trunc i64 %961 to i32
  %963 = lshr i64 %954, %838
  %shft.chk189 = icmp ult i64 %838, 64
  %964 = select i1 %shft.chk189, i64 %963, i64 0
  %965 = trunc i64 %964 to i32
  %966 = trunc i64 %911 to i32
  %967 = xor i32 %965, %966
  %region_0_250_constant_158190 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %968 = xor i32 %967, %region_0_250_constant_158190
  %969 = zext i32 %968 to i64
  %970 = mul i64 %969, %region_0_250_constant_46164
  %971 = trunc i64 %970 to i32
  %972 = xor i32 %962, %971
  %region_0_250_constant_165191 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %973 = xor i32 %972, %region_0_250_constant_165191
  %974 = zext i32 %973 to i64
  %975 = mul i64 %974, %region_0_250_constant_18157
  %976 = lshr i64 %975, %838
  %shft.chk192 = icmp ult i64 %838, 64
  %977 = select i1 %shft.chk192, i64 %976, i64 0
  %978 = trunc i64 %977 to i32
  %979 = lshr i64 %970, %838
  %shft.chk193 = icmp ult i64 %838, 64
  %980 = select i1 %shft.chk193, i64 %979, i64 0
  %981 = trunc i64 %980 to i32
  %982 = trunc i64 %927 to i32
  %983 = xor i32 %981, %982
  %region_0_250_constant_176194 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %984 = xor i32 %983, %region_0_250_constant_176194
  %985 = zext i32 %984 to i64
  %986 = mul i64 %985, %region_0_250_constant_18157
  %987 = trunc i64 %986 to i32
  %988 = xor i32 %978, %987
  %region_0_250_constant_183195 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %989 = xor i32 %988, %region_0_250_constant_183195
  %990 = zext i32 %989 to i64
  %991 = mul i64 %990, %region_0_250_constant_46164
  %992 = lshr i64 %991, %838
  %shft.chk196 = icmp ult i64 %838, 64
  %993 = select i1 %shft.chk196, i64 %992, i64 0
  %994 = trunc i64 %993 to i32
  %995 = lshr i64 %986, %838
  %shft.chk197 = icmp ult i64 %838, 64
  %996 = select i1 %shft.chk197, i64 %995, i64 0
  %997 = trunc i64 %996 to i32
  %998 = trunc i64 %943 to i32
  %999 = xor i32 %997, %998
  %region_0_250_constant_194198 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %1000 = xor i32 %999, %region_0_250_constant_194198
  %1001 = zext i32 %1000 to i64
  %1002 = mul i64 %1001, %region_0_250_constant_46164
  %1003 = trunc i64 %1002 to i32
  %1004 = xor i32 %994, %1003
  %region_0_250_constant_201199 = load i32, i32* bitcast ([4 x i8]* @56 to i32*), align 4
  %1005 = xor i32 %1004, %region_0_250_constant_201199
  br label %concatenate.226.merge153

concat_index_from_operand_id1200:                 ; preds = %concatenate.pivot.1.333
  %1006 = phi i32 [ 1, %concatenate.pivot.1.333 ]
  %1007 = sub nsw i32 %813, %1006
  %1008 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1009 = load i64, i64* %1008, align 8, !invariant.load !22
  %1010 = trunc i64 %1009 to i32
  %1011 = zext i32 %1010 to i64
  %1012 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1013 = lshr i64 %1009, %1012
  %shft.chk201 = icmp ult i64 %1012, 64
  %1014 = select i1 %shft.chk201, i64 %1013, i64 0
  %1015 = trunc i64 %1014 to i32
  %1016 = zext i32 %1015 to i64
  %1017 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1018 = shl i64 %1016, %1017
  %shft.chk202 = icmp ult i64 %1017, 64
  %1019 = select i1 %shft.chk202, i64 %1018, i64 0
  %1020 = or i64 %1011, %1019
  %1021 = mul nuw nsw i32 %814, 1
  %1022 = add nuw nsw i32 0, %1021
  %1023 = zext i32 %1022 to i64
  %1024 = add i64 %1020, %1023
  %1025 = trunc i64 %1024 to i32
  %1026 = zext i32 %1025 to i64
  %region_0_250_constant_18203 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %1027 = mul i64 %1026, %region_0_250_constant_18203
  %1028 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1029 = lshr i64 %1027, %1028
  %shft.chk204 = icmp ult i64 %1028, 64
  %1030 = select i1 %shft.chk204, i64 %1029, i64 0
  %1031 = trunc i64 %1030 to i32
  %1032 = icmp ult i64 %1024, %1020
  %1033 = zext i1 %1032 to i8
  %1034 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1035 = load i64, i64* %1034, align 8, !invariant.load !22
  %1036 = trunc i64 %1035 to i32
  %1037 = zext i32 %1036 to i64
  %1038 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1039 = lshr i64 %1035, %1038
  %shft.chk205 = icmp ult i64 %1038, 64
  %1040 = select i1 %shft.chk205, i64 %1039, i64 0
  %1041 = trunc i64 %1040 to i32
  %1042 = zext i32 %1041 to i64
  %1043 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1044 = shl i64 %1042, %1043
  %shft.chk206 = icmp ult i64 %1043, 64
  %1045 = select i1 %shft.chk206, i64 %1044, i64 0
  %1046 = or i64 %1037, %1045
  %region_0_250_constant_34207 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1047 = add i64 %1046, %region_0_250_constant_34207
  %1048 = trunc i8 %1033 to i1
  %1049 = select i1 %1048, i64 %1047, i64 %1046
  %1050 = lshr i64 %1049, %1028
  %shft.chk208 = icmp ult i64 %1028, 64
  %1051 = select i1 %shft.chk208, i64 %1050, i64 0
  %1052 = trunc i64 %1051 to i32
  %1053 = xor i32 %1031, %1052
  %region_0_250_constant_42209 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %1054 = xor i32 %1053, %region_0_250_constant_42209
  %1055 = zext i32 %1054 to i64
  %region_0_250_constant_46210 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1056 = mul i64 %1055, %region_0_250_constant_46210
  %1057 = lshr i64 %1056, %1028
  %shft.chk211 = icmp ult i64 %1028, 64
  %1058 = select i1 %shft.chk211, i64 %1057, i64 0
  %1059 = trunc i64 %1058 to i32
  %1060 = trunc i64 %1049 to i32
  %1061 = zext i32 %1060 to i64
  %1062 = mul i64 %1061, %region_0_250_constant_46210
  %1063 = trunc i64 %1062 to i32
  %1064 = xor i32 %1059, %1063
  %region_0_250_constant_56212 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %1065 = xor i32 %1064, %region_0_250_constant_56212
  %1066 = zext i32 %1065 to i64
  %1067 = mul i64 %1066, %region_0_250_constant_18203
  %1068 = lshr i64 %1067, %1028
  %shft.chk213 = icmp ult i64 %1028, 64
  %1069 = select i1 %shft.chk213, i64 %1068, i64 0
  %1070 = trunc i64 %1069 to i32
  %1071 = lshr i64 %1062, %1028
  %shft.chk214 = icmp ult i64 %1028, 64
  %1072 = select i1 %shft.chk214, i64 %1071, i64 0
  %1073 = trunc i64 %1072 to i32
  %1074 = lshr i64 %1024, %1028
  %shft.chk215 = icmp ult i64 %1028, 64
  %1075 = select i1 %shft.chk215, i64 %1074, i64 0
  %1076 = trunc i64 %1075 to i32
  %1077 = xor i32 %1073, %1076
  %region_0_250_constant_68216 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1078 = xor i32 %1077, %region_0_250_constant_68216
  %1079 = zext i32 %1078 to i64
  %1080 = mul i64 %1079, %region_0_250_constant_18203
  %1081 = trunc i64 %1080 to i32
  %1082 = xor i32 %1070, %1081
  %region_0_250_constant_75217 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1083 = xor i32 %1082, %region_0_250_constant_75217
  %1084 = zext i32 %1083 to i64
  %1085 = mul i64 %1084, %region_0_250_constant_46210
  %1086 = lshr i64 %1085, %1028
  %shft.chk218 = icmp ult i64 %1028, 64
  %1087 = select i1 %shft.chk218, i64 %1086, i64 0
  %1088 = trunc i64 %1087 to i32
  %1089 = lshr i64 %1080, %1028
  %shft.chk219 = icmp ult i64 %1028, 64
  %1090 = select i1 %shft.chk219, i64 %1089, i64 0
  %1091 = trunc i64 %1090 to i32
  %1092 = trunc i64 %1027 to i32
  %1093 = xor i32 %1091, %1092
  %region_0_250_constant_86220 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %1094 = xor i32 %1093, %region_0_250_constant_86220
  %1095 = zext i32 %1094 to i64
  %1096 = mul i64 %1095, %region_0_250_constant_46210
  %1097 = trunc i64 %1096 to i32
  %1098 = xor i32 %1088, %1097
  %region_0_250_constant_93221 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %1099 = xor i32 %1098, %region_0_250_constant_93221
  %1100 = zext i32 %1099 to i64
  %1101 = mul i64 %1100, %region_0_250_constant_18203
  %1102 = lshr i64 %1101, %1028
  %shft.chk222 = icmp ult i64 %1028, 64
  %1103 = select i1 %shft.chk222, i64 %1102, i64 0
  %1104 = trunc i64 %1103 to i32
  %1105 = lshr i64 %1096, %1028
  %shft.chk223 = icmp ult i64 %1028, 64
  %1106 = select i1 %shft.chk223, i64 %1105, i64 0
  %1107 = trunc i64 %1106 to i32
  %1108 = trunc i64 %1056 to i32
  %1109 = xor i32 %1107, %1108
  %region_0_250_constant_104224 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %1110 = xor i32 %1109, %region_0_250_constant_104224
  %1111 = zext i32 %1110 to i64
  %1112 = mul i64 %1111, %region_0_250_constant_18203
  %1113 = trunc i64 %1112 to i32
  %1114 = xor i32 %1104, %1113
  %region_0_250_constant_111225 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %1115 = xor i32 %1114, %region_0_250_constant_111225
  %1116 = zext i32 %1115 to i64
  %1117 = mul i64 %1116, %region_0_250_constant_46210
  %1118 = lshr i64 %1117, %1028
  %shft.chk226 = icmp ult i64 %1028, 64
  %1119 = select i1 %shft.chk226, i64 %1118, i64 0
  %1120 = trunc i64 %1119 to i32
  %1121 = lshr i64 %1112, %1028
  %shft.chk227 = icmp ult i64 %1028, 64
  %1122 = select i1 %shft.chk227, i64 %1121, i64 0
  %1123 = trunc i64 %1122 to i32
  %1124 = trunc i64 %1067 to i32
  %1125 = xor i32 %1123, %1124
  %region_0_250_constant_122228 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %1126 = xor i32 %1125, %region_0_250_constant_122228
  %1127 = zext i32 %1126 to i64
  %1128 = mul i64 %1127, %region_0_250_constant_46210
  %1129 = trunc i64 %1128 to i32
  %1130 = xor i32 %1120, %1129
  %region_0_250_constant_129229 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %1131 = xor i32 %1130, %region_0_250_constant_129229
  %1132 = zext i32 %1131 to i64
  %1133 = mul i64 %1132, %region_0_250_constant_18203
  %1134 = lshr i64 %1133, %1028
  %shft.chk230 = icmp ult i64 %1028, 64
  %1135 = select i1 %shft.chk230, i64 %1134, i64 0
  %1136 = trunc i64 %1135 to i32
  %1137 = lshr i64 %1128, %1028
  %shft.chk231 = icmp ult i64 %1028, 64
  %1138 = select i1 %shft.chk231, i64 %1137, i64 0
  %1139 = trunc i64 %1138 to i32
  %1140 = trunc i64 %1085 to i32
  %1141 = xor i32 %1139, %1140
  %region_0_250_constant_140232 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1142 = xor i32 %1141, %region_0_250_constant_140232
  %1143 = zext i32 %1142 to i64
  %1144 = mul i64 %1143, %region_0_250_constant_18203
  %1145 = trunc i64 %1144 to i32
  %1146 = xor i32 %1136, %1145
  %region_0_250_constant_147233 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1147 = xor i32 %1146, %region_0_250_constant_147233
  %1148 = zext i32 %1147 to i64
  %1149 = mul i64 %1148, %region_0_250_constant_46210
  %1150 = lshr i64 %1149, %1028
  %shft.chk234 = icmp ult i64 %1028, 64
  %1151 = select i1 %shft.chk234, i64 %1150, i64 0
  %1152 = trunc i64 %1151 to i32
  %1153 = lshr i64 %1144, %1028
  %shft.chk235 = icmp ult i64 %1028, 64
  %1154 = select i1 %shft.chk235, i64 %1153, i64 0
  %1155 = trunc i64 %1154 to i32
  %1156 = trunc i64 %1101 to i32
  %1157 = xor i32 %1155, %1156
  %region_0_250_constant_158236 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %1158 = xor i32 %1157, %region_0_250_constant_158236
  %1159 = zext i32 %1158 to i64
  %1160 = mul i64 %1159, %region_0_250_constant_46210
  %1161 = trunc i64 %1160 to i32
  %1162 = xor i32 %1152, %1161
  %region_0_250_constant_165237 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %1163 = xor i32 %1162, %region_0_250_constant_165237
  %1164 = zext i32 %1163 to i64
  %1165 = mul i64 %1164, %region_0_250_constant_18203
  %1166 = lshr i64 %1165, %1028
  %shft.chk238 = icmp ult i64 %1028, 64
  %1167 = select i1 %shft.chk238, i64 %1166, i64 0
  %1168 = trunc i64 %1167 to i32
  %1169 = lshr i64 %1160, %1028
  %shft.chk239 = icmp ult i64 %1028, 64
  %1170 = select i1 %shft.chk239, i64 %1169, i64 0
  %1171 = trunc i64 %1170 to i32
  %1172 = trunc i64 %1117 to i32
  %1173 = xor i32 %1171, %1172
  %region_0_250_constant_176240 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %1174 = xor i32 %1173, %region_0_250_constant_176240
  %1175 = zext i32 %1174 to i64
  %1176 = mul i64 %1175, %region_0_250_constant_18203
  %1177 = trunc i64 %1176 to i32
  %1178 = xor i32 %1168, %1177
  %region_0_250_constant_183241 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %1179 = xor i32 %1178, %region_0_250_constant_183241
  %1180 = zext i32 %1179 to i64
  %1181 = mul i64 %1180, %region_0_250_constant_46210
  %1182 = trunc i64 %1181 to i32
  br label %concatenate.226.merge153

concat_index_from_operand_id2242:                 ; preds = %concatenate.pivot.2.335
  %1183 = phi i32 [ 2, %concatenate.pivot.2.335 ]
  %1184 = sub nsw i32 %813, %1183
  %1185 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1186 = load i64, i64* %1185, align 8, !invariant.load !22
  %1187 = trunc i64 %1186 to i32
  %1188 = zext i32 %1187 to i64
  %1189 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1190 = lshr i64 %1186, %1189
  %shft.chk243 = icmp ult i64 %1189, 64
  %1191 = select i1 %shft.chk243, i64 %1190, i64 0
  %1192 = trunc i64 %1191 to i32
  %1193 = zext i32 %1192 to i64
  %1194 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1195 = shl i64 %1193, %1194
  %shft.chk244 = icmp ult i64 %1194, 64
  %1196 = select i1 %shft.chk244, i64 %1195, i64 0
  %1197 = or i64 %1188, %1196
  %1198 = mul nuw nsw i32 %814, 1
  %1199 = add nuw nsw i32 0, %1198
  %1200 = zext i32 %1199 to i64
  %1201 = add i64 %1197, %1200
  %1202 = icmp ult i64 %1201, %1197
  %1203 = zext i1 %1202 to i8
  %1204 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1205 = load i64, i64* %1204, align 8, !invariant.load !22
  %1206 = trunc i64 %1205 to i32
  %1207 = zext i32 %1206 to i64
  %1208 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1209 = lshr i64 %1205, %1208
  %shft.chk245 = icmp ult i64 %1208, 64
  %1210 = select i1 %shft.chk245, i64 %1209, i64 0
  %1211 = trunc i64 %1210 to i32
  %1212 = zext i32 %1211 to i64
  %1213 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1214 = shl i64 %1212, %1213
  %shft.chk246 = icmp ult i64 %1213, 64
  %1215 = select i1 %shft.chk246, i64 %1214, i64 0
  %1216 = or i64 %1207, %1215
  %region_0_250_constant_34247 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1217 = add i64 %1216, %region_0_250_constant_34247
  %1218 = trunc i8 %1203 to i1
  %1219 = select i1 %1218, i64 %1217, i64 %1216
  %1220 = trunc i64 %1219 to i32
  %1221 = zext i32 %1220 to i64
  %region_0_250_constant_46248 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1222 = mul i64 %1221, %region_0_250_constant_46248
  %1223 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1224 = lshr i64 %1222, %1223
  %shft.chk249 = icmp ult i64 %1223, 64
  %1225 = select i1 %shft.chk249, i64 %1224, i64 0
  %1226 = trunc i64 %1225 to i32
  %1227 = lshr i64 %1201, %1223
  %shft.chk250 = icmp ult i64 %1223, 64
  %1228 = select i1 %shft.chk250, i64 %1227, i64 0
  %1229 = trunc i64 %1228 to i32
  %1230 = xor i32 %1226, %1229
  %region_0_250_constant_68251 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1231 = xor i32 %1230, %region_0_250_constant_68251
  %1232 = zext i32 %1231 to i64
  %region_0_250_constant_18252 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %1233 = mul i64 %1232, %region_0_250_constant_18252
  %1234 = lshr i64 %1233, %1223
  %shft.chk253 = icmp ult i64 %1223, 64
  %1235 = select i1 %shft.chk253, i64 %1234, i64 0
  %1236 = trunc i64 %1235 to i32
  %1237 = trunc i64 %1201 to i32
  %1238 = zext i32 %1237 to i64
  %1239 = mul i64 %1238, %region_0_250_constant_18252
  %1240 = trunc i64 %1239 to i32
  %1241 = xor i32 %1236, %1240
  %region_0_250_constant_86254 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %1242 = xor i32 %1241, %region_0_250_constant_86254
  %1243 = zext i32 %1242 to i64
  %1244 = mul i64 %1243, %region_0_250_constant_46248
  %1245 = lshr i64 %1244, %1223
  %shft.chk255 = icmp ult i64 %1223, 64
  %1246 = select i1 %shft.chk255, i64 %1245, i64 0
  %1247 = trunc i64 %1246 to i32
  %1248 = lshr i64 %1239, %1223
  %shft.chk256 = icmp ult i64 %1223, 64
  %1249 = select i1 %shft.chk256, i64 %1248, i64 0
  %1250 = trunc i64 %1249 to i32
  %1251 = lshr i64 %1219, %1223
  %shft.chk257 = icmp ult i64 %1223, 64
  %1252 = select i1 %shft.chk257, i64 %1251, i64 0
  %1253 = trunc i64 %1252 to i32
  %1254 = xor i32 %1250, %1253
  %region_0_250_constant_42258 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %1255 = xor i32 %1254, %region_0_250_constant_42258
  %1256 = zext i32 %1255 to i64
  %1257 = mul i64 %1256, %region_0_250_constant_46248
  %1258 = trunc i64 %1257 to i32
  %1259 = xor i32 %1247, %1258
  %region_0_250_constant_104259 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %1260 = xor i32 %1259, %region_0_250_constant_104259
  %1261 = zext i32 %1260 to i64
  %1262 = mul i64 %1261, %region_0_250_constant_18252
  %1263 = lshr i64 %1262, %1223
  %shft.chk260 = icmp ult i64 %1223, 64
  %1264 = select i1 %shft.chk260, i64 %1263, i64 0
  %1265 = trunc i64 %1264 to i32
  %1266 = lshr i64 %1257, %1223
  %shft.chk261 = icmp ult i64 %1223, 64
  %1267 = select i1 %shft.chk261, i64 %1266, i64 0
  %1268 = trunc i64 %1267 to i32
  %1269 = trunc i64 %1222 to i32
  %1270 = xor i32 %1268, %1269
  %region_0_250_constant_56262 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %1271 = xor i32 %1270, %region_0_250_constant_56262
  %1272 = zext i32 %1271 to i64
  %1273 = mul i64 %1272, %region_0_250_constant_18252
  %1274 = trunc i64 %1273 to i32
  %1275 = xor i32 %1265, %1274
  %region_0_250_constant_122263 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %1276 = xor i32 %1275, %region_0_250_constant_122263
  %1277 = zext i32 %1276 to i64
  %1278 = mul i64 %1277, %region_0_250_constant_46248
  %1279 = lshr i64 %1278, %1223
  %shft.chk264 = icmp ult i64 %1223, 64
  %1280 = select i1 %shft.chk264, i64 %1279, i64 0
  %1281 = trunc i64 %1280 to i32
  %1282 = lshr i64 %1273, %1223
  %shft.chk265 = icmp ult i64 %1223, 64
  %1283 = select i1 %shft.chk265, i64 %1282, i64 0
  %1284 = trunc i64 %1283 to i32
  %1285 = trunc i64 %1233 to i32
  %1286 = xor i32 %1284, %1285
  %region_0_250_constant_75266 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1287 = xor i32 %1286, %region_0_250_constant_75266
  %1288 = zext i32 %1287 to i64
  %1289 = mul i64 %1288, %region_0_250_constant_46248
  %1290 = trunc i64 %1289 to i32
  %1291 = xor i32 %1281, %1290
  %region_0_250_constant_140267 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1292 = xor i32 %1291, %region_0_250_constant_140267
  %1293 = zext i32 %1292 to i64
  %1294 = mul i64 %1293, %region_0_250_constant_18252
  %1295 = lshr i64 %1294, %1223
  %shft.chk268 = icmp ult i64 %1223, 64
  %1296 = select i1 %shft.chk268, i64 %1295, i64 0
  %1297 = trunc i64 %1296 to i32
  %1298 = lshr i64 %1289, %1223
  %shft.chk269 = icmp ult i64 %1223, 64
  %1299 = select i1 %shft.chk269, i64 %1298, i64 0
  %1300 = trunc i64 %1299 to i32
  %1301 = trunc i64 %1244 to i32
  %1302 = xor i32 %1300, %1301
  %region_0_250_constant_93270 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %1303 = xor i32 %1302, %region_0_250_constant_93270
  %1304 = zext i32 %1303 to i64
  %1305 = mul i64 %1304, %region_0_250_constant_18252
  %1306 = trunc i64 %1305 to i32
  %1307 = xor i32 %1297, %1306
  %region_0_250_constant_158271 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %1308 = xor i32 %1307, %region_0_250_constant_158271
  %1309 = zext i32 %1308 to i64
  %1310 = mul i64 %1309, %region_0_250_constant_46248
  %1311 = lshr i64 %1310, %1223
  %shft.chk272 = icmp ult i64 %1223, 64
  %1312 = select i1 %shft.chk272, i64 %1311, i64 0
  %1313 = trunc i64 %1312 to i32
  %1314 = lshr i64 %1305, %1223
  %shft.chk273 = icmp ult i64 %1223, 64
  %1315 = select i1 %shft.chk273, i64 %1314, i64 0
  %1316 = trunc i64 %1315 to i32
  %1317 = trunc i64 %1262 to i32
  %1318 = xor i32 %1316, %1317
  %region_0_250_constant_111274 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %1319 = xor i32 %1318, %region_0_250_constant_111274
  %1320 = zext i32 %1319 to i64
  %1321 = mul i64 %1320, %region_0_250_constant_46248
  %1322 = trunc i64 %1321 to i32
  %1323 = xor i32 %1313, %1322
  %region_0_250_constant_176275 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %1324 = xor i32 %1323, %region_0_250_constant_176275
  %1325 = zext i32 %1324 to i64
  %1326 = mul i64 %1325, %region_0_250_constant_18252
  %1327 = lshr i64 %1326, %1223
  %shft.chk276 = icmp ult i64 %1223, 64
  %1328 = select i1 %shft.chk276, i64 %1327, i64 0
  %1329 = trunc i64 %1328 to i32
  %1330 = lshr i64 %1321, %1223
  %shft.chk277 = icmp ult i64 %1223, 64
  %1331 = select i1 %shft.chk277, i64 %1330, i64 0
  %1332 = trunc i64 %1331 to i32
  %1333 = trunc i64 %1278 to i32
  %1334 = xor i32 %1332, %1333
  %region_0_250_constant_129278 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %1335 = xor i32 %1334, %region_0_250_constant_129278
  %1336 = zext i32 %1335 to i64
  %1337 = mul i64 %1336, %region_0_250_constant_18252
  %1338 = trunc i64 %1337 to i32
  %1339 = xor i32 %1329, %1338
  %region_0_250_constant_194279 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %1340 = xor i32 %1339, %region_0_250_constant_194279
  %1341 = zext i32 %1340 to i64
  %1342 = mul i64 %1341, %region_0_250_constant_46248
  %1343 = lshr i64 %1342, %1223
  %shft.chk280 = icmp ult i64 %1223, 64
  %1344 = select i1 %shft.chk280, i64 %1343, i64 0
  %1345 = trunc i64 %1344 to i32
  %1346 = lshr i64 %1337, %1223
  %shft.chk281 = icmp ult i64 %1223, 64
  %1347 = select i1 %shft.chk281, i64 %1346, i64 0
  %1348 = trunc i64 %1347 to i32
  %1349 = trunc i64 %1294 to i32
  %1350 = xor i32 %1348, %1349
  %region_0_250_constant_147282 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1351 = xor i32 %1350, %region_0_250_constant_147282
  %1352 = zext i32 %1351 to i64
  %1353 = mul i64 %1352, %region_0_250_constant_46248
  %1354 = trunc i64 %1353 to i32
  %1355 = xor i32 %1345, %1354
  %region_0_250_constant_211283 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1356 = xor i32 %1355, %region_0_250_constant_211283
  %1357 = zext i32 %1356 to i64
  %1358 = mul i64 %1357, %region_0_250_constant_18252
  %1359 = lshr i64 %1358, %1223
  %shft.chk284 = icmp ult i64 %1223, 64
  %1360 = select i1 %shft.chk284, i64 %1359, i64 0
  %1361 = trunc i64 %1360 to i32
  %1362 = lshr i64 %1353, %1223
  %shft.chk285 = icmp ult i64 %1223, 64
  %1363 = select i1 %shft.chk285, i64 %1362, i64 0
  %1364 = trunc i64 %1363 to i32
  %1365 = trunc i64 %1310 to i32
  %1366 = xor i32 %1364, %1365
  %region_0_250_constant_165286 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %1367 = xor i32 %1366, %region_0_250_constant_165286
  %1368 = zext i32 %1367 to i64
  %1369 = mul i64 %1368, %region_0_250_constant_18252
  %1370 = trunc i64 %1369 to i32
  %1371 = xor i32 %1361, %1370
  %region_0_250_constant_220287 = load i32, i32* bitcast ([4 x i8]* @53 to i32*), align 4
  %1372 = xor i32 %1371, %region_0_250_constant_220287
  br label %concatenate.226.merge153

concat_index_from_operand_id3288:                 ; preds = %concatenate.pivot.3.336
  %1373 = phi i32 [ 3, %concatenate.pivot.3.336 ]
  %1374 = sub nsw i32 %813, %1373
  %1375 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1376 = load i64, i64* %1375, align 8, !invariant.load !22
  %1377 = trunc i64 %1376 to i32
  %1378 = zext i32 %1377 to i64
  %1379 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1380 = lshr i64 %1376, %1379
  %shft.chk289 = icmp ult i64 %1379, 64
  %1381 = select i1 %shft.chk289, i64 %1380, i64 0
  %1382 = trunc i64 %1381 to i32
  %1383 = zext i32 %1382 to i64
  %1384 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1385 = shl i64 %1383, %1384
  %shft.chk290 = icmp ult i64 %1384, 64
  %1386 = select i1 %shft.chk290, i64 %1385, i64 0
  %1387 = or i64 %1378, %1386
  %1388 = mul nuw nsw i32 %814, 1
  %1389 = add nuw nsw i32 0, %1388
  %1390 = zext i32 %1389 to i64
  %1391 = add i64 %1387, %1390
  %1392 = icmp ult i64 %1391, %1387
  %1393 = zext i1 %1392 to i8
  %1394 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1395 = load i64, i64* %1394, align 8, !invariant.load !22
  %1396 = trunc i64 %1395 to i32
  %1397 = zext i32 %1396 to i64
  %1398 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1399 = lshr i64 %1395, %1398
  %shft.chk291 = icmp ult i64 %1398, 64
  %1400 = select i1 %shft.chk291, i64 %1399, i64 0
  %1401 = trunc i64 %1400 to i32
  %1402 = zext i32 %1401 to i64
  %1403 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1404 = shl i64 %1402, %1403
  %shft.chk292 = icmp ult i64 %1403, 64
  %1405 = select i1 %shft.chk292, i64 %1404, i64 0
  %1406 = or i64 %1397, %1405
  %region_0_250_constant_34293 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1407 = add i64 %1406, %region_0_250_constant_34293
  %1408 = trunc i8 %1393 to i1
  %1409 = select i1 %1408, i64 %1407, i64 %1406
  %1410 = trunc i64 %1409 to i32
  %1411 = zext i32 %1410 to i64
  %region_0_250_constant_46294 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1412 = mul i64 %1411, %region_0_250_constant_46294
  %1413 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1414 = lshr i64 %1412, %1413
  %shft.chk295 = icmp ult i64 %1413, 64
  %1415 = select i1 %shft.chk295, i64 %1414, i64 0
  %1416 = trunc i64 %1415 to i32
  %1417 = lshr i64 %1391, %1413
  %shft.chk296 = icmp ult i64 %1413, 64
  %1418 = select i1 %shft.chk296, i64 %1417, i64 0
  %1419 = trunc i64 %1418 to i32
  %1420 = xor i32 %1416, %1419
  %region_0_250_constant_68297 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1421 = xor i32 %1420, %region_0_250_constant_68297
  %1422 = zext i32 %1421 to i64
  %region_0_250_constant_18298 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %1423 = mul i64 %1422, %region_0_250_constant_18298
  %1424 = lshr i64 %1423, %1413
  %shft.chk299 = icmp ult i64 %1413, 64
  %1425 = select i1 %shft.chk299, i64 %1424, i64 0
  %1426 = trunc i64 %1425 to i32
  %1427 = trunc i64 %1391 to i32
  %1428 = zext i32 %1427 to i64
  %1429 = mul i64 %1428, %region_0_250_constant_18298
  %1430 = trunc i64 %1429 to i32
  %1431 = xor i32 %1426, %1430
  %region_0_250_constant_86300 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %1432 = xor i32 %1431, %region_0_250_constant_86300
  %1433 = zext i32 %1432 to i64
  %1434 = mul i64 %1433, %region_0_250_constant_46294
  %1435 = lshr i64 %1434, %1413
  %shft.chk301 = icmp ult i64 %1413, 64
  %1436 = select i1 %shft.chk301, i64 %1435, i64 0
  %1437 = trunc i64 %1436 to i32
  %1438 = lshr i64 %1429, %1413
  %shft.chk302 = icmp ult i64 %1413, 64
  %1439 = select i1 %shft.chk302, i64 %1438, i64 0
  %1440 = trunc i64 %1439 to i32
  %1441 = lshr i64 %1409, %1413
  %shft.chk303 = icmp ult i64 %1413, 64
  %1442 = select i1 %shft.chk303, i64 %1441, i64 0
  %1443 = trunc i64 %1442 to i32
  %1444 = xor i32 %1440, %1443
  %region_0_250_constant_42304 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %1445 = xor i32 %1444, %region_0_250_constant_42304
  %1446 = zext i32 %1445 to i64
  %1447 = mul i64 %1446, %region_0_250_constant_46294
  %1448 = trunc i64 %1447 to i32
  %1449 = xor i32 %1437, %1448
  %region_0_250_constant_104305 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %1450 = xor i32 %1449, %region_0_250_constant_104305
  %1451 = zext i32 %1450 to i64
  %1452 = mul i64 %1451, %region_0_250_constant_18298
  %1453 = lshr i64 %1452, %1413
  %shft.chk306 = icmp ult i64 %1413, 64
  %1454 = select i1 %shft.chk306, i64 %1453, i64 0
  %1455 = trunc i64 %1454 to i32
  %1456 = lshr i64 %1447, %1413
  %shft.chk307 = icmp ult i64 %1413, 64
  %1457 = select i1 %shft.chk307, i64 %1456, i64 0
  %1458 = trunc i64 %1457 to i32
  %1459 = trunc i64 %1412 to i32
  %1460 = xor i32 %1458, %1459
  %region_0_250_constant_56308 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %1461 = xor i32 %1460, %region_0_250_constant_56308
  %1462 = zext i32 %1461 to i64
  %1463 = mul i64 %1462, %region_0_250_constant_18298
  %1464 = trunc i64 %1463 to i32
  %1465 = xor i32 %1455, %1464
  %region_0_250_constant_122309 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %1466 = xor i32 %1465, %region_0_250_constant_122309
  %1467 = zext i32 %1466 to i64
  %1468 = mul i64 %1467, %region_0_250_constant_46294
  %1469 = lshr i64 %1468, %1413
  %shft.chk310 = icmp ult i64 %1413, 64
  %1470 = select i1 %shft.chk310, i64 %1469, i64 0
  %1471 = trunc i64 %1470 to i32
  %1472 = lshr i64 %1463, %1413
  %shft.chk311 = icmp ult i64 %1413, 64
  %1473 = select i1 %shft.chk311, i64 %1472, i64 0
  %1474 = trunc i64 %1473 to i32
  %1475 = trunc i64 %1423 to i32
  %1476 = xor i32 %1474, %1475
  %region_0_250_constant_75312 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1477 = xor i32 %1476, %region_0_250_constant_75312
  %1478 = zext i32 %1477 to i64
  %1479 = mul i64 %1478, %region_0_250_constant_46294
  %1480 = trunc i64 %1479 to i32
  %1481 = xor i32 %1471, %1480
  %region_0_250_constant_140313 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1482 = xor i32 %1481, %region_0_250_constant_140313
  %1483 = zext i32 %1482 to i64
  %1484 = mul i64 %1483, %region_0_250_constant_18298
  %1485 = lshr i64 %1484, %1413
  %shft.chk314 = icmp ult i64 %1413, 64
  %1486 = select i1 %shft.chk314, i64 %1485, i64 0
  %1487 = trunc i64 %1486 to i32
  %1488 = lshr i64 %1479, %1413
  %shft.chk315 = icmp ult i64 %1413, 64
  %1489 = select i1 %shft.chk315, i64 %1488, i64 0
  %1490 = trunc i64 %1489 to i32
  %1491 = trunc i64 %1434 to i32
  %1492 = xor i32 %1490, %1491
  %region_0_250_constant_93316 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %1493 = xor i32 %1492, %region_0_250_constant_93316
  %1494 = zext i32 %1493 to i64
  %1495 = mul i64 %1494, %region_0_250_constant_18298
  %1496 = trunc i64 %1495 to i32
  %1497 = xor i32 %1487, %1496
  %region_0_250_constant_158317 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %1498 = xor i32 %1497, %region_0_250_constant_158317
  %1499 = zext i32 %1498 to i64
  %1500 = mul i64 %1499, %region_0_250_constant_46294
  %1501 = lshr i64 %1500, %1413
  %shft.chk318 = icmp ult i64 %1413, 64
  %1502 = select i1 %shft.chk318, i64 %1501, i64 0
  %1503 = trunc i64 %1502 to i32
  %1504 = lshr i64 %1495, %1413
  %shft.chk319 = icmp ult i64 %1413, 64
  %1505 = select i1 %shft.chk319, i64 %1504, i64 0
  %1506 = trunc i64 %1505 to i32
  %1507 = trunc i64 %1452 to i32
  %1508 = xor i32 %1506, %1507
  %region_0_250_constant_111320 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %1509 = xor i32 %1508, %region_0_250_constant_111320
  %1510 = zext i32 %1509 to i64
  %1511 = mul i64 %1510, %region_0_250_constant_46294
  %1512 = trunc i64 %1511 to i32
  %1513 = xor i32 %1503, %1512
  %region_0_250_constant_176321 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %1514 = xor i32 %1513, %region_0_250_constant_176321
  %1515 = zext i32 %1514 to i64
  %1516 = mul i64 %1515, %region_0_250_constant_18298
  %1517 = lshr i64 %1516, %1413
  %shft.chk322 = icmp ult i64 %1413, 64
  %1518 = select i1 %shft.chk322, i64 %1517, i64 0
  %1519 = trunc i64 %1518 to i32
  %1520 = lshr i64 %1511, %1413
  %shft.chk323 = icmp ult i64 %1413, 64
  %1521 = select i1 %shft.chk323, i64 %1520, i64 0
  %1522 = trunc i64 %1521 to i32
  %1523 = trunc i64 %1468 to i32
  %1524 = xor i32 %1522, %1523
  %region_0_250_constant_129324 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %1525 = xor i32 %1524, %region_0_250_constant_129324
  %1526 = zext i32 %1525 to i64
  %1527 = mul i64 %1526, %region_0_250_constant_18298
  %1528 = trunc i64 %1527 to i32
  %1529 = xor i32 %1519, %1528
  %region_0_250_constant_194325 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %1530 = xor i32 %1529, %region_0_250_constant_194325
  %1531 = zext i32 %1530 to i64
  %1532 = mul i64 %1531, %region_0_250_constant_46294
  %1533 = lshr i64 %1532, %1413
  %shft.chk326 = icmp ult i64 %1413, 64
  %1534 = select i1 %shft.chk326, i64 %1533, i64 0
  %1535 = trunc i64 %1534 to i32
  %1536 = lshr i64 %1527, %1413
  %shft.chk327 = icmp ult i64 %1413, 64
  %1537 = select i1 %shft.chk327, i64 %1536, i64 0
  %1538 = trunc i64 %1537 to i32
  %1539 = trunc i64 %1484 to i32
  %1540 = xor i32 %1538, %1539
  %region_0_250_constant_147328 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1541 = xor i32 %1540, %region_0_250_constant_147328
  %1542 = zext i32 %1541 to i64
  %1543 = mul i64 %1542, %region_0_250_constant_46294
  %1544 = trunc i64 %1543 to i32
  %1545 = xor i32 %1535, %1544
  %region_0_250_constant_211329 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1546 = xor i32 %1545, %region_0_250_constant_211329
  %1547 = zext i32 %1546 to i64
  %1548 = mul i64 %1547, %region_0_250_constant_18298
  %1549 = trunc i64 %1548 to i32
  br label %concatenate.226.merge153

concatenate.pivot.2.330:                          ; preds = %concatenate.226.merge
  %1550 = icmp ult i32 %813, 2
  br i1 %1550, label %concatenate.pivot.1.331, label %concatenate.pivot.3.334

concatenate.pivot.1.331:                          ; preds = %concatenate.pivot.2.330
  %1551 = icmp ult i32 %813, 1
  br i1 %1551, label %concatenate.pivot.0.332, label %concatenate.pivot.1.333

concatenate.pivot.0.332:                          ; preds = %concatenate.pivot.1.331
  br label %concat_index_from_operand_id0154

concatenate.pivot.1.333:                          ; preds = %concatenate.pivot.1.331
  br label %concat_index_from_operand_id1200

concatenate.pivot.3.334:                          ; preds = %concatenate.pivot.2.330
  %1552 = icmp ult i32 %813, 3
  br i1 %1552, label %concatenate.pivot.2.335, label %concatenate.pivot.3.336

concatenate.pivot.2.335:                          ; preds = %concatenate.pivot.3.334
  br label %concat_index_from_operand_id2242

concatenate.pivot.3.336:                          ; preds = %concatenate.pivot.3.334
  br label %concat_index_from_operand_id3288

concatenate.226.merge153:                         ; preds = %concat_index_from_operand_id3288, %concat_index_from_operand_id2242, %concat_index_from_operand_id1200, %concat_index_from_operand_id0154
  %1553 = phi i32 [ %1005, %concat_index_from_operand_id0154 ], [ %1182, %concat_index_from_operand_id1200 ], [ %1372, %concat_index_from_operand_id2242 ], [ %1549, %concat_index_from_operand_id3288 ]
  %region_0_250_constant_227337 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1554 = lshr i32 %1553, %region_0_250_constant_227337
  %shft.chk338 = icmp ult i32 %region_0_250_constant_227337, 32
  %1555 = select i1 %shft.chk338, i32 %1554, i32 0
  %1556 = uitofp i32 %1555 to float
  %region_0_250_constant_231339 = load float, float* bitcast ([4 x i8]* @32 to float*), align 4
  %multiply.233340 = fmul float %1556, %region_0_250_constant_231339
  %region_0_250_constant_234341 = load float, float* bitcast ([4 x i8]* @31 to float*), align 4
  %compare.236342 = fcmp olt float %multiply.233340, %region_0_250_constant_234341
  %1557 = zext i1 %compare.236342 to i8
  %region_0_250_constant_238343 = load i8, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @30, i32 0, i32 0), align 1
  %1558 = icmp eq i8 %1557, %region_0_250_constant_238343
  %1559 = zext i1 %1558 to i8
  %1560 = mul nuw nsw i32 %805, 1
  %1561 = add nuw nsw i32 0, %1560
  %1562 = udiv i32 %1561, 16
  %1563 = mul nuw nsw i32 %801, 1
  %1564 = add nuw nsw i32 0, %1563
  %1565 = mul nuw nsw i32 %802, 32
  %1566 = add nuw nsw i32 %1564, %1565
  %1567 = udiv i32 %1566, 256
  %1568 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %1, i32 0, i32 %1566, i32 %1561
  %1569 = load float, float* %1568, align 4, !invariant.load !22
  %region_0_250_constant_242344 = load float, float* bitcast ([4 x i8]* @29 to float*), align 4
  %1570 = trunc i8 %1559 to i1
  %1571 = select i1 %1570, float %1569, float %region_0_250_constant_242344
  %region_0_250_constant_245345 = load float, float* bitcast ([4 x i8]* @28 to float*), align 4
  %multiply.247346 = fmul float %1571, %region_0_250_constant_245345
  %1572 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %5, i32 0, i32 %13, i32 %15
  store float %multiply.247346, float* %1572, align 4
  %1573 = mul nuw nsw i32 %17, 1
  %1574 = add nuw nsw i32 0, %1573
  %1575 = urem i32 %1574, 32
  %1576 = udiv i32 %1574, 32
  %1577 = udiv i32 %1576, 8
  %1578 = mul nuw nsw i32 %19, 1
  %1579 = add nuw nsw i32 0, %1578
  %1580 = udiv i32 %1579, 16
  %1581 = mul nuw nsw i32 %1579, 1
  %1582 = add nuw nsw i32 0, %1581
  %1583 = mul nuw nsw i32 %1575, 16
  %1584 = add nuw nsw i32 %1582, %1583
  %1585 = mul nuw nsw i32 %1576, 512
  %1586 = add nuw nsw i32 %1584, %1585
  %1587 = urem i32 %1586, 4
  %1588 = udiv i32 %1586, 4
  %1589 = udiv i32 %1588, 1024
  br label %concatenate.pivot.2.524

concat_index_from_operand_id0348:                 ; preds = %concatenate.pivot.0.526
  %1590 = phi i32 [ 0, %concatenate.pivot.0.526 ]
  %1591 = sub nsw i32 %1587, %1590
  %1592 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1593 = load i64, i64* %1592, align 8, !invariant.load !22
  %1594 = trunc i64 %1593 to i32
  %1595 = zext i32 %1594 to i64
  %1596 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1597 = lshr i64 %1593, %1596
  %shft.chk349 = icmp ult i64 %1596, 64
  %1598 = select i1 %shft.chk349, i64 %1597, i64 0
  %1599 = trunc i64 %1598 to i32
  %1600 = zext i32 %1599 to i64
  %1601 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1602 = shl i64 %1600, %1601
  %shft.chk350 = icmp ult i64 %1601, 64
  %1603 = select i1 %shft.chk350, i64 %1602, i64 0
  %1604 = or i64 %1595, %1603
  %1605 = mul nuw nsw i32 %1588, 1
  %1606 = add nuw nsw i32 0, %1605
  %1607 = zext i32 %1606 to i64
  %1608 = add i64 %1604, %1607
  %1609 = trunc i64 %1608 to i32
  %1610 = zext i32 %1609 to i64
  %region_0_250_constant_18351 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %1611 = mul i64 %1610, %region_0_250_constant_18351
  %1612 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1613 = lshr i64 %1611, %1612
  %shft.chk352 = icmp ult i64 %1612, 64
  %1614 = select i1 %shft.chk352, i64 %1613, i64 0
  %1615 = trunc i64 %1614 to i32
  %1616 = icmp ult i64 %1608, %1604
  %1617 = zext i1 %1616 to i8
  %1618 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1619 = load i64, i64* %1618, align 8, !invariant.load !22
  %1620 = trunc i64 %1619 to i32
  %1621 = zext i32 %1620 to i64
  %1622 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1623 = lshr i64 %1619, %1622
  %shft.chk353 = icmp ult i64 %1622, 64
  %1624 = select i1 %shft.chk353, i64 %1623, i64 0
  %1625 = trunc i64 %1624 to i32
  %1626 = zext i32 %1625 to i64
  %1627 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1628 = shl i64 %1626, %1627
  %shft.chk354 = icmp ult i64 %1627, 64
  %1629 = select i1 %shft.chk354, i64 %1628, i64 0
  %1630 = or i64 %1621, %1629
  %region_0_250_constant_34355 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1631 = add i64 %1630, %region_0_250_constant_34355
  %1632 = trunc i8 %1617 to i1
  %1633 = select i1 %1632, i64 %1631, i64 %1630
  %1634 = lshr i64 %1633, %1612
  %shft.chk356 = icmp ult i64 %1612, 64
  %1635 = select i1 %shft.chk356, i64 %1634, i64 0
  %1636 = trunc i64 %1635 to i32
  %1637 = xor i32 %1615, %1636
  %region_0_250_constant_42357 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %1638 = xor i32 %1637, %region_0_250_constant_42357
  %1639 = zext i32 %1638 to i64
  %region_0_250_constant_46358 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1640 = mul i64 %1639, %region_0_250_constant_46358
  %1641 = lshr i64 %1640, %1612
  %shft.chk359 = icmp ult i64 %1612, 64
  %1642 = select i1 %shft.chk359, i64 %1641, i64 0
  %1643 = trunc i64 %1642 to i32
  %1644 = trunc i64 %1633 to i32
  %1645 = zext i32 %1644 to i64
  %1646 = mul i64 %1645, %region_0_250_constant_46358
  %1647 = trunc i64 %1646 to i32
  %1648 = xor i32 %1643, %1647
  %region_0_250_constant_56360 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %1649 = xor i32 %1648, %region_0_250_constant_56360
  %1650 = zext i32 %1649 to i64
  %1651 = mul i64 %1650, %region_0_250_constant_18351
  %1652 = lshr i64 %1651, %1612
  %shft.chk361 = icmp ult i64 %1612, 64
  %1653 = select i1 %shft.chk361, i64 %1652, i64 0
  %1654 = trunc i64 %1653 to i32
  %1655 = lshr i64 %1646, %1612
  %shft.chk362 = icmp ult i64 %1612, 64
  %1656 = select i1 %shft.chk362, i64 %1655, i64 0
  %1657 = trunc i64 %1656 to i32
  %1658 = lshr i64 %1608, %1612
  %shft.chk363 = icmp ult i64 %1612, 64
  %1659 = select i1 %shft.chk363, i64 %1658, i64 0
  %1660 = trunc i64 %1659 to i32
  %1661 = xor i32 %1657, %1660
  %region_0_250_constant_68364 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1662 = xor i32 %1661, %region_0_250_constant_68364
  %1663 = zext i32 %1662 to i64
  %1664 = mul i64 %1663, %region_0_250_constant_18351
  %1665 = trunc i64 %1664 to i32
  %1666 = xor i32 %1654, %1665
  %region_0_250_constant_75365 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1667 = xor i32 %1666, %region_0_250_constant_75365
  %1668 = zext i32 %1667 to i64
  %1669 = mul i64 %1668, %region_0_250_constant_46358
  %1670 = lshr i64 %1669, %1612
  %shft.chk366 = icmp ult i64 %1612, 64
  %1671 = select i1 %shft.chk366, i64 %1670, i64 0
  %1672 = trunc i64 %1671 to i32
  %1673 = lshr i64 %1664, %1612
  %shft.chk367 = icmp ult i64 %1612, 64
  %1674 = select i1 %shft.chk367, i64 %1673, i64 0
  %1675 = trunc i64 %1674 to i32
  %1676 = trunc i64 %1611 to i32
  %1677 = xor i32 %1675, %1676
  %region_0_250_constant_86368 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %1678 = xor i32 %1677, %region_0_250_constant_86368
  %1679 = zext i32 %1678 to i64
  %1680 = mul i64 %1679, %region_0_250_constant_46358
  %1681 = trunc i64 %1680 to i32
  %1682 = xor i32 %1672, %1681
  %region_0_250_constant_93369 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %1683 = xor i32 %1682, %region_0_250_constant_93369
  %1684 = zext i32 %1683 to i64
  %1685 = mul i64 %1684, %region_0_250_constant_18351
  %1686 = lshr i64 %1685, %1612
  %shft.chk370 = icmp ult i64 %1612, 64
  %1687 = select i1 %shft.chk370, i64 %1686, i64 0
  %1688 = trunc i64 %1687 to i32
  %1689 = lshr i64 %1680, %1612
  %shft.chk371 = icmp ult i64 %1612, 64
  %1690 = select i1 %shft.chk371, i64 %1689, i64 0
  %1691 = trunc i64 %1690 to i32
  %1692 = trunc i64 %1640 to i32
  %1693 = xor i32 %1691, %1692
  %region_0_250_constant_104372 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %1694 = xor i32 %1693, %region_0_250_constant_104372
  %1695 = zext i32 %1694 to i64
  %1696 = mul i64 %1695, %region_0_250_constant_18351
  %1697 = trunc i64 %1696 to i32
  %1698 = xor i32 %1688, %1697
  %region_0_250_constant_111373 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %1699 = xor i32 %1698, %region_0_250_constant_111373
  %1700 = zext i32 %1699 to i64
  %1701 = mul i64 %1700, %region_0_250_constant_46358
  %1702 = lshr i64 %1701, %1612
  %shft.chk374 = icmp ult i64 %1612, 64
  %1703 = select i1 %shft.chk374, i64 %1702, i64 0
  %1704 = trunc i64 %1703 to i32
  %1705 = lshr i64 %1696, %1612
  %shft.chk375 = icmp ult i64 %1612, 64
  %1706 = select i1 %shft.chk375, i64 %1705, i64 0
  %1707 = trunc i64 %1706 to i32
  %1708 = trunc i64 %1651 to i32
  %1709 = xor i32 %1707, %1708
  %region_0_250_constant_122376 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %1710 = xor i32 %1709, %region_0_250_constant_122376
  %1711 = zext i32 %1710 to i64
  %1712 = mul i64 %1711, %region_0_250_constant_46358
  %1713 = trunc i64 %1712 to i32
  %1714 = xor i32 %1704, %1713
  %region_0_250_constant_129377 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %1715 = xor i32 %1714, %region_0_250_constant_129377
  %1716 = zext i32 %1715 to i64
  %1717 = mul i64 %1716, %region_0_250_constant_18351
  %1718 = lshr i64 %1717, %1612
  %shft.chk378 = icmp ult i64 %1612, 64
  %1719 = select i1 %shft.chk378, i64 %1718, i64 0
  %1720 = trunc i64 %1719 to i32
  %1721 = lshr i64 %1712, %1612
  %shft.chk379 = icmp ult i64 %1612, 64
  %1722 = select i1 %shft.chk379, i64 %1721, i64 0
  %1723 = trunc i64 %1722 to i32
  %1724 = trunc i64 %1669 to i32
  %1725 = xor i32 %1723, %1724
  %region_0_250_constant_140380 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1726 = xor i32 %1725, %region_0_250_constant_140380
  %1727 = zext i32 %1726 to i64
  %1728 = mul i64 %1727, %region_0_250_constant_18351
  %1729 = trunc i64 %1728 to i32
  %1730 = xor i32 %1720, %1729
  %region_0_250_constant_147381 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1731 = xor i32 %1730, %region_0_250_constant_147381
  %1732 = zext i32 %1731 to i64
  %1733 = mul i64 %1732, %region_0_250_constant_46358
  %1734 = lshr i64 %1733, %1612
  %shft.chk382 = icmp ult i64 %1612, 64
  %1735 = select i1 %shft.chk382, i64 %1734, i64 0
  %1736 = trunc i64 %1735 to i32
  %1737 = lshr i64 %1728, %1612
  %shft.chk383 = icmp ult i64 %1612, 64
  %1738 = select i1 %shft.chk383, i64 %1737, i64 0
  %1739 = trunc i64 %1738 to i32
  %1740 = trunc i64 %1685 to i32
  %1741 = xor i32 %1739, %1740
  %region_0_250_constant_158384 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %1742 = xor i32 %1741, %region_0_250_constant_158384
  %1743 = zext i32 %1742 to i64
  %1744 = mul i64 %1743, %region_0_250_constant_46358
  %1745 = trunc i64 %1744 to i32
  %1746 = xor i32 %1736, %1745
  %region_0_250_constant_165385 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %1747 = xor i32 %1746, %region_0_250_constant_165385
  %1748 = zext i32 %1747 to i64
  %1749 = mul i64 %1748, %region_0_250_constant_18351
  %1750 = lshr i64 %1749, %1612
  %shft.chk386 = icmp ult i64 %1612, 64
  %1751 = select i1 %shft.chk386, i64 %1750, i64 0
  %1752 = trunc i64 %1751 to i32
  %1753 = lshr i64 %1744, %1612
  %shft.chk387 = icmp ult i64 %1612, 64
  %1754 = select i1 %shft.chk387, i64 %1753, i64 0
  %1755 = trunc i64 %1754 to i32
  %1756 = trunc i64 %1701 to i32
  %1757 = xor i32 %1755, %1756
  %region_0_250_constant_176388 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %1758 = xor i32 %1757, %region_0_250_constant_176388
  %1759 = zext i32 %1758 to i64
  %1760 = mul i64 %1759, %region_0_250_constant_18351
  %1761 = trunc i64 %1760 to i32
  %1762 = xor i32 %1752, %1761
  %region_0_250_constant_183389 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %1763 = xor i32 %1762, %region_0_250_constant_183389
  %1764 = zext i32 %1763 to i64
  %1765 = mul i64 %1764, %region_0_250_constant_46358
  %1766 = lshr i64 %1765, %1612
  %shft.chk390 = icmp ult i64 %1612, 64
  %1767 = select i1 %shft.chk390, i64 %1766, i64 0
  %1768 = trunc i64 %1767 to i32
  %1769 = lshr i64 %1760, %1612
  %shft.chk391 = icmp ult i64 %1612, 64
  %1770 = select i1 %shft.chk391, i64 %1769, i64 0
  %1771 = trunc i64 %1770 to i32
  %1772 = trunc i64 %1717 to i32
  %1773 = xor i32 %1771, %1772
  %region_0_250_constant_194392 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %1774 = xor i32 %1773, %region_0_250_constant_194392
  %1775 = zext i32 %1774 to i64
  %1776 = mul i64 %1775, %region_0_250_constant_46358
  %1777 = trunc i64 %1776 to i32
  %1778 = xor i32 %1768, %1777
  %region_0_250_constant_201393 = load i32, i32* bitcast ([4 x i8]* @56 to i32*), align 4
  %1779 = xor i32 %1778, %region_0_250_constant_201393
  br label %concatenate.226.merge347

concat_index_from_operand_id1394:                 ; preds = %concatenate.pivot.1.527
  %1780 = phi i32 [ 1, %concatenate.pivot.1.527 ]
  %1781 = sub nsw i32 %1587, %1780
  %1782 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1783 = load i64, i64* %1782, align 8, !invariant.load !22
  %1784 = trunc i64 %1783 to i32
  %1785 = zext i32 %1784 to i64
  %1786 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1787 = lshr i64 %1783, %1786
  %shft.chk395 = icmp ult i64 %1786, 64
  %1788 = select i1 %shft.chk395, i64 %1787, i64 0
  %1789 = trunc i64 %1788 to i32
  %1790 = zext i32 %1789 to i64
  %1791 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1792 = shl i64 %1790, %1791
  %shft.chk396 = icmp ult i64 %1791, 64
  %1793 = select i1 %shft.chk396, i64 %1792, i64 0
  %1794 = or i64 %1785, %1793
  %1795 = mul nuw nsw i32 %1588, 1
  %1796 = add nuw nsw i32 0, %1795
  %1797 = zext i32 %1796 to i64
  %1798 = add i64 %1794, %1797
  %1799 = trunc i64 %1798 to i32
  %1800 = zext i32 %1799 to i64
  %region_0_250_constant_18397 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %1801 = mul i64 %1800, %region_0_250_constant_18397
  %1802 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1803 = lshr i64 %1801, %1802
  %shft.chk398 = icmp ult i64 %1802, 64
  %1804 = select i1 %shft.chk398, i64 %1803, i64 0
  %1805 = trunc i64 %1804 to i32
  %1806 = icmp ult i64 %1798, %1794
  %1807 = zext i1 %1806 to i8
  %1808 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1809 = load i64, i64* %1808, align 8, !invariant.load !22
  %1810 = trunc i64 %1809 to i32
  %1811 = zext i32 %1810 to i64
  %1812 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1813 = lshr i64 %1809, %1812
  %shft.chk399 = icmp ult i64 %1812, 64
  %1814 = select i1 %shft.chk399, i64 %1813, i64 0
  %1815 = trunc i64 %1814 to i32
  %1816 = zext i32 %1815 to i64
  %1817 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1818 = shl i64 %1816, %1817
  %shft.chk400 = icmp ult i64 %1817, 64
  %1819 = select i1 %shft.chk400, i64 %1818, i64 0
  %1820 = or i64 %1811, %1819
  %region_0_250_constant_34401 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1821 = add i64 %1820, %region_0_250_constant_34401
  %1822 = trunc i8 %1807 to i1
  %1823 = select i1 %1822, i64 %1821, i64 %1820
  %1824 = lshr i64 %1823, %1802
  %shft.chk402 = icmp ult i64 %1802, 64
  %1825 = select i1 %shft.chk402, i64 %1824, i64 0
  %1826 = trunc i64 %1825 to i32
  %1827 = xor i32 %1805, %1826
  %region_0_250_constant_42403 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %1828 = xor i32 %1827, %region_0_250_constant_42403
  %1829 = zext i32 %1828 to i64
  %region_0_250_constant_46404 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1830 = mul i64 %1829, %region_0_250_constant_46404
  %1831 = lshr i64 %1830, %1802
  %shft.chk405 = icmp ult i64 %1802, 64
  %1832 = select i1 %shft.chk405, i64 %1831, i64 0
  %1833 = trunc i64 %1832 to i32
  %1834 = trunc i64 %1823 to i32
  %1835 = zext i32 %1834 to i64
  %1836 = mul i64 %1835, %region_0_250_constant_46404
  %1837 = trunc i64 %1836 to i32
  %1838 = xor i32 %1833, %1837
  %region_0_250_constant_56406 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %1839 = xor i32 %1838, %region_0_250_constant_56406
  %1840 = zext i32 %1839 to i64
  %1841 = mul i64 %1840, %region_0_250_constant_18397
  %1842 = lshr i64 %1841, %1802
  %shft.chk407 = icmp ult i64 %1802, 64
  %1843 = select i1 %shft.chk407, i64 %1842, i64 0
  %1844 = trunc i64 %1843 to i32
  %1845 = lshr i64 %1836, %1802
  %shft.chk408 = icmp ult i64 %1802, 64
  %1846 = select i1 %shft.chk408, i64 %1845, i64 0
  %1847 = trunc i64 %1846 to i32
  %1848 = lshr i64 %1798, %1802
  %shft.chk409 = icmp ult i64 %1802, 64
  %1849 = select i1 %shft.chk409, i64 %1848, i64 0
  %1850 = trunc i64 %1849 to i32
  %1851 = xor i32 %1847, %1850
  %region_0_250_constant_68410 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1852 = xor i32 %1851, %region_0_250_constant_68410
  %1853 = zext i32 %1852 to i64
  %1854 = mul i64 %1853, %region_0_250_constant_18397
  %1855 = trunc i64 %1854 to i32
  %1856 = xor i32 %1844, %1855
  %region_0_250_constant_75411 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1857 = xor i32 %1856, %region_0_250_constant_75411
  %1858 = zext i32 %1857 to i64
  %1859 = mul i64 %1858, %region_0_250_constant_46404
  %1860 = lshr i64 %1859, %1802
  %shft.chk412 = icmp ult i64 %1802, 64
  %1861 = select i1 %shft.chk412, i64 %1860, i64 0
  %1862 = trunc i64 %1861 to i32
  %1863 = lshr i64 %1854, %1802
  %shft.chk413 = icmp ult i64 %1802, 64
  %1864 = select i1 %shft.chk413, i64 %1863, i64 0
  %1865 = trunc i64 %1864 to i32
  %1866 = trunc i64 %1801 to i32
  %1867 = xor i32 %1865, %1866
  %region_0_250_constant_86414 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %1868 = xor i32 %1867, %region_0_250_constant_86414
  %1869 = zext i32 %1868 to i64
  %1870 = mul i64 %1869, %region_0_250_constant_46404
  %1871 = trunc i64 %1870 to i32
  %1872 = xor i32 %1862, %1871
  %region_0_250_constant_93415 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %1873 = xor i32 %1872, %region_0_250_constant_93415
  %1874 = zext i32 %1873 to i64
  %1875 = mul i64 %1874, %region_0_250_constant_18397
  %1876 = lshr i64 %1875, %1802
  %shft.chk416 = icmp ult i64 %1802, 64
  %1877 = select i1 %shft.chk416, i64 %1876, i64 0
  %1878 = trunc i64 %1877 to i32
  %1879 = lshr i64 %1870, %1802
  %shft.chk417 = icmp ult i64 %1802, 64
  %1880 = select i1 %shft.chk417, i64 %1879, i64 0
  %1881 = trunc i64 %1880 to i32
  %1882 = trunc i64 %1830 to i32
  %1883 = xor i32 %1881, %1882
  %region_0_250_constant_104418 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %1884 = xor i32 %1883, %region_0_250_constant_104418
  %1885 = zext i32 %1884 to i64
  %1886 = mul i64 %1885, %region_0_250_constant_18397
  %1887 = trunc i64 %1886 to i32
  %1888 = xor i32 %1878, %1887
  %region_0_250_constant_111419 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %1889 = xor i32 %1888, %region_0_250_constant_111419
  %1890 = zext i32 %1889 to i64
  %1891 = mul i64 %1890, %region_0_250_constant_46404
  %1892 = lshr i64 %1891, %1802
  %shft.chk420 = icmp ult i64 %1802, 64
  %1893 = select i1 %shft.chk420, i64 %1892, i64 0
  %1894 = trunc i64 %1893 to i32
  %1895 = lshr i64 %1886, %1802
  %shft.chk421 = icmp ult i64 %1802, 64
  %1896 = select i1 %shft.chk421, i64 %1895, i64 0
  %1897 = trunc i64 %1896 to i32
  %1898 = trunc i64 %1841 to i32
  %1899 = xor i32 %1897, %1898
  %region_0_250_constant_122422 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %1900 = xor i32 %1899, %region_0_250_constant_122422
  %1901 = zext i32 %1900 to i64
  %1902 = mul i64 %1901, %region_0_250_constant_46404
  %1903 = trunc i64 %1902 to i32
  %1904 = xor i32 %1894, %1903
  %region_0_250_constant_129423 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %1905 = xor i32 %1904, %region_0_250_constant_129423
  %1906 = zext i32 %1905 to i64
  %1907 = mul i64 %1906, %region_0_250_constant_18397
  %1908 = lshr i64 %1907, %1802
  %shft.chk424 = icmp ult i64 %1802, 64
  %1909 = select i1 %shft.chk424, i64 %1908, i64 0
  %1910 = trunc i64 %1909 to i32
  %1911 = lshr i64 %1902, %1802
  %shft.chk425 = icmp ult i64 %1802, 64
  %1912 = select i1 %shft.chk425, i64 %1911, i64 0
  %1913 = trunc i64 %1912 to i32
  %1914 = trunc i64 %1859 to i32
  %1915 = xor i32 %1913, %1914
  %region_0_250_constant_140426 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1916 = xor i32 %1915, %region_0_250_constant_140426
  %1917 = zext i32 %1916 to i64
  %1918 = mul i64 %1917, %region_0_250_constant_18397
  %1919 = trunc i64 %1918 to i32
  %1920 = xor i32 %1910, %1919
  %region_0_250_constant_147427 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1921 = xor i32 %1920, %region_0_250_constant_147427
  %1922 = zext i32 %1921 to i64
  %1923 = mul i64 %1922, %region_0_250_constant_46404
  %1924 = lshr i64 %1923, %1802
  %shft.chk428 = icmp ult i64 %1802, 64
  %1925 = select i1 %shft.chk428, i64 %1924, i64 0
  %1926 = trunc i64 %1925 to i32
  %1927 = lshr i64 %1918, %1802
  %shft.chk429 = icmp ult i64 %1802, 64
  %1928 = select i1 %shft.chk429, i64 %1927, i64 0
  %1929 = trunc i64 %1928 to i32
  %1930 = trunc i64 %1875 to i32
  %1931 = xor i32 %1929, %1930
  %region_0_250_constant_158430 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %1932 = xor i32 %1931, %region_0_250_constant_158430
  %1933 = zext i32 %1932 to i64
  %1934 = mul i64 %1933, %region_0_250_constant_46404
  %1935 = trunc i64 %1934 to i32
  %1936 = xor i32 %1926, %1935
  %region_0_250_constant_165431 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %1937 = xor i32 %1936, %region_0_250_constant_165431
  %1938 = zext i32 %1937 to i64
  %1939 = mul i64 %1938, %region_0_250_constant_18397
  %1940 = lshr i64 %1939, %1802
  %shft.chk432 = icmp ult i64 %1802, 64
  %1941 = select i1 %shft.chk432, i64 %1940, i64 0
  %1942 = trunc i64 %1941 to i32
  %1943 = lshr i64 %1934, %1802
  %shft.chk433 = icmp ult i64 %1802, 64
  %1944 = select i1 %shft.chk433, i64 %1943, i64 0
  %1945 = trunc i64 %1944 to i32
  %1946 = trunc i64 %1891 to i32
  %1947 = xor i32 %1945, %1946
  %region_0_250_constant_176434 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %1948 = xor i32 %1947, %region_0_250_constant_176434
  %1949 = zext i32 %1948 to i64
  %1950 = mul i64 %1949, %region_0_250_constant_18397
  %1951 = trunc i64 %1950 to i32
  %1952 = xor i32 %1942, %1951
  %region_0_250_constant_183435 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %1953 = xor i32 %1952, %region_0_250_constant_183435
  %1954 = zext i32 %1953 to i64
  %1955 = mul i64 %1954, %region_0_250_constant_46404
  %1956 = trunc i64 %1955 to i32
  br label %concatenate.226.merge347

concat_index_from_operand_id2436:                 ; preds = %concatenate.pivot.2.529
  %1957 = phi i32 [ 2, %concatenate.pivot.2.529 ]
  %1958 = sub nsw i32 %1587, %1957
  %1959 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %1960 = load i64, i64* %1959, align 8, !invariant.load !22
  %1961 = trunc i64 %1960 to i32
  %1962 = zext i32 %1961 to i64
  %1963 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1964 = lshr i64 %1960, %1963
  %shft.chk437 = icmp ult i64 %1963, 64
  %1965 = select i1 %shft.chk437, i64 %1964, i64 0
  %1966 = trunc i64 %1965 to i32
  %1967 = zext i32 %1966 to i64
  %1968 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1969 = shl i64 %1967, %1968
  %shft.chk438 = icmp ult i64 %1968, 64
  %1970 = select i1 %shft.chk438, i64 %1969, i64 0
  %1971 = or i64 %1962, %1970
  %1972 = mul nuw nsw i32 %1588, 1
  %1973 = add nuw nsw i32 0, %1972
  %1974 = zext i32 %1973 to i64
  %1975 = add i64 %1971, %1974
  %1976 = icmp ult i64 %1975, %1971
  %1977 = zext i1 %1976 to i8
  %1978 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %1979 = load i64, i64* %1978, align 8, !invariant.load !22
  %1980 = trunc i64 %1979 to i32
  %1981 = zext i32 %1980 to i64
  %1982 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1983 = lshr i64 %1979, %1982
  %shft.chk439 = icmp ult i64 %1982, 64
  %1984 = select i1 %shft.chk439, i64 %1983, i64 0
  %1985 = trunc i64 %1984 to i32
  %1986 = zext i32 %1985 to i64
  %1987 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1988 = shl i64 %1986, %1987
  %shft.chk440 = icmp ult i64 %1987, 64
  %1989 = select i1 %shft.chk440, i64 %1988, i64 0
  %1990 = or i64 %1981, %1989
  %region_0_250_constant_34441 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %1991 = add i64 %1990, %region_0_250_constant_34441
  %1992 = trunc i8 %1977 to i1
  %1993 = select i1 %1992, i64 %1991, i64 %1990
  %1994 = trunc i64 %1993 to i32
  %1995 = zext i32 %1994 to i64
  %region_0_250_constant_46442 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %1996 = mul i64 %1995, %region_0_250_constant_46442
  %1997 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %1998 = lshr i64 %1996, %1997
  %shft.chk443 = icmp ult i64 %1997, 64
  %1999 = select i1 %shft.chk443, i64 %1998, i64 0
  %2000 = trunc i64 %1999 to i32
  %2001 = lshr i64 %1975, %1997
  %shft.chk444 = icmp ult i64 %1997, 64
  %2002 = select i1 %shft.chk444, i64 %2001, i64 0
  %2003 = trunc i64 %2002 to i32
  %2004 = xor i32 %2000, %2003
  %region_0_250_constant_68445 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2005 = xor i32 %2004, %region_0_250_constant_68445
  %2006 = zext i32 %2005 to i64
  %region_0_250_constant_18446 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2007 = mul i64 %2006, %region_0_250_constant_18446
  %2008 = lshr i64 %2007, %1997
  %shft.chk447 = icmp ult i64 %1997, 64
  %2009 = select i1 %shft.chk447, i64 %2008, i64 0
  %2010 = trunc i64 %2009 to i32
  %2011 = trunc i64 %1975 to i32
  %2012 = zext i32 %2011 to i64
  %2013 = mul i64 %2012, %region_0_250_constant_18446
  %2014 = trunc i64 %2013 to i32
  %2015 = xor i32 %2010, %2014
  %region_0_250_constant_86448 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2016 = xor i32 %2015, %region_0_250_constant_86448
  %2017 = zext i32 %2016 to i64
  %2018 = mul i64 %2017, %region_0_250_constant_46442
  %2019 = lshr i64 %2018, %1997
  %shft.chk449 = icmp ult i64 %1997, 64
  %2020 = select i1 %shft.chk449, i64 %2019, i64 0
  %2021 = trunc i64 %2020 to i32
  %2022 = lshr i64 %2013, %1997
  %shft.chk450 = icmp ult i64 %1997, 64
  %2023 = select i1 %shft.chk450, i64 %2022, i64 0
  %2024 = trunc i64 %2023 to i32
  %2025 = lshr i64 %1993, %1997
  %shft.chk451 = icmp ult i64 %1997, 64
  %2026 = select i1 %shft.chk451, i64 %2025, i64 0
  %2027 = trunc i64 %2026 to i32
  %2028 = xor i32 %2024, %2027
  %region_0_250_constant_42452 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2029 = xor i32 %2028, %region_0_250_constant_42452
  %2030 = zext i32 %2029 to i64
  %2031 = mul i64 %2030, %region_0_250_constant_46442
  %2032 = trunc i64 %2031 to i32
  %2033 = xor i32 %2021, %2032
  %region_0_250_constant_104453 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2034 = xor i32 %2033, %region_0_250_constant_104453
  %2035 = zext i32 %2034 to i64
  %2036 = mul i64 %2035, %region_0_250_constant_18446
  %2037 = lshr i64 %2036, %1997
  %shft.chk454 = icmp ult i64 %1997, 64
  %2038 = select i1 %shft.chk454, i64 %2037, i64 0
  %2039 = trunc i64 %2038 to i32
  %2040 = lshr i64 %2031, %1997
  %shft.chk455 = icmp ult i64 %1997, 64
  %2041 = select i1 %shft.chk455, i64 %2040, i64 0
  %2042 = trunc i64 %2041 to i32
  %2043 = trunc i64 %1996 to i32
  %2044 = xor i32 %2042, %2043
  %region_0_250_constant_56456 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %2045 = xor i32 %2044, %region_0_250_constant_56456
  %2046 = zext i32 %2045 to i64
  %2047 = mul i64 %2046, %region_0_250_constant_18446
  %2048 = trunc i64 %2047 to i32
  %2049 = xor i32 %2039, %2048
  %region_0_250_constant_122457 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %2050 = xor i32 %2049, %region_0_250_constant_122457
  %2051 = zext i32 %2050 to i64
  %2052 = mul i64 %2051, %region_0_250_constant_46442
  %2053 = lshr i64 %2052, %1997
  %shft.chk458 = icmp ult i64 %1997, 64
  %2054 = select i1 %shft.chk458, i64 %2053, i64 0
  %2055 = trunc i64 %2054 to i32
  %2056 = lshr i64 %2047, %1997
  %shft.chk459 = icmp ult i64 %1997, 64
  %2057 = select i1 %shft.chk459, i64 %2056, i64 0
  %2058 = trunc i64 %2057 to i32
  %2059 = trunc i64 %2007 to i32
  %2060 = xor i32 %2058, %2059
  %region_0_250_constant_75460 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2061 = xor i32 %2060, %region_0_250_constant_75460
  %2062 = zext i32 %2061 to i64
  %2063 = mul i64 %2062, %region_0_250_constant_46442
  %2064 = trunc i64 %2063 to i32
  %2065 = xor i32 %2055, %2064
  %region_0_250_constant_140461 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2066 = xor i32 %2065, %region_0_250_constant_140461
  %2067 = zext i32 %2066 to i64
  %2068 = mul i64 %2067, %region_0_250_constant_18446
  %2069 = lshr i64 %2068, %1997
  %shft.chk462 = icmp ult i64 %1997, 64
  %2070 = select i1 %shft.chk462, i64 %2069, i64 0
  %2071 = trunc i64 %2070 to i32
  %2072 = lshr i64 %2063, %1997
  %shft.chk463 = icmp ult i64 %1997, 64
  %2073 = select i1 %shft.chk463, i64 %2072, i64 0
  %2074 = trunc i64 %2073 to i32
  %2075 = trunc i64 %2018 to i32
  %2076 = xor i32 %2074, %2075
  %region_0_250_constant_93464 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %2077 = xor i32 %2076, %region_0_250_constant_93464
  %2078 = zext i32 %2077 to i64
  %2079 = mul i64 %2078, %region_0_250_constant_18446
  %2080 = trunc i64 %2079 to i32
  %2081 = xor i32 %2071, %2080
  %region_0_250_constant_158465 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %2082 = xor i32 %2081, %region_0_250_constant_158465
  %2083 = zext i32 %2082 to i64
  %2084 = mul i64 %2083, %region_0_250_constant_46442
  %2085 = lshr i64 %2084, %1997
  %shft.chk466 = icmp ult i64 %1997, 64
  %2086 = select i1 %shft.chk466, i64 %2085, i64 0
  %2087 = trunc i64 %2086 to i32
  %2088 = lshr i64 %2079, %1997
  %shft.chk467 = icmp ult i64 %1997, 64
  %2089 = select i1 %shft.chk467, i64 %2088, i64 0
  %2090 = trunc i64 %2089 to i32
  %2091 = trunc i64 %2036 to i32
  %2092 = xor i32 %2090, %2091
  %region_0_250_constant_111468 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %2093 = xor i32 %2092, %region_0_250_constant_111468
  %2094 = zext i32 %2093 to i64
  %2095 = mul i64 %2094, %region_0_250_constant_46442
  %2096 = trunc i64 %2095 to i32
  %2097 = xor i32 %2087, %2096
  %region_0_250_constant_176469 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %2098 = xor i32 %2097, %region_0_250_constant_176469
  %2099 = zext i32 %2098 to i64
  %2100 = mul i64 %2099, %region_0_250_constant_18446
  %2101 = lshr i64 %2100, %1997
  %shft.chk470 = icmp ult i64 %1997, 64
  %2102 = select i1 %shft.chk470, i64 %2101, i64 0
  %2103 = trunc i64 %2102 to i32
  %2104 = lshr i64 %2095, %1997
  %shft.chk471 = icmp ult i64 %1997, 64
  %2105 = select i1 %shft.chk471, i64 %2104, i64 0
  %2106 = trunc i64 %2105 to i32
  %2107 = trunc i64 %2052 to i32
  %2108 = xor i32 %2106, %2107
  %region_0_250_constant_129472 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %2109 = xor i32 %2108, %region_0_250_constant_129472
  %2110 = zext i32 %2109 to i64
  %2111 = mul i64 %2110, %region_0_250_constant_18446
  %2112 = trunc i64 %2111 to i32
  %2113 = xor i32 %2103, %2112
  %region_0_250_constant_194473 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %2114 = xor i32 %2113, %region_0_250_constant_194473
  %2115 = zext i32 %2114 to i64
  %2116 = mul i64 %2115, %region_0_250_constant_46442
  %2117 = lshr i64 %2116, %1997
  %shft.chk474 = icmp ult i64 %1997, 64
  %2118 = select i1 %shft.chk474, i64 %2117, i64 0
  %2119 = trunc i64 %2118 to i32
  %2120 = lshr i64 %2111, %1997
  %shft.chk475 = icmp ult i64 %1997, 64
  %2121 = select i1 %shft.chk475, i64 %2120, i64 0
  %2122 = trunc i64 %2121 to i32
  %2123 = trunc i64 %2068 to i32
  %2124 = xor i32 %2122, %2123
  %region_0_250_constant_147476 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2125 = xor i32 %2124, %region_0_250_constant_147476
  %2126 = zext i32 %2125 to i64
  %2127 = mul i64 %2126, %region_0_250_constant_46442
  %2128 = trunc i64 %2127 to i32
  %2129 = xor i32 %2119, %2128
  %region_0_250_constant_211477 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2130 = xor i32 %2129, %region_0_250_constant_211477
  %2131 = zext i32 %2130 to i64
  %2132 = mul i64 %2131, %region_0_250_constant_18446
  %2133 = lshr i64 %2132, %1997
  %shft.chk478 = icmp ult i64 %1997, 64
  %2134 = select i1 %shft.chk478, i64 %2133, i64 0
  %2135 = trunc i64 %2134 to i32
  %2136 = lshr i64 %2127, %1997
  %shft.chk479 = icmp ult i64 %1997, 64
  %2137 = select i1 %shft.chk479, i64 %2136, i64 0
  %2138 = trunc i64 %2137 to i32
  %2139 = trunc i64 %2084 to i32
  %2140 = xor i32 %2138, %2139
  %region_0_250_constant_165480 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %2141 = xor i32 %2140, %region_0_250_constant_165480
  %2142 = zext i32 %2141 to i64
  %2143 = mul i64 %2142, %region_0_250_constant_18446
  %2144 = trunc i64 %2143 to i32
  %2145 = xor i32 %2135, %2144
  %region_0_250_constant_220481 = load i32, i32* bitcast ([4 x i8]* @53 to i32*), align 4
  %2146 = xor i32 %2145, %region_0_250_constant_220481
  br label %concatenate.226.merge347

concat_index_from_operand_id3482:                 ; preds = %concatenate.pivot.3.530
  %2147 = phi i32 [ 3, %concatenate.pivot.3.530 ]
  %2148 = sub nsw i32 %1587, %2147
  %2149 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2150 = load i64, i64* %2149, align 8, !invariant.load !22
  %2151 = trunc i64 %2150 to i32
  %2152 = zext i32 %2151 to i64
  %2153 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2154 = lshr i64 %2150, %2153
  %shft.chk483 = icmp ult i64 %2153, 64
  %2155 = select i1 %shft.chk483, i64 %2154, i64 0
  %2156 = trunc i64 %2155 to i32
  %2157 = zext i32 %2156 to i64
  %2158 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2159 = shl i64 %2157, %2158
  %shft.chk484 = icmp ult i64 %2158, 64
  %2160 = select i1 %shft.chk484, i64 %2159, i64 0
  %2161 = or i64 %2152, %2160
  %2162 = mul nuw nsw i32 %1588, 1
  %2163 = add nuw nsw i32 0, %2162
  %2164 = zext i32 %2163 to i64
  %2165 = add i64 %2161, %2164
  %2166 = icmp ult i64 %2165, %2161
  %2167 = zext i1 %2166 to i8
  %2168 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2169 = load i64, i64* %2168, align 8, !invariant.load !22
  %2170 = trunc i64 %2169 to i32
  %2171 = zext i32 %2170 to i64
  %2172 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2173 = lshr i64 %2169, %2172
  %shft.chk485 = icmp ult i64 %2172, 64
  %2174 = select i1 %shft.chk485, i64 %2173, i64 0
  %2175 = trunc i64 %2174 to i32
  %2176 = zext i32 %2175 to i64
  %2177 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2178 = shl i64 %2176, %2177
  %shft.chk486 = icmp ult i64 %2177, 64
  %2179 = select i1 %shft.chk486, i64 %2178, i64 0
  %2180 = or i64 %2171, %2179
  %region_0_250_constant_34487 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %2181 = add i64 %2180, %region_0_250_constant_34487
  %2182 = trunc i8 %2167 to i1
  %2183 = select i1 %2182, i64 %2181, i64 %2180
  %2184 = trunc i64 %2183 to i32
  %2185 = zext i32 %2184 to i64
  %region_0_250_constant_46488 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %2186 = mul i64 %2185, %region_0_250_constant_46488
  %2187 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2188 = lshr i64 %2186, %2187
  %shft.chk489 = icmp ult i64 %2187, 64
  %2189 = select i1 %shft.chk489, i64 %2188, i64 0
  %2190 = trunc i64 %2189 to i32
  %2191 = lshr i64 %2165, %2187
  %shft.chk490 = icmp ult i64 %2187, 64
  %2192 = select i1 %shft.chk490, i64 %2191, i64 0
  %2193 = trunc i64 %2192 to i32
  %2194 = xor i32 %2190, %2193
  %region_0_250_constant_68491 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2195 = xor i32 %2194, %region_0_250_constant_68491
  %2196 = zext i32 %2195 to i64
  %region_0_250_constant_18492 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2197 = mul i64 %2196, %region_0_250_constant_18492
  %2198 = lshr i64 %2197, %2187
  %shft.chk493 = icmp ult i64 %2187, 64
  %2199 = select i1 %shft.chk493, i64 %2198, i64 0
  %2200 = trunc i64 %2199 to i32
  %2201 = trunc i64 %2165 to i32
  %2202 = zext i32 %2201 to i64
  %2203 = mul i64 %2202, %region_0_250_constant_18492
  %2204 = trunc i64 %2203 to i32
  %2205 = xor i32 %2200, %2204
  %region_0_250_constant_86494 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2206 = xor i32 %2205, %region_0_250_constant_86494
  %2207 = zext i32 %2206 to i64
  %2208 = mul i64 %2207, %region_0_250_constant_46488
  %2209 = lshr i64 %2208, %2187
  %shft.chk495 = icmp ult i64 %2187, 64
  %2210 = select i1 %shft.chk495, i64 %2209, i64 0
  %2211 = trunc i64 %2210 to i32
  %2212 = lshr i64 %2203, %2187
  %shft.chk496 = icmp ult i64 %2187, 64
  %2213 = select i1 %shft.chk496, i64 %2212, i64 0
  %2214 = trunc i64 %2213 to i32
  %2215 = lshr i64 %2183, %2187
  %shft.chk497 = icmp ult i64 %2187, 64
  %2216 = select i1 %shft.chk497, i64 %2215, i64 0
  %2217 = trunc i64 %2216 to i32
  %2218 = xor i32 %2214, %2217
  %region_0_250_constant_42498 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2219 = xor i32 %2218, %region_0_250_constant_42498
  %2220 = zext i32 %2219 to i64
  %2221 = mul i64 %2220, %region_0_250_constant_46488
  %2222 = trunc i64 %2221 to i32
  %2223 = xor i32 %2211, %2222
  %region_0_250_constant_104499 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2224 = xor i32 %2223, %region_0_250_constant_104499
  %2225 = zext i32 %2224 to i64
  %2226 = mul i64 %2225, %region_0_250_constant_18492
  %2227 = lshr i64 %2226, %2187
  %shft.chk500 = icmp ult i64 %2187, 64
  %2228 = select i1 %shft.chk500, i64 %2227, i64 0
  %2229 = trunc i64 %2228 to i32
  %2230 = lshr i64 %2221, %2187
  %shft.chk501 = icmp ult i64 %2187, 64
  %2231 = select i1 %shft.chk501, i64 %2230, i64 0
  %2232 = trunc i64 %2231 to i32
  %2233 = trunc i64 %2186 to i32
  %2234 = xor i32 %2232, %2233
  %region_0_250_constant_56502 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %2235 = xor i32 %2234, %region_0_250_constant_56502
  %2236 = zext i32 %2235 to i64
  %2237 = mul i64 %2236, %region_0_250_constant_18492
  %2238 = trunc i64 %2237 to i32
  %2239 = xor i32 %2229, %2238
  %region_0_250_constant_122503 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %2240 = xor i32 %2239, %region_0_250_constant_122503
  %2241 = zext i32 %2240 to i64
  %2242 = mul i64 %2241, %region_0_250_constant_46488
  %2243 = lshr i64 %2242, %2187
  %shft.chk504 = icmp ult i64 %2187, 64
  %2244 = select i1 %shft.chk504, i64 %2243, i64 0
  %2245 = trunc i64 %2244 to i32
  %2246 = lshr i64 %2237, %2187
  %shft.chk505 = icmp ult i64 %2187, 64
  %2247 = select i1 %shft.chk505, i64 %2246, i64 0
  %2248 = trunc i64 %2247 to i32
  %2249 = trunc i64 %2197 to i32
  %2250 = xor i32 %2248, %2249
  %region_0_250_constant_75506 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2251 = xor i32 %2250, %region_0_250_constant_75506
  %2252 = zext i32 %2251 to i64
  %2253 = mul i64 %2252, %region_0_250_constant_46488
  %2254 = trunc i64 %2253 to i32
  %2255 = xor i32 %2245, %2254
  %region_0_250_constant_140507 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2256 = xor i32 %2255, %region_0_250_constant_140507
  %2257 = zext i32 %2256 to i64
  %2258 = mul i64 %2257, %region_0_250_constant_18492
  %2259 = lshr i64 %2258, %2187
  %shft.chk508 = icmp ult i64 %2187, 64
  %2260 = select i1 %shft.chk508, i64 %2259, i64 0
  %2261 = trunc i64 %2260 to i32
  %2262 = lshr i64 %2253, %2187
  %shft.chk509 = icmp ult i64 %2187, 64
  %2263 = select i1 %shft.chk509, i64 %2262, i64 0
  %2264 = trunc i64 %2263 to i32
  %2265 = trunc i64 %2208 to i32
  %2266 = xor i32 %2264, %2265
  %region_0_250_constant_93510 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %2267 = xor i32 %2266, %region_0_250_constant_93510
  %2268 = zext i32 %2267 to i64
  %2269 = mul i64 %2268, %region_0_250_constant_18492
  %2270 = trunc i64 %2269 to i32
  %2271 = xor i32 %2261, %2270
  %region_0_250_constant_158511 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %2272 = xor i32 %2271, %region_0_250_constant_158511
  %2273 = zext i32 %2272 to i64
  %2274 = mul i64 %2273, %region_0_250_constant_46488
  %2275 = lshr i64 %2274, %2187
  %shft.chk512 = icmp ult i64 %2187, 64
  %2276 = select i1 %shft.chk512, i64 %2275, i64 0
  %2277 = trunc i64 %2276 to i32
  %2278 = lshr i64 %2269, %2187
  %shft.chk513 = icmp ult i64 %2187, 64
  %2279 = select i1 %shft.chk513, i64 %2278, i64 0
  %2280 = trunc i64 %2279 to i32
  %2281 = trunc i64 %2226 to i32
  %2282 = xor i32 %2280, %2281
  %region_0_250_constant_111514 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %2283 = xor i32 %2282, %region_0_250_constant_111514
  %2284 = zext i32 %2283 to i64
  %2285 = mul i64 %2284, %region_0_250_constant_46488
  %2286 = trunc i64 %2285 to i32
  %2287 = xor i32 %2277, %2286
  %region_0_250_constant_176515 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %2288 = xor i32 %2287, %region_0_250_constant_176515
  %2289 = zext i32 %2288 to i64
  %2290 = mul i64 %2289, %region_0_250_constant_18492
  %2291 = lshr i64 %2290, %2187
  %shft.chk516 = icmp ult i64 %2187, 64
  %2292 = select i1 %shft.chk516, i64 %2291, i64 0
  %2293 = trunc i64 %2292 to i32
  %2294 = lshr i64 %2285, %2187
  %shft.chk517 = icmp ult i64 %2187, 64
  %2295 = select i1 %shft.chk517, i64 %2294, i64 0
  %2296 = trunc i64 %2295 to i32
  %2297 = trunc i64 %2242 to i32
  %2298 = xor i32 %2296, %2297
  %region_0_250_constant_129518 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %2299 = xor i32 %2298, %region_0_250_constant_129518
  %2300 = zext i32 %2299 to i64
  %2301 = mul i64 %2300, %region_0_250_constant_18492
  %2302 = trunc i64 %2301 to i32
  %2303 = xor i32 %2293, %2302
  %region_0_250_constant_194519 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %2304 = xor i32 %2303, %region_0_250_constant_194519
  %2305 = zext i32 %2304 to i64
  %2306 = mul i64 %2305, %region_0_250_constant_46488
  %2307 = lshr i64 %2306, %2187
  %shft.chk520 = icmp ult i64 %2187, 64
  %2308 = select i1 %shft.chk520, i64 %2307, i64 0
  %2309 = trunc i64 %2308 to i32
  %2310 = lshr i64 %2301, %2187
  %shft.chk521 = icmp ult i64 %2187, 64
  %2311 = select i1 %shft.chk521, i64 %2310, i64 0
  %2312 = trunc i64 %2311 to i32
  %2313 = trunc i64 %2258 to i32
  %2314 = xor i32 %2312, %2313
  %region_0_250_constant_147522 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2315 = xor i32 %2314, %region_0_250_constant_147522
  %2316 = zext i32 %2315 to i64
  %2317 = mul i64 %2316, %region_0_250_constant_46488
  %2318 = trunc i64 %2317 to i32
  %2319 = xor i32 %2309, %2318
  %region_0_250_constant_211523 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2320 = xor i32 %2319, %region_0_250_constant_211523
  %2321 = zext i32 %2320 to i64
  %2322 = mul i64 %2321, %region_0_250_constant_18492
  %2323 = trunc i64 %2322 to i32
  br label %concatenate.226.merge347

concatenate.pivot.2.524:                          ; preds = %concatenate.226.merge153
  %2324 = icmp ult i32 %1587, 2
  br i1 %2324, label %concatenate.pivot.1.525, label %concatenate.pivot.3.528

concatenate.pivot.1.525:                          ; preds = %concatenate.pivot.2.524
  %2325 = icmp ult i32 %1587, 1
  br i1 %2325, label %concatenate.pivot.0.526, label %concatenate.pivot.1.527

concatenate.pivot.0.526:                          ; preds = %concatenate.pivot.1.525
  br label %concat_index_from_operand_id0348

concatenate.pivot.1.527:                          ; preds = %concatenate.pivot.1.525
  br label %concat_index_from_operand_id1394

concatenate.pivot.3.528:                          ; preds = %concatenate.pivot.2.524
  %2326 = icmp ult i32 %1587, 3
  br i1 %2326, label %concatenate.pivot.2.529, label %concatenate.pivot.3.530

concatenate.pivot.2.529:                          ; preds = %concatenate.pivot.3.528
  br label %concat_index_from_operand_id2436

concatenate.pivot.3.530:                          ; preds = %concatenate.pivot.3.528
  br label %concat_index_from_operand_id3482

concatenate.226.merge347:                         ; preds = %concat_index_from_operand_id3482, %concat_index_from_operand_id2436, %concat_index_from_operand_id1394, %concat_index_from_operand_id0348
  %2327 = phi i32 [ %1779, %concat_index_from_operand_id0348 ], [ %1956, %concat_index_from_operand_id1394 ], [ %2146, %concat_index_from_operand_id2436 ], [ %2323, %concat_index_from_operand_id3482 ]
  %region_0_250_constant_227531 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2328 = lshr i32 %2327, %region_0_250_constant_227531
  %shft.chk532 = icmp ult i32 %region_0_250_constant_227531, 32
  %2329 = select i1 %shft.chk532, i32 %2328, i32 0
  %2330 = uitofp i32 %2329 to float
  %region_0_250_constant_231533 = load float, float* bitcast ([4 x i8]* @32 to float*), align 4
  %multiply.233534 = fmul float %2330, %region_0_250_constant_231533
  %region_0_250_constant_234535 = load float, float* bitcast ([4 x i8]* @31 to float*), align 4
  %compare.236536 = fcmp olt float %multiply.233534, %region_0_250_constant_234535
  %2331 = zext i1 %compare.236536 to i8
  %region_0_250_constant_238537 = load i8, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @30, i32 0, i32 0), align 1
  %2332 = icmp eq i8 %2331, %region_0_250_constant_238537
  %2333 = zext i1 %2332 to i8
  %2334 = mul nuw nsw i32 %1579, 1
  %2335 = add nuw nsw i32 0, %2334
  %2336 = udiv i32 %2335, 16
  %2337 = mul nuw nsw i32 %1575, 1
  %2338 = add nuw nsw i32 0, %2337
  %2339 = mul nuw nsw i32 %1576, 32
  %2340 = add nuw nsw i32 %2338, %2339
  %2341 = udiv i32 %2340, 256
  %2342 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %1, i32 0, i32 %2340, i32 %2335
  %2343 = load float, float* %2342, align 4, !invariant.load !22
  %region_0_250_constant_242538 = load float, float* bitcast ([4 x i8]* @29 to float*), align 4
  %2344 = trunc i8 %2333 to i1
  %2345 = select i1 %2344, float %2343, float %region_0_250_constant_242538
  %region_0_250_constant_245539 = load float, float* bitcast ([4 x i8]* @28 to float*), align 4
  %multiply.247540 = fmul float %2345, %region_0_250_constant_245539
  %2346 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %5, i32 0, i32 %17, i32 %19
  store float %multiply.247540, float* %2346, align 4
  %2347 = mul nuw nsw i32 %21, 1
  %2348 = add nuw nsw i32 0, %2347
  %2349 = urem i32 %2348, 32
  %2350 = udiv i32 %2348, 32
  %2351 = udiv i32 %2350, 8
  %2352 = mul nuw nsw i32 %23, 1
  %2353 = add nuw nsw i32 0, %2352
  %2354 = udiv i32 %2353, 16
  %2355 = mul nuw nsw i32 %2353, 1
  %2356 = add nuw nsw i32 0, %2355
  %2357 = mul nuw nsw i32 %2349, 16
  %2358 = add nuw nsw i32 %2356, %2357
  %2359 = mul nuw nsw i32 %2350, 512
  %2360 = add nuw nsw i32 %2358, %2359
  %2361 = urem i32 %2360, 4
  %2362 = udiv i32 %2360, 4
  %2363 = udiv i32 %2362, 1024
  br label %concatenate.pivot.2.718

concat_index_from_operand_id0542:                 ; preds = %concatenate.pivot.0.720
  %2364 = phi i32 [ 0, %concatenate.pivot.0.720 ]
  %2365 = sub nsw i32 %2361, %2364
  %2366 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2367 = load i64, i64* %2366, align 8, !invariant.load !22
  %2368 = trunc i64 %2367 to i32
  %2369 = zext i32 %2368 to i64
  %2370 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2371 = lshr i64 %2367, %2370
  %shft.chk543 = icmp ult i64 %2370, 64
  %2372 = select i1 %shft.chk543, i64 %2371, i64 0
  %2373 = trunc i64 %2372 to i32
  %2374 = zext i32 %2373 to i64
  %2375 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2376 = shl i64 %2374, %2375
  %shft.chk544 = icmp ult i64 %2375, 64
  %2377 = select i1 %shft.chk544, i64 %2376, i64 0
  %2378 = or i64 %2369, %2377
  %2379 = mul nuw nsw i32 %2362, 1
  %2380 = add nuw nsw i32 0, %2379
  %2381 = zext i32 %2380 to i64
  %2382 = add i64 %2378, %2381
  %2383 = trunc i64 %2382 to i32
  %2384 = zext i32 %2383 to i64
  %region_0_250_constant_18545 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2385 = mul i64 %2384, %region_0_250_constant_18545
  %2386 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2387 = lshr i64 %2385, %2386
  %shft.chk546 = icmp ult i64 %2386, 64
  %2388 = select i1 %shft.chk546, i64 %2387, i64 0
  %2389 = trunc i64 %2388 to i32
  %2390 = icmp ult i64 %2382, %2378
  %2391 = zext i1 %2390 to i8
  %2392 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2393 = load i64, i64* %2392, align 8, !invariant.load !22
  %2394 = trunc i64 %2393 to i32
  %2395 = zext i32 %2394 to i64
  %2396 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2397 = lshr i64 %2393, %2396
  %shft.chk547 = icmp ult i64 %2396, 64
  %2398 = select i1 %shft.chk547, i64 %2397, i64 0
  %2399 = trunc i64 %2398 to i32
  %2400 = zext i32 %2399 to i64
  %2401 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2402 = shl i64 %2400, %2401
  %shft.chk548 = icmp ult i64 %2401, 64
  %2403 = select i1 %shft.chk548, i64 %2402, i64 0
  %2404 = or i64 %2395, %2403
  %region_0_250_constant_34549 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %2405 = add i64 %2404, %region_0_250_constant_34549
  %2406 = trunc i8 %2391 to i1
  %2407 = select i1 %2406, i64 %2405, i64 %2404
  %2408 = lshr i64 %2407, %2386
  %shft.chk550 = icmp ult i64 %2386, 64
  %2409 = select i1 %shft.chk550, i64 %2408, i64 0
  %2410 = trunc i64 %2409 to i32
  %2411 = xor i32 %2389, %2410
  %region_0_250_constant_42551 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2412 = xor i32 %2411, %region_0_250_constant_42551
  %2413 = zext i32 %2412 to i64
  %region_0_250_constant_46552 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %2414 = mul i64 %2413, %region_0_250_constant_46552
  %2415 = lshr i64 %2414, %2386
  %shft.chk553 = icmp ult i64 %2386, 64
  %2416 = select i1 %shft.chk553, i64 %2415, i64 0
  %2417 = trunc i64 %2416 to i32
  %2418 = trunc i64 %2407 to i32
  %2419 = zext i32 %2418 to i64
  %2420 = mul i64 %2419, %region_0_250_constant_46552
  %2421 = trunc i64 %2420 to i32
  %2422 = xor i32 %2417, %2421
  %region_0_250_constant_56554 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %2423 = xor i32 %2422, %region_0_250_constant_56554
  %2424 = zext i32 %2423 to i64
  %2425 = mul i64 %2424, %region_0_250_constant_18545
  %2426 = lshr i64 %2425, %2386
  %shft.chk555 = icmp ult i64 %2386, 64
  %2427 = select i1 %shft.chk555, i64 %2426, i64 0
  %2428 = trunc i64 %2427 to i32
  %2429 = lshr i64 %2420, %2386
  %shft.chk556 = icmp ult i64 %2386, 64
  %2430 = select i1 %shft.chk556, i64 %2429, i64 0
  %2431 = trunc i64 %2430 to i32
  %2432 = lshr i64 %2382, %2386
  %shft.chk557 = icmp ult i64 %2386, 64
  %2433 = select i1 %shft.chk557, i64 %2432, i64 0
  %2434 = trunc i64 %2433 to i32
  %2435 = xor i32 %2431, %2434
  %region_0_250_constant_68558 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2436 = xor i32 %2435, %region_0_250_constant_68558
  %2437 = zext i32 %2436 to i64
  %2438 = mul i64 %2437, %region_0_250_constant_18545
  %2439 = trunc i64 %2438 to i32
  %2440 = xor i32 %2428, %2439
  %region_0_250_constant_75559 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2441 = xor i32 %2440, %region_0_250_constant_75559
  %2442 = zext i32 %2441 to i64
  %2443 = mul i64 %2442, %region_0_250_constant_46552
  %2444 = lshr i64 %2443, %2386
  %shft.chk560 = icmp ult i64 %2386, 64
  %2445 = select i1 %shft.chk560, i64 %2444, i64 0
  %2446 = trunc i64 %2445 to i32
  %2447 = lshr i64 %2438, %2386
  %shft.chk561 = icmp ult i64 %2386, 64
  %2448 = select i1 %shft.chk561, i64 %2447, i64 0
  %2449 = trunc i64 %2448 to i32
  %2450 = trunc i64 %2385 to i32
  %2451 = xor i32 %2449, %2450
  %region_0_250_constant_86562 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2452 = xor i32 %2451, %region_0_250_constant_86562
  %2453 = zext i32 %2452 to i64
  %2454 = mul i64 %2453, %region_0_250_constant_46552
  %2455 = trunc i64 %2454 to i32
  %2456 = xor i32 %2446, %2455
  %region_0_250_constant_93563 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %2457 = xor i32 %2456, %region_0_250_constant_93563
  %2458 = zext i32 %2457 to i64
  %2459 = mul i64 %2458, %region_0_250_constant_18545
  %2460 = lshr i64 %2459, %2386
  %shft.chk564 = icmp ult i64 %2386, 64
  %2461 = select i1 %shft.chk564, i64 %2460, i64 0
  %2462 = trunc i64 %2461 to i32
  %2463 = lshr i64 %2454, %2386
  %shft.chk565 = icmp ult i64 %2386, 64
  %2464 = select i1 %shft.chk565, i64 %2463, i64 0
  %2465 = trunc i64 %2464 to i32
  %2466 = trunc i64 %2414 to i32
  %2467 = xor i32 %2465, %2466
  %region_0_250_constant_104566 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2468 = xor i32 %2467, %region_0_250_constant_104566
  %2469 = zext i32 %2468 to i64
  %2470 = mul i64 %2469, %region_0_250_constant_18545
  %2471 = trunc i64 %2470 to i32
  %2472 = xor i32 %2462, %2471
  %region_0_250_constant_111567 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %2473 = xor i32 %2472, %region_0_250_constant_111567
  %2474 = zext i32 %2473 to i64
  %2475 = mul i64 %2474, %region_0_250_constant_46552
  %2476 = lshr i64 %2475, %2386
  %shft.chk568 = icmp ult i64 %2386, 64
  %2477 = select i1 %shft.chk568, i64 %2476, i64 0
  %2478 = trunc i64 %2477 to i32
  %2479 = lshr i64 %2470, %2386
  %shft.chk569 = icmp ult i64 %2386, 64
  %2480 = select i1 %shft.chk569, i64 %2479, i64 0
  %2481 = trunc i64 %2480 to i32
  %2482 = trunc i64 %2425 to i32
  %2483 = xor i32 %2481, %2482
  %region_0_250_constant_122570 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %2484 = xor i32 %2483, %region_0_250_constant_122570
  %2485 = zext i32 %2484 to i64
  %2486 = mul i64 %2485, %region_0_250_constant_46552
  %2487 = trunc i64 %2486 to i32
  %2488 = xor i32 %2478, %2487
  %region_0_250_constant_129571 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %2489 = xor i32 %2488, %region_0_250_constant_129571
  %2490 = zext i32 %2489 to i64
  %2491 = mul i64 %2490, %region_0_250_constant_18545
  %2492 = lshr i64 %2491, %2386
  %shft.chk572 = icmp ult i64 %2386, 64
  %2493 = select i1 %shft.chk572, i64 %2492, i64 0
  %2494 = trunc i64 %2493 to i32
  %2495 = lshr i64 %2486, %2386
  %shft.chk573 = icmp ult i64 %2386, 64
  %2496 = select i1 %shft.chk573, i64 %2495, i64 0
  %2497 = trunc i64 %2496 to i32
  %2498 = trunc i64 %2443 to i32
  %2499 = xor i32 %2497, %2498
  %region_0_250_constant_140574 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2500 = xor i32 %2499, %region_0_250_constant_140574
  %2501 = zext i32 %2500 to i64
  %2502 = mul i64 %2501, %region_0_250_constant_18545
  %2503 = trunc i64 %2502 to i32
  %2504 = xor i32 %2494, %2503
  %region_0_250_constant_147575 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2505 = xor i32 %2504, %region_0_250_constant_147575
  %2506 = zext i32 %2505 to i64
  %2507 = mul i64 %2506, %region_0_250_constant_46552
  %2508 = lshr i64 %2507, %2386
  %shft.chk576 = icmp ult i64 %2386, 64
  %2509 = select i1 %shft.chk576, i64 %2508, i64 0
  %2510 = trunc i64 %2509 to i32
  %2511 = lshr i64 %2502, %2386
  %shft.chk577 = icmp ult i64 %2386, 64
  %2512 = select i1 %shft.chk577, i64 %2511, i64 0
  %2513 = trunc i64 %2512 to i32
  %2514 = trunc i64 %2459 to i32
  %2515 = xor i32 %2513, %2514
  %region_0_250_constant_158578 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %2516 = xor i32 %2515, %region_0_250_constant_158578
  %2517 = zext i32 %2516 to i64
  %2518 = mul i64 %2517, %region_0_250_constant_46552
  %2519 = trunc i64 %2518 to i32
  %2520 = xor i32 %2510, %2519
  %region_0_250_constant_165579 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %2521 = xor i32 %2520, %region_0_250_constant_165579
  %2522 = zext i32 %2521 to i64
  %2523 = mul i64 %2522, %region_0_250_constant_18545
  %2524 = lshr i64 %2523, %2386
  %shft.chk580 = icmp ult i64 %2386, 64
  %2525 = select i1 %shft.chk580, i64 %2524, i64 0
  %2526 = trunc i64 %2525 to i32
  %2527 = lshr i64 %2518, %2386
  %shft.chk581 = icmp ult i64 %2386, 64
  %2528 = select i1 %shft.chk581, i64 %2527, i64 0
  %2529 = trunc i64 %2528 to i32
  %2530 = trunc i64 %2475 to i32
  %2531 = xor i32 %2529, %2530
  %region_0_250_constant_176582 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %2532 = xor i32 %2531, %region_0_250_constant_176582
  %2533 = zext i32 %2532 to i64
  %2534 = mul i64 %2533, %region_0_250_constant_18545
  %2535 = trunc i64 %2534 to i32
  %2536 = xor i32 %2526, %2535
  %region_0_250_constant_183583 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %2537 = xor i32 %2536, %region_0_250_constant_183583
  %2538 = zext i32 %2537 to i64
  %2539 = mul i64 %2538, %region_0_250_constant_46552
  %2540 = lshr i64 %2539, %2386
  %shft.chk584 = icmp ult i64 %2386, 64
  %2541 = select i1 %shft.chk584, i64 %2540, i64 0
  %2542 = trunc i64 %2541 to i32
  %2543 = lshr i64 %2534, %2386
  %shft.chk585 = icmp ult i64 %2386, 64
  %2544 = select i1 %shft.chk585, i64 %2543, i64 0
  %2545 = trunc i64 %2544 to i32
  %2546 = trunc i64 %2491 to i32
  %2547 = xor i32 %2545, %2546
  %region_0_250_constant_194586 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %2548 = xor i32 %2547, %region_0_250_constant_194586
  %2549 = zext i32 %2548 to i64
  %2550 = mul i64 %2549, %region_0_250_constant_46552
  %2551 = trunc i64 %2550 to i32
  %2552 = xor i32 %2542, %2551
  %region_0_250_constant_201587 = load i32, i32* bitcast ([4 x i8]* @56 to i32*), align 4
  %2553 = xor i32 %2552, %region_0_250_constant_201587
  br label %concatenate.226.merge541

concat_index_from_operand_id1588:                 ; preds = %concatenate.pivot.1.721
  %2554 = phi i32 [ 1, %concatenate.pivot.1.721 ]
  %2555 = sub nsw i32 %2361, %2554
  %2556 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2557 = load i64, i64* %2556, align 8, !invariant.load !22
  %2558 = trunc i64 %2557 to i32
  %2559 = zext i32 %2558 to i64
  %2560 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2561 = lshr i64 %2557, %2560
  %shft.chk589 = icmp ult i64 %2560, 64
  %2562 = select i1 %shft.chk589, i64 %2561, i64 0
  %2563 = trunc i64 %2562 to i32
  %2564 = zext i32 %2563 to i64
  %2565 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2566 = shl i64 %2564, %2565
  %shft.chk590 = icmp ult i64 %2565, 64
  %2567 = select i1 %shft.chk590, i64 %2566, i64 0
  %2568 = or i64 %2559, %2567
  %2569 = mul nuw nsw i32 %2362, 1
  %2570 = add nuw nsw i32 0, %2569
  %2571 = zext i32 %2570 to i64
  %2572 = add i64 %2568, %2571
  %2573 = trunc i64 %2572 to i32
  %2574 = zext i32 %2573 to i64
  %region_0_250_constant_18591 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2575 = mul i64 %2574, %region_0_250_constant_18591
  %2576 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2577 = lshr i64 %2575, %2576
  %shft.chk592 = icmp ult i64 %2576, 64
  %2578 = select i1 %shft.chk592, i64 %2577, i64 0
  %2579 = trunc i64 %2578 to i32
  %2580 = icmp ult i64 %2572, %2568
  %2581 = zext i1 %2580 to i8
  %2582 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2583 = load i64, i64* %2582, align 8, !invariant.load !22
  %2584 = trunc i64 %2583 to i32
  %2585 = zext i32 %2584 to i64
  %2586 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2587 = lshr i64 %2583, %2586
  %shft.chk593 = icmp ult i64 %2586, 64
  %2588 = select i1 %shft.chk593, i64 %2587, i64 0
  %2589 = trunc i64 %2588 to i32
  %2590 = zext i32 %2589 to i64
  %2591 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2592 = shl i64 %2590, %2591
  %shft.chk594 = icmp ult i64 %2591, 64
  %2593 = select i1 %shft.chk594, i64 %2592, i64 0
  %2594 = or i64 %2585, %2593
  %region_0_250_constant_34595 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %2595 = add i64 %2594, %region_0_250_constant_34595
  %2596 = trunc i8 %2581 to i1
  %2597 = select i1 %2596, i64 %2595, i64 %2594
  %2598 = lshr i64 %2597, %2576
  %shft.chk596 = icmp ult i64 %2576, 64
  %2599 = select i1 %shft.chk596, i64 %2598, i64 0
  %2600 = trunc i64 %2599 to i32
  %2601 = xor i32 %2579, %2600
  %region_0_250_constant_42597 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2602 = xor i32 %2601, %region_0_250_constant_42597
  %2603 = zext i32 %2602 to i64
  %region_0_250_constant_46598 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %2604 = mul i64 %2603, %region_0_250_constant_46598
  %2605 = lshr i64 %2604, %2576
  %shft.chk599 = icmp ult i64 %2576, 64
  %2606 = select i1 %shft.chk599, i64 %2605, i64 0
  %2607 = trunc i64 %2606 to i32
  %2608 = trunc i64 %2597 to i32
  %2609 = zext i32 %2608 to i64
  %2610 = mul i64 %2609, %region_0_250_constant_46598
  %2611 = trunc i64 %2610 to i32
  %2612 = xor i32 %2607, %2611
  %region_0_250_constant_56600 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %2613 = xor i32 %2612, %region_0_250_constant_56600
  %2614 = zext i32 %2613 to i64
  %2615 = mul i64 %2614, %region_0_250_constant_18591
  %2616 = lshr i64 %2615, %2576
  %shft.chk601 = icmp ult i64 %2576, 64
  %2617 = select i1 %shft.chk601, i64 %2616, i64 0
  %2618 = trunc i64 %2617 to i32
  %2619 = lshr i64 %2610, %2576
  %shft.chk602 = icmp ult i64 %2576, 64
  %2620 = select i1 %shft.chk602, i64 %2619, i64 0
  %2621 = trunc i64 %2620 to i32
  %2622 = lshr i64 %2572, %2576
  %shft.chk603 = icmp ult i64 %2576, 64
  %2623 = select i1 %shft.chk603, i64 %2622, i64 0
  %2624 = trunc i64 %2623 to i32
  %2625 = xor i32 %2621, %2624
  %region_0_250_constant_68604 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2626 = xor i32 %2625, %region_0_250_constant_68604
  %2627 = zext i32 %2626 to i64
  %2628 = mul i64 %2627, %region_0_250_constant_18591
  %2629 = trunc i64 %2628 to i32
  %2630 = xor i32 %2618, %2629
  %region_0_250_constant_75605 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2631 = xor i32 %2630, %region_0_250_constant_75605
  %2632 = zext i32 %2631 to i64
  %2633 = mul i64 %2632, %region_0_250_constant_46598
  %2634 = lshr i64 %2633, %2576
  %shft.chk606 = icmp ult i64 %2576, 64
  %2635 = select i1 %shft.chk606, i64 %2634, i64 0
  %2636 = trunc i64 %2635 to i32
  %2637 = lshr i64 %2628, %2576
  %shft.chk607 = icmp ult i64 %2576, 64
  %2638 = select i1 %shft.chk607, i64 %2637, i64 0
  %2639 = trunc i64 %2638 to i32
  %2640 = trunc i64 %2575 to i32
  %2641 = xor i32 %2639, %2640
  %region_0_250_constant_86608 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2642 = xor i32 %2641, %region_0_250_constant_86608
  %2643 = zext i32 %2642 to i64
  %2644 = mul i64 %2643, %region_0_250_constant_46598
  %2645 = trunc i64 %2644 to i32
  %2646 = xor i32 %2636, %2645
  %region_0_250_constant_93609 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %2647 = xor i32 %2646, %region_0_250_constant_93609
  %2648 = zext i32 %2647 to i64
  %2649 = mul i64 %2648, %region_0_250_constant_18591
  %2650 = lshr i64 %2649, %2576
  %shft.chk610 = icmp ult i64 %2576, 64
  %2651 = select i1 %shft.chk610, i64 %2650, i64 0
  %2652 = trunc i64 %2651 to i32
  %2653 = lshr i64 %2644, %2576
  %shft.chk611 = icmp ult i64 %2576, 64
  %2654 = select i1 %shft.chk611, i64 %2653, i64 0
  %2655 = trunc i64 %2654 to i32
  %2656 = trunc i64 %2604 to i32
  %2657 = xor i32 %2655, %2656
  %region_0_250_constant_104612 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2658 = xor i32 %2657, %region_0_250_constant_104612
  %2659 = zext i32 %2658 to i64
  %2660 = mul i64 %2659, %region_0_250_constant_18591
  %2661 = trunc i64 %2660 to i32
  %2662 = xor i32 %2652, %2661
  %region_0_250_constant_111613 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %2663 = xor i32 %2662, %region_0_250_constant_111613
  %2664 = zext i32 %2663 to i64
  %2665 = mul i64 %2664, %region_0_250_constant_46598
  %2666 = lshr i64 %2665, %2576
  %shft.chk614 = icmp ult i64 %2576, 64
  %2667 = select i1 %shft.chk614, i64 %2666, i64 0
  %2668 = trunc i64 %2667 to i32
  %2669 = lshr i64 %2660, %2576
  %shft.chk615 = icmp ult i64 %2576, 64
  %2670 = select i1 %shft.chk615, i64 %2669, i64 0
  %2671 = trunc i64 %2670 to i32
  %2672 = trunc i64 %2615 to i32
  %2673 = xor i32 %2671, %2672
  %region_0_250_constant_122616 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %2674 = xor i32 %2673, %region_0_250_constant_122616
  %2675 = zext i32 %2674 to i64
  %2676 = mul i64 %2675, %region_0_250_constant_46598
  %2677 = trunc i64 %2676 to i32
  %2678 = xor i32 %2668, %2677
  %region_0_250_constant_129617 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %2679 = xor i32 %2678, %region_0_250_constant_129617
  %2680 = zext i32 %2679 to i64
  %2681 = mul i64 %2680, %region_0_250_constant_18591
  %2682 = lshr i64 %2681, %2576
  %shft.chk618 = icmp ult i64 %2576, 64
  %2683 = select i1 %shft.chk618, i64 %2682, i64 0
  %2684 = trunc i64 %2683 to i32
  %2685 = lshr i64 %2676, %2576
  %shft.chk619 = icmp ult i64 %2576, 64
  %2686 = select i1 %shft.chk619, i64 %2685, i64 0
  %2687 = trunc i64 %2686 to i32
  %2688 = trunc i64 %2633 to i32
  %2689 = xor i32 %2687, %2688
  %region_0_250_constant_140620 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2690 = xor i32 %2689, %region_0_250_constant_140620
  %2691 = zext i32 %2690 to i64
  %2692 = mul i64 %2691, %region_0_250_constant_18591
  %2693 = trunc i64 %2692 to i32
  %2694 = xor i32 %2684, %2693
  %region_0_250_constant_147621 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2695 = xor i32 %2694, %region_0_250_constant_147621
  %2696 = zext i32 %2695 to i64
  %2697 = mul i64 %2696, %region_0_250_constant_46598
  %2698 = lshr i64 %2697, %2576
  %shft.chk622 = icmp ult i64 %2576, 64
  %2699 = select i1 %shft.chk622, i64 %2698, i64 0
  %2700 = trunc i64 %2699 to i32
  %2701 = lshr i64 %2692, %2576
  %shft.chk623 = icmp ult i64 %2576, 64
  %2702 = select i1 %shft.chk623, i64 %2701, i64 0
  %2703 = trunc i64 %2702 to i32
  %2704 = trunc i64 %2649 to i32
  %2705 = xor i32 %2703, %2704
  %region_0_250_constant_158624 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %2706 = xor i32 %2705, %region_0_250_constant_158624
  %2707 = zext i32 %2706 to i64
  %2708 = mul i64 %2707, %region_0_250_constant_46598
  %2709 = trunc i64 %2708 to i32
  %2710 = xor i32 %2700, %2709
  %region_0_250_constant_165625 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %2711 = xor i32 %2710, %region_0_250_constant_165625
  %2712 = zext i32 %2711 to i64
  %2713 = mul i64 %2712, %region_0_250_constant_18591
  %2714 = lshr i64 %2713, %2576
  %shft.chk626 = icmp ult i64 %2576, 64
  %2715 = select i1 %shft.chk626, i64 %2714, i64 0
  %2716 = trunc i64 %2715 to i32
  %2717 = lshr i64 %2708, %2576
  %shft.chk627 = icmp ult i64 %2576, 64
  %2718 = select i1 %shft.chk627, i64 %2717, i64 0
  %2719 = trunc i64 %2718 to i32
  %2720 = trunc i64 %2665 to i32
  %2721 = xor i32 %2719, %2720
  %region_0_250_constant_176628 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %2722 = xor i32 %2721, %region_0_250_constant_176628
  %2723 = zext i32 %2722 to i64
  %2724 = mul i64 %2723, %region_0_250_constant_18591
  %2725 = trunc i64 %2724 to i32
  %2726 = xor i32 %2716, %2725
  %region_0_250_constant_183629 = load i32, i32* bitcast ([4 x i8]* @55 to i32*), align 4
  %2727 = xor i32 %2726, %region_0_250_constant_183629
  %2728 = zext i32 %2727 to i64
  %2729 = mul i64 %2728, %region_0_250_constant_46598
  %2730 = trunc i64 %2729 to i32
  br label %concatenate.226.merge541

concat_index_from_operand_id2630:                 ; preds = %concatenate.pivot.2.723
  %2731 = phi i32 [ 2, %concatenate.pivot.2.723 ]
  %2732 = sub nsw i32 %2361, %2731
  %2733 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2734 = load i64, i64* %2733, align 8, !invariant.load !22
  %2735 = trunc i64 %2734 to i32
  %2736 = zext i32 %2735 to i64
  %2737 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2738 = lshr i64 %2734, %2737
  %shft.chk631 = icmp ult i64 %2737, 64
  %2739 = select i1 %shft.chk631, i64 %2738, i64 0
  %2740 = trunc i64 %2739 to i32
  %2741 = zext i32 %2740 to i64
  %2742 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2743 = shl i64 %2741, %2742
  %shft.chk632 = icmp ult i64 %2742, 64
  %2744 = select i1 %shft.chk632, i64 %2743, i64 0
  %2745 = or i64 %2736, %2744
  %2746 = mul nuw nsw i32 %2362, 1
  %2747 = add nuw nsw i32 0, %2746
  %2748 = zext i32 %2747 to i64
  %2749 = add i64 %2745, %2748
  %2750 = icmp ult i64 %2749, %2745
  %2751 = zext i1 %2750 to i8
  %2752 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2753 = load i64, i64* %2752, align 8, !invariant.load !22
  %2754 = trunc i64 %2753 to i32
  %2755 = zext i32 %2754 to i64
  %2756 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2757 = lshr i64 %2753, %2756
  %shft.chk633 = icmp ult i64 %2756, 64
  %2758 = select i1 %shft.chk633, i64 %2757, i64 0
  %2759 = trunc i64 %2758 to i32
  %2760 = zext i32 %2759 to i64
  %2761 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2762 = shl i64 %2760, %2761
  %shft.chk634 = icmp ult i64 %2761, 64
  %2763 = select i1 %shft.chk634, i64 %2762, i64 0
  %2764 = or i64 %2755, %2763
  %region_0_250_constant_34635 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %2765 = add i64 %2764, %region_0_250_constant_34635
  %2766 = trunc i8 %2751 to i1
  %2767 = select i1 %2766, i64 %2765, i64 %2764
  %2768 = trunc i64 %2767 to i32
  %2769 = zext i32 %2768 to i64
  %region_0_250_constant_46636 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %2770 = mul i64 %2769, %region_0_250_constant_46636
  %2771 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2772 = lshr i64 %2770, %2771
  %shft.chk637 = icmp ult i64 %2771, 64
  %2773 = select i1 %shft.chk637, i64 %2772, i64 0
  %2774 = trunc i64 %2773 to i32
  %2775 = lshr i64 %2749, %2771
  %shft.chk638 = icmp ult i64 %2771, 64
  %2776 = select i1 %shft.chk638, i64 %2775, i64 0
  %2777 = trunc i64 %2776 to i32
  %2778 = xor i32 %2774, %2777
  %region_0_250_constant_68639 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2779 = xor i32 %2778, %region_0_250_constant_68639
  %2780 = zext i32 %2779 to i64
  %region_0_250_constant_18640 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2781 = mul i64 %2780, %region_0_250_constant_18640
  %2782 = lshr i64 %2781, %2771
  %shft.chk641 = icmp ult i64 %2771, 64
  %2783 = select i1 %shft.chk641, i64 %2782, i64 0
  %2784 = trunc i64 %2783 to i32
  %2785 = trunc i64 %2749 to i32
  %2786 = zext i32 %2785 to i64
  %2787 = mul i64 %2786, %region_0_250_constant_18640
  %2788 = trunc i64 %2787 to i32
  %2789 = xor i32 %2784, %2788
  %region_0_250_constant_86642 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2790 = xor i32 %2789, %region_0_250_constant_86642
  %2791 = zext i32 %2790 to i64
  %2792 = mul i64 %2791, %region_0_250_constant_46636
  %2793 = lshr i64 %2792, %2771
  %shft.chk643 = icmp ult i64 %2771, 64
  %2794 = select i1 %shft.chk643, i64 %2793, i64 0
  %2795 = trunc i64 %2794 to i32
  %2796 = lshr i64 %2787, %2771
  %shft.chk644 = icmp ult i64 %2771, 64
  %2797 = select i1 %shft.chk644, i64 %2796, i64 0
  %2798 = trunc i64 %2797 to i32
  %2799 = lshr i64 %2767, %2771
  %shft.chk645 = icmp ult i64 %2771, 64
  %2800 = select i1 %shft.chk645, i64 %2799, i64 0
  %2801 = trunc i64 %2800 to i32
  %2802 = xor i32 %2798, %2801
  %region_0_250_constant_42646 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2803 = xor i32 %2802, %region_0_250_constant_42646
  %2804 = zext i32 %2803 to i64
  %2805 = mul i64 %2804, %region_0_250_constant_46636
  %2806 = trunc i64 %2805 to i32
  %2807 = xor i32 %2795, %2806
  %region_0_250_constant_104647 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2808 = xor i32 %2807, %region_0_250_constant_104647
  %2809 = zext i32 %2808 to i64
  %2810 = mul i64 %2809, %region_0_250_constant_18640
  %2811 = lshr i64 %2810, %2771
  %shft.chk648 = icmp ult i64 %2771, 64
  %2812 = select i1 %shft.chk648, i64 %2811, i64 0
  %2813 = trunc i64 %2812 to i32
  %2814 = lshr i64 %2805, %2771
  %shft.chk649 = icmp ult i64 %2771, 64
  %2815 = select i1 %shft.chk649, i64 %2814, i64 0
  %2816 = trunc i64 %2815 to i32
  %2817 = trunc i64 %2770 to i32
  %2818 = xor i32 %2816, %2817
  %region_0_250_constant_56650 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %2819 = xor i32 %2818, %region_0_250_constant_56650
  %2820 = zext i32 %2819 to i64
  %2821 = mul i64 %2820, %region_0_250_constant_18640
  %2822 = trunc i64 %2821 to i32
  %2823 = xor i32 %2813, %2822
  %region_0_250_constant_122651 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %2824 = xor i32 %2823, %region_0_250_constant_122651
  %2825 = zext i32 %2824 to i64
  %2826 = mul i64 %2825, %region_0_250_constant_46636
  %2827 = lshr i64 %2826, %2771
  %shft.chk652 = icmp ult i64 %2771, 64
  %2828 = select i1 %shft.chk652, i64 %2827, i64 0
  %2829 = trunc i64 %2828 to i32
  %2830 = lshr i64 %2821, %2771
  %shft.chk653 = icmp ult i64 %2771, 64
  %2831 = select i1 %shft.chk653, i64 %2830, i64 0
  %2832 = trunc i64 %2831 to i32
  %2833 = trunc i64 %2781 to i32
  %2834 = xor i32 %2832, %2833
  %region_0_250_constant_75654 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2835 = xor i32 %2834, %region_0_250_constant_75654
  %2836 = zext i32 %2835 to i64
  %2837 = mul i64 %2836, %region_0_250_constant_46636
  %2838 = trunc i64 %2837 to i32
  %2839 = xor i32 %2829, %2838
  %region_0_250_constant_140655 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2840 = xor i32 %2839, %region_0_250_constant_140655
  %2841 = zext i32 %2840 to i64
  %2842 = mul i64 %2841, %region_0_250_constant_18640
  %2843 = lshr i64 %2842, %2771
  %shft.chk656 = icmp ult i64 %2771, 64
  %2844 = select i1 %shft.chk656, i64 %2843, i64 0
  %2845 = trunc i64 %2844 to i32
  %2846 = lshr i64 %2837, %2771
  %shft.chk657 = icmp ult i64 %2771, 64
  %2847 = select i1 %shft.chk657, i64 %2846, i64 0
  %2848 = trunc i64 %2847 to i32
  %2849 = trunc i64 %2792 to i32
  %2850 = xor i32 %2848, %2849
  %region_0_250_constant_93658 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %2851 = xor i32 %2850, %region_0_250_constant_93658
  %2852 = zext i32 %2851 to i64
  %2853 = mul i64 %2852, %region_0_250_constant_18640
  %2854 = trunc i64 %2853 to i32
  %2855 = xor i32 %2845, %2854
  %region_0_250_constant_158659 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %2856 = xor i32 %2855, %region_0_250_constant_158659
  %2857 = zext i32 %2856 to i64
  %2858 = mul i64 %2857, %region_0_250_constant_46636
  %2859 = lshr i64 %2858, %2771
  %shft.chk660 = icmp ult i64 %2771, 64
  %2860 = select i1 %shft.chk660, i64 %2859, i64 0
  %2861 = trunc i64 %2860 to i32
  %2862 = lshr i64 %2853, %2771
  %shft.chk661 = icmp ult i64 %2771, 64
  %2863 = select i1 %shft.chk661, i64 %2862, i64 0
  %2864 = trunc i64 %2863 to i32
  %2865 = trunc i64 %2810 to i32
  %2866 = xor i32 %2864, %2865
  %region_0_250_constant_111662 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %2867 = xor i32 %2866, %region_0_250_constant_111662
  %2868 = zext i32 %2867 to i64
  %2869 = mul i64 %2868, %region_0_250_constant_46636
  %2870 = trunc i64 %2869 to i32
  %2871 = xor i32 %2861, %2870
  %region_0_250_constant_176663 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %2872 = xor i32 %2871, %region_0_250_constant_176663
  %2873 = zext i32 %2872 to i64
  %2874 = mul i64 %2873, %region_0_250_constant_18640
  %2875 = lshr i64 %2874, %2771
  %shft.chk664 = icmp ult i64 %2771, 64
  %2876 = select i1 %shft.chk664, i64 %2875, i64 0
  %2877 = trunc i64 %2876 to i32
  %2878 = lshr i64 %2869, %2771
  %shft.chk665 = icmp ult i64 %2771, 64
  %2879 = select i1 %shft.chk665, i64 %2878, i64 0
  %2880 = trunc i64 %2879 to i32
  %2881 = trunc i64 %2826 to i32
  %2882 = xor i32 %2880, %2881
  %region_0_250_constant_129666 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %2883 = xor i32 %2882, %region_0_250_constant_129666
  %2884 = zext i32 %2883 to i64
  %2885 = mul i64 %2884, %region_0_250_constant_18640
  %2886 = trunc i64 %2885 to i32
  %2887 = xor i32 %2877, %2886
  %region_0_250_constant_194667 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %2888 = xor i32 %2887, %region_0_250_constant_194667
  %2889 = zext i32 %2888 to i64
  %2890 = mul i64 %2889, %region_0_250_constant_46636
  %2891 = lshr i64 %2890, %2771
  %shft.chk668 = icmp ult i64 %2771, 64
  %2892 = select i1 %shft.chk668, i64 %2891, i64 0
  %2893 = trunc i64 %2892 to i32
  %2894 = lshr i64 %2885, %2771
  %shft.chk669 = icmp ult i64 %2771, 64
  %2895 = select i1 %shft.chk669, i64 %2894, i64 0
  %2896 = trunc i64 %2895 to i32
  %2897 = trunc i64 %2842 to i32
  %2898 = xor i32 %2896, %2897
  %region_0_250_constant_147670 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2899 = xor i32 %2898, %region_0_250_constant_147670
  %2900 = zext i32 %2899 to i64
  %2901 = mul i64 %2900, %region_0_250_constant_46636
  %2902 = trunc i64 %2901 to i32
  %2903 = xor i32 %2893, %2902
  %region_0_250_constant_211671 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2904 = xor i32 %2903, %region_0_250_constant_211671
  %2905 = zext i32 %2904 to i64
  %2906 = mul i64 %2905, %region_0_250_constant_18640
  %2907 = lshr i64 %2906, %2771
  %shft.chk672 = icmp ult i64 %2771, 64
  %2908 = select i1 %shft.chk672, i64 %2907, i64 0
  %2909 = trunc i64 %2908 to i32
  %2910 = lshr i64 %2901, %2771
  %shft.chk673 = icmp ult i64 %2771, 64
  %2911 = select i1 %shft.chk673, i64 %2910, i64 0
  %2912 = trunc i64 %2911 to i32
  %2913 = trunc i64 %2858 to i32
  %2914 = xor i32 %2912, %2913
  %region_0_250_constant_165674 = load i32, i32* bitcast ([4 x i8]* @54 to i32*), align 4
  %2915 = xor i32 %2914, %region_0_250_constant_165674
  %2916 = zext i32 %2915 to i64
  %2917 = mul i64 %2916, %region_0_250_constant_18640
  %2918 = trunc i64 %2917 to i32
  %2919 = xor i32 %2909, %2918
  %region_0_250_constant_220675 = load i32, i32* bitcast ([4 x i8]* @53 to i32*), align 4
  %2920 = xor i32 %2919, %region_0_250_constant_220675
  br label %concatenate.226.merge541

concat_index_from_operand_id3676:                 ; preds = %concatenate.pivot.3.724
  %2921 = phi i32 [ 3, %concatenate.pivot.3.724 ]
  %2922 = sub nsw i32 %2361, %2921
  %2923 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 0
  %2924 = load i64, i64* %2923, align 8, !invariant.load !22
  %2925 = trunc i64 %2924 to i32
  %2926 = zext i32 %2925 to i64
  %2927 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2928 = lshr i64 %2924, %2927
  %shft.chk677 = icmp ult i64 %2927, 64
  %2929 = select i1 %shft.chk677, i64 %2928, i64 0
  %2930 = trunc i64 %2929 to i32
  %2931 = zext i32 %2930 to i64
  %2932 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2933 = shl i64 %2931, %2932
  %shft.chk678 = icmp ult i64 %2932, 64
  %2934 = select i1 %shft.chk678, i64 %2933, i64 0
  %2935 = or i64 %2926, %2934
  %2936 = mul nuw nsw i32 %2362, 1
  %2937 = add nuw nsw i32 0, %2936
  %2938 = zext i32 %2937 to i64
  %2939 = add i64 %2935, %2938
  %2940 = icmp ult i64 %2939, %2935
  %2941 = zext i1 %2940 to i8
  %2942 = getelementptr inbounds [2 x i64], [2 x i64]* %3, i32 0, i32 1
  %2943 = load i64, i64* %2942, align 8, !invariant.load !22
  %2944 = trunc i64 %2943 to i32
  %2945 = zext i32 %2944 to i64
  %2946 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2947 = lshr i64 %2943, %2946
  %shft.chk679 = icmp ult i64 %2946, 64
  %2948 = select i1 %shft.chk679, i64 %2947, i64 0
  %2949 = trunc i64 %2948 to i32
  %2950 = zext i32 %2949 to i64
  %2951 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2952 = shl i64 %2950, %2951
  %shft.chk680 = icmp ult i64 %2951, 64
  %2953 = select i1 %shft.chk680, i64 %2952, i64 0
  %2954 = or i64 %2945, %2953
  %region_0_250_constant_34681 = load i64, i64* bitcast ([8 x i8]* @41 to i64*), align 8
  %2955 = add i64 %2954, %region_0_250_constant_34681
  %2956 = trunc i8 %2941 to i1
  %2957 = select i1 %2956, i64 %2955, i64 %2954
  %2958 = trunc i64 %2957 to i32
  %2959 = zext i32 %2958 to i64
  %region_0_250_constant_46682 = load i64, i64* bitcast ([8 x i8]* @36 to i64*), align 8
  %2960 = mul i64 %2959, %region_0_250_constant_46682
  %2961 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_3 to i64*), align 8, !invariant.load !22
  %2962 = lshr i64 %2960, %2961
  %shft.chk683 = icmp ult i64 %2961, 64
  %2963 = select i1 %shft.chk683, i64 %2962, i64 0
  %2964 = trunc i64 %2963 to i32
  %2965 = lshr i64 %2939, %2961
  %shft.chk684 = icmp ult i64 %2961, 64
  %2966 = select i1 %shft.chk684, i64 %2965, i64 0
  %2967 = trunc i64 %2966 to i32
  %2968 = xor i32 %2964, %2967
  %region_0_250_constant_68685 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2969 = xor i32 %2968, %region_0_250_constant_68685
  %2970 = zext i32 %2969 to i64
  %region_0_250_constant_18686 = load i64, i64* bitcast ([8 x i8]* @34 to i64*), align 8
  %2971 = mul i64 %2970, %region_0_250_constant_18686
  %2972 = lshr i64 %2971, %2961
  %shft.chk687 = icmp ult i64 %2961, 64
  %2973 = select i1 %shft.chk687, i64 %2972, i64 0
  %2974 = trunc i64 %2973 to i32
  %2975 = trunc i64 %2939 to i32
  %2976 = zext i32 %2975 to i64
  %2977 = mul i64 %2976, %region_0_250_constant_18686
  %2978 = trunc i64 %2977 to i32
  %2979 = xor i32 %2974, %2978
  %region_0_250_constant_86688 = load i32, i32* bitcast ([4 x i8]* @46 to i32*), align 4
  %2980 = xor i32 %2979, %region_0_250_constant_86688
  %2981 = zext i32 %2980 to i64
  %2982 = mul i64 %2981, %region_0_250_constant_46682
  %2983 = lshr i64 %2982, %2961
  %shft.chk689 = icmp ult i64 %2961, 64
  %2984 = select i1 %shft.chk689, i64 %2983, i64 0
  %2985 = trunc i64 %2984 to i32
  %2986 = lshr i64 %2977, %2961
  %shft.chk690 = icmp ult i64 %2961, 64
  %2987 = select i1 %shft.chk690, i64 %2986, i64 0
  %2988 = trunc i64 %2987 to i32
  %2989 = lshr i64 %2957, %2961
  %shft.chk691 = icmp ult i64 %2961, 64
  %2990 = select i1 %shft.chk691, i64 %2989, i64 0
  %2991 = trunc i64 %2990 to i32
  %2992 = xor i32 %2988, %2991
  %region_0_250_constant_42692 = load i32, i32* bitcast ([4 x i8]* @43 to i32*), align 4
  %2993 = xor i32 %2992, %region_0_250_constant_42692
  %2994 = zext i32 %2993 to i64
  %2995 = mul i64 %2994, %region_0_250_constant_46682
  %2996 = trunc i64 %2995 to i32
  %2997 = xor i32 %2985, %2996
  %region_0_250_constant_104693 = load i32, i32* bitcast ([4 x i8]* @45 to i32*), align 4
  %2998 = xor i32 %2997, %region_0_250_constant_104693
  %2999 = zext i32 %2998 to i64
  %3000 = mul i64 %2999, %region_0_250_constant_18686
  %3001 = lshr i64 %3000, %2961
  %shft.chk694 = icmp ult i64 %2961, 64
  %3002 = select i1 %shft.chk694, i64 %3001, i64 0
  %3003 = trunc i64 %3002 to i32
  %3004 = lshr i64 %2995, %2961
  %shft.chk695 = icmp ult i64 %2961, 64
  %3005 = select i1 %shft.chk695, i64 %3004, i64 0
  %3006 = trunc i64 %3005 to i32
  %3007 = trunc i64 %2960 to i32
  %3008 = xor i32 %3006, %3007
  %region_0_250_constant_56696 = load i32, i32* bitcast ([4 x i8]* @42 to i32*), align 4
  %3009 = xor i32 %3008, %region_0_250_constant_56696
  %3010 = zext i32 %3009 to i64
  %3011 = mul i64 %3010, %region_0_250_constant_18686
  %3012 = trunc i64 %3011 to i32
  %3013 = xor i32 %3003, %3012
  %region_0_250_constant_122697 = load i32, i32* bitcast ([4 x i8]* @44 to i32*), align 4
  %3014 = xor i32 %3013, %region_0_250_constant_122697
  %3015 = zext i32 %3014 to i64
  %3016 = mul i64 %3015, %region_0_250_constant_46682
  %3017 = lshr i64 %3016, %2961
  %shft.chk698 = icmp ult i64 %2961, 64
  %3018 = select i1 %shft.chk698, i64 %3017, i64 0
  %3019 = trunc i64 %3018 to i32
  %3020 = lshr i64 %3011, %2961
  %shft.chk699 = icmp ult i64 %2961, 64
  %3021 = select i1 %shft.chk699, i64 %3020, i64 0
  %3022 = trunc i64 %3021 to i32
  %3023 = trunc i64 %2971 to i32
  %3024 = xor i32 %3022, %3023
  %region_0_250_constant_75700 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %3025 = xor i32 %3024, %region_0_250_constant_75700
  %3026 = zext i32 %3025 to i64
  %3027 = mul i64 %3026, %region_0_250_constant_46682
  %3028 = trunc i64 %3027 to i32
  %3029 = xor i32 %3019, %3028
  %region_0_250_constant_140701 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %3030 = xor i32 %3029, %region_0_250_constant_140701
  %3031 = zext i32 %3030 to i64
  %3032 = mul i64 %3031, %region_0_250_constant_18686
  %3033 = lshr i64 %3032, %2961
  %shft.chk702 = icmp ult i64 %2961, 64
  %3034 = select i1 %shft.chk702, i64 %3033, i64 0
  %3035 = trunc i64 %3034 to i32
  %3036 = lshr i64 %3027, %2961
  %shft.chk703 = icmp ult i64 %2961, 64
  %3037 = select i1 %shft.chk703, i64 %3036, i64 0
  %3038 = trunc i64 %3037 to i32
  %3039 = trunc i64 %2982 to i32
  %3040 = xor i32 %3038, %3039
  %region_0_250_constant_93704 = load i32, i32* bitcast ([4 x i8]* @49 to i32*), align 4
  %3041 = xor i32 %3040, %region_0_250_constant_93704
  %3042 = zext i32 %3041 to i64
  %3043 = mul i64 %3042, %region_0_250_constant_18686
  %3044 = trunc i64 %3043 to i32
  %3045 = xor i32 %3035, %3044
  %region_0_250_constant_158705 = load i32, i32* bitcast ([4 x i8]* @52 to i32*), align 4
  %3046 = xor i32 %3045, %region_0_250_constant_158705
  %3047 = zext i32 %3046 to i64
  %3048 = mul i64 %3047, %region_0_250_constant_46682
  %3049 = lshr i64 %3048, %2961
  %shft.chk706 = icmp ult i64 %2961, 64
  %3050 = select i1 %shft.chk706, i64 %3049, i64 0
  %3051 = trunc i64 %3050 to i32
  %3052 = lshr i64 %3043, %2961
  %shft.chk707 = icmp ult i64 %2961, 64
  %3053 = select i1 %shft.chk707, i64 %3052, i64 0
  %3054 = trunc i64 %3053 to i32
  %3055 = trunc i64 %3000 to i32
  %3056 = xor i32 %3054, %3055
  %region_0_250_constant_111708 = load i32, i32* bitcast ([4 x i8]* @48 to i32*), align 4
  %3057 = xor i32 %3056, %region_0_250_constant_111708
  %3058 = zext i32 %3057 to i64
  %3059 = mul i64 %3058, %region_0_250_constant_46682
  %3060 = trunc i64 %3059 to i32
  %3061 = xor i32 %3051, %3060
  %region_0_250_constant_176709 = load i32, i32* bitcast ([4 x i8]* @51 to i32*), align 4
  %3062 = xor i32 %3061, %region_0_250_constant_176709
  %3063 = zext i32 %3062 to i64
  %3064 = mul i64 %3063, %region_0_250_constant_18686
  %3065 = lshr i64 %3064, %2961
  %shft.chk710 = icmp ult i64 %2961, 64
  %3066 = select i1 %shft.chk710, i64 %3065, i64 0
  %3067 = trunc i64 %3066 to i32
  %3068 = lshr i64 %3059, %2961
  %shft.chk711 = icmp ult i64 %2961, 64
  %3069 = select i1 %shft.chk711, i64 %3068, i64 0
  %3070 = trunc i64 %3069 to i32
  %3071 = trunc i64 %3016 to i32
  %3072 = xor i32 %3070, %3071
  %region_0_250_constant_129712 = load i32, i32* bitcast ([4 x i8]* @47 to i32*), align 4
  %3073 = xor i32 %3072, %region_0_250_constant_129712
  %3074 = zext i32 %3073 to i64
  %3075 = mul i64 %3074, %region_0_250_constant_18686
  %3076 = trunc i64 %3075 to i32
  %3077 = xor i32 %3067, %3076
  %region_0_250_constant_194713 = load i32, i32* bitcast ([4 x i8]* @50 to i32*), align 4
  %3078 = xor i32 %3077, %region_0_250_constant_194713
  %3079 = zext i32 %3078 to i64
  %3080 = mul i64 %3079, %region_0_250_constant_46682
  %3081 = lshr i64 %3080, %2961
  %shft.chk714 = icmp ult i64 %2961, 64
  %3082 = select i1 %shft.chk714, i64 %3081, i64 0
  %3083 = trunc i64 %3082 to i32
  %3084 = lshr i64 %3075, %2961
  %shft.chk715 = icmp ult i64 %2961, 64
  %3085 = select i1 %shft.chk715, i64 %3084, i64 0
  %3086 = trunc i64 %3085 to i32
  %3087 = trunc i64 %3032 to i32
  %3088 = xor i32 %3086, %3087
  %region_0_250_constant_147716 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %3089 = xor i32 %3088, %region_0_250_constant_147716
  %3090 = zext i32 %3089 to i64
  %3091 = mul i64 %3090, %region_0_250_constant_46682
  %3092 = trunc i64 %3091 to i32
  %3093 = xor i32 %3083, %3092
  %region_0_250_constant_211717 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %3094 = xor i32 %3093, %region_0_250_constant_211717
  %3095 = zext i32 %3094 to i64
  %3096 = mul i64 %3095, %region_0_250_constant_18686
  %3097 = trunc i64 %3096 to i32
  br label %concatenate.226.merge541

concatenate.pivot.2.718:                          ; preds = %concatenate.226.merge347
  %3098 = icmp ult i32 %2361, 2
  br i1 %3098, label %concatenate.pivot.1.719, label %concatenate.pivot.3.722

concatenate.pivot.1.719:                          ; preds = %concatenate.pivot.2.718
  %3099 = icmp ult i32 %2361, 1
  br i1 %3099, label %concatenate.pivot.0.720, label %concatenate.pivot.1.721

concatenate.pivot.0.720:                          ; preds = %concatenate.pivot.1.719
  br label %concat_index_from_operand_id0542

concatenate.pivot.1.721:                          ; preds = %concatenate.pivot.1.719
  br label %concat_index_from_operand_id1588

concatenate.pivot.3.722:                          ; preds = %concatenate.pivot.2.718
  %3100 = icmp ult i32 %2361, 3
  br i1 %3100, label %concatenate.pivot.2.723, label %concatenate.pivot.3.724

concatenate.pivot.2.723:                          ; preds = %concatenate.pivot.3.722
  br label %concat_index_from_operand_id2630

concatenate.pivot.3.724:                          ; preds = %concatenate.pivot.3.722
  br label %concat_index_from_operand_id3676

concatenate.226.merge541:                         ; preds = %concat_index_from_operand_id3676, %concat_index_from_operand_id2630, %concat_index_from_operand_id1588, %concat_index_from_operand_id0542
  %3101 = phi i32 [ %2553, %concat_index_from_operand_id0542 ], [ %2730, %concat_index_from_operand_id1588 ], [ %2920, %concat_index_from_operand_id2630 ], [ %3097, %concat_index_from_operand_id3676 ]
  %region_0_250_constant_227725 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %3102 = lshr i32 %3101, %region_0_250_constant_227725
  %shft.chk726 = icmp ult i32 %region_0_250_constant_227725, 32
  %3103 = select i1 %shft.chk726, i32 %3102, i32 0
  %3104 = uitofp i32 %3103 to float
  %region_0_250_constant_231727 = load float, float* bitcast ([4 x i8]* @32 to float*), align 4
  %multiply.233728 = fmul float %3104, %region_0_250_constant_231727
  %region_0_250_constant_234729 = load float, float* bitcast ([4 x i8]* @31 to float*), align 4
  %compare.236730 = fcmp olt float %multiply.233728, %region_0_250_constant_234729
  %3105 = zext i1 %compare.236730 to i8
  %region_0_250_constant_238731 = load i8, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @30, i32 0, i32 0), align 1
  %3106 = icmp eq i8 %3105, %region_0_250_constant_238731
  %3107 = zext i1 %3106 to i8
  %3108 = mul nuw nsw i32 %2353, 1
  %3109 = add nuw nsw i32 0, %3108
  %3110 = udiv i32 %3109, 16
  %3111 = mul nuw nsw i32 %2349, 1
  %3112 = add nuw nsw i32 0, %3111
  %3113 = mul nuw nsw i32 %2350, 32
  %3114 = add nuw nsw i32 %3112, %3113
  %3115 = udiv i32 %3114, 256
  %3116 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %1, i32 0, i32 %3114, i32 %3109
  %3117 = load float, float* %3116, align 4, !invariant.load !22
  %region_0_250_constant_242732 = load float, float* bitcast ([4 x i8]* @29 to float*), align 4
  %3118 = trunc i8 %3107 to i1
  %3119 = select i1 %3118, float %3117, float %region_0_250_constant_242732
  %region_0_250_constant_245733 = load float, float* bitcast ([4 x i8]* @28 to float*), align 4
  %multiply.247734 = fmul float %3119, %region_0_250_constant_245733
  %3120 = getelementptr inbounds [256 x [16 x float]], [256 x [16 x float]]* %5, i32 0, i32 %21, i32 %23
  store float %multiply.247734, float* %3120, align 4
  br label %after-fusion-body
}

define void @concatenate_8(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 16384
  %1 = bitcast i8* %0 to [256 x float]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 49152
  %3 = bitcast i8* %2 to [256 x float]*
  %4 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %5 = bitcast i8* %4 to [512 x float]*
  %6 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !23
  %7 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !24
  %8 = mul nuw nsw i32 %6, 128
  %linear_index = add nuw nsw i32 %8, %7
  %linear_index_in_range = icmp ult i32 %linear_index, 128
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %9 = udiv i32 %linear_index_base, 1
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %10 = udiv i32 %linear_index1, 1
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %11 = udiv i32 %linear_index2, 1
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %12 = udiv i32 %linear_index3, 1
  %13 = icmp ult i32 %linear_index_base, 512
  br i1 %13, label %concatenate_8.in_bounds-true, label %concatenate_8.in_bounds-after

concatenate_8.in_bounds-after:                    ; preds = %concatenate.3.merge14, %entry
  ret void

concatenate_8.in_bounds-true:                     ; preds = %entry
  br label %concatenate.pivot.256.

concat_index_from_operand_id0:                    ; preds = %concatenate.pivot.0.
  %14 = phi i32 [ 0, %concatenate.pivot.0. ]
  %15 = sub nsw i32 %9, %14
  %16 = getelementptr inbounds [256 x float], [256 x float]* %1, i32 0, i32 %15
  %17 = load float, float* %16, align 4, !invariant.load !22
  br label %concatenate.3.merge

concat_index_from_operand_id1:                    ; preds = %concatenate.pivot.256.1
  %18 = phi i32 [ 256, %concatenate.pivot.256.1 ]
  %19 = sub nsw i32 %9, %18
  %20 = getelementptr inbounds [256 x float], [256 x float]* %3, i32 0, i32 %19
  %21 = load float, float* %20, align 4, !invariant.load !22
  br label %concatenate.3.merge

concatenate.pivot.256.:                           ; preds = %concatenate_8.in_bounds-true
  %22 = icmp ult i32 %9, 256
  br i1 %22, label %concatenate.pivot.0., label %concatenate.pivot.256.1

concatenate.pivot.0.:                             ; preds = %concatenate.pivot.256.
  br label %concat_index_from_operand_id0

concatenate.pivot.256.1:                          ; preds = %concatenate.pivot.256.
  br label %concat_index_from_operand_id1

concatenate.3.merge:                              ; preds = %concat_index_from_operand_id1, %concat_index_from_operand_id0
  %23 = phi float [ %17, %concat_index_from_operand_id0 ], [ %21, %concat_index_from_operand_id1 ]
  %24 = bitcast [512 x float]* %5 to float*
  %25 = getelementptr inbounds float, float* %24, i32 %linear_index_base
  store float %23, float* %25, align 4
  br label %concatenate.pivot.256.5

concat_index_from_operand_id03:                   ; preds = %concatenate.pivot.0.6
  %26 = phi i32 [ 0, %concatenate.pivot.0.6 ]
  %27 = sub nsw i32 %10, %26
  %28 = getelementptr inbounds [256 x float], [256 x float]* %1, i32 0, i32 %27
  %29 = load float, float* %28, align 4, !invariant.load !22
  br label %concatenate.3.merge2

concat_index_from_operand_id14:                   ; preds = %concatenate.pivot.256.7
  %30 = phi i32 [ 256, %concatenate.pivot.256.7 ]
  %31 = sub nsw i32 %10, %30
  %32 = getelementptr inbounds [256 x float], [256 x float]* %3, i32 0, i32 %31
  %33 = load float, float* %32, align 4, !invariant.load !22
  br label %concatenate.3.merge2

concatenate.pivot.256.5:                          ; preds = %concatenate.3.merge
  %34 = icmp ult i32 %10, 256
  br i1 %34, label %concatenate.pivot.0.6, label %concatenate.pivot.256.7

concatenate.pivot.0.6:                            ; preds = %concatenate.pivot.256.5
  br label %concat_index_from_operand_id03

concatenate.pivot.256.7:                          ; preds = %concatenate.pivot.256.5
  br label %concat_index_from_operand_id14

concatenate.3.merge2:                             ; preds = %concat_index_from_operand_id14, %concat_index_from_operand_id03
  %35 = phi float [ %29, %concat_index_from_operand_id03 ], [ %33, %concat_index_from_operand_id14 ]
  %36 = bitcast [512 x float]* %5 to float*
  %37 = getelementptr inbounds float, float* %36, i32 %linear_index1
  store float %35, float* %37, align 4
  br label %concatenate.pivot.256.11

concat_index_from_operand_id09:                   ; preds = %concatenate.pivot.0.12
  %38 = phi i32 [ 0, %concatenate.pivot.0.12 ]
  %39 = sub nsw i32 %11, %38
  %40 = getelementptr inbounds [256 x float], [256 x float]* %1, i32 0, i32 %39
  %41 = load float, float* %40, align 4, !invariant.load !22
  br label %concatenate.3.merge8

concat_index_from_operand_id110:                  ; preds = %concatenate.pivot.256.13
  %42 = phi i32 [ 256, %concatenate.pivot.256.13 ]
  %43 = sub nsw i32 %11, %42
  %44 = getelementptr inbounds [256 x float], [256 x float]* %3, i32 0, i32 %43
  %45 = load float, float* %44, align 4, !invariant.load !22
  br label %concatenate.3.merge8

concatenate.pivot.256.11:                         ; preds = %concatenate.3.merge2
  %46 = icmp ult i32 %11, 256
  br i1 %46, label %concatenate.pivot.0.12, label %concatenate.pivot.256.13

concatenate.pivot.0.12:                           ; preds = %concatenate.pivot.256.11
  br label %concat_index_from_operand_id09

concatenate.pivot.256.13:                         ; preds = %concatenate.pivot.256.11
  br label %concat_index_from_operand_id110

concatenate.3.merge8:                             ; preds = %concat_index_from_operand_id110, %concat_index_from_operand_id09
  %47 = phi float [ %41, %concat_index_from_operand_id09 ], [ %45, %concat_index_from_operand_id110 ]
  %48 = bitcast [512 x float]* %5 to float*
  %49 = getelementptr inbounds float, float* %48, i32 %linear_index2
  store float %47, float* %49, align 4
  br label %concatenate.pivot.256.17

concat_index_from_operand_id015:                  ; preds = %concatenate.pivot.0.18
  %50 = phi i32 [ 0, %concatenate.pivot.0.18 ]
  %51 = sub nsw i32 %12, %50
  %52 = getelementptr inbounds [256 x float], [256 x float]* %1, i32 0, i32 %51
  %53 = load float, float* %52, align 4, !invariant.load !22
  br label %concatenate.3.merge14

concat_index_from_operand_id116:                  ; preds = %concatenate.pivot.256.19
  %54 = phi i32 [ 256, %concatenate.pivot.256.19 ]
  %55 = sub nsw i32 %12, %54
  %56 = getelementptr inbounds [256 x float], [256 x float]* %3, i32 0, i32 %55
  %57 = load float, float* %56, align 4, !invariant.load !22
  br label %concatenate.3.merge14

concatenate.pivot.256.17:                         ; preds = %concatenate.3.merge8
  %58 = icmp ult i32 %12, 256
  br i1 %58, label %concatenate.pivot.0.18, label %concatenate.pivot.256.19

concatenate.pivot.0.18:                           ; preds = %concatenate.pivot.256.17
  br label %concat_index_from_operand_id015

concatenate.pivot.256.19:                         ; preds = %concatenate.pivot.256.17
  br label %concat_index_from_operand_id116

concatenate.3.merge14:                            ; preds = %concat_index_from_operand_id116, %concat_index_from_operand_id015
  %59 = phi float [ %53, %concat_index_from_operand_id015 ], [ %57, %concat_index_from_operand_id116 ]
  %60 = bitcast [512 x float]* %5 to float*
  %61 = getelementptr inbounds float, float* %60, i32 %linear_index3
  store float %59, float* %61, align 4
  br label %concatenate_8.in_bounds-after
}

define void @slice_17(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [512 x float]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 2048
  %3 = bitcast i8* %2 to [256 x float]*
  %4 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !23
  %5 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !25
  %6 = mul nuw nsw i32 %4, 64
  %linear_index = add nuw nsw i32 %6, %5
  %linear_index_in_range = icmp ult i32 %linear_index, 64
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %7 = udiv i32 %linear_index_base, 1
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %8 = udiv i32 %linear_index1, 1
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %9 = udiv i32 %linear_index2, 1
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %10 = udiv i32 %linear_index3, 1
  %11 = icmp ult i32 %linear_index_base, 256
  br i1 %11, label %slice_17.in_bounds-true, label %slice_17.in_bounds-after

slice_17.in_bounds-after:                         ; preds = %slice_17.in_bounds-true, %entry
  ret void

slice_17.in_bounds-true:                          ; preds = %entry
  %12 = add i32 %7, 0
  %13 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %12
  %14 = load float, float* %13, align 4, !invariant.load !22
  %15 = bitcast [256 x float]* %3 to float*
  %16 = getelementptr inbounds float, float* %15, i32 %linear_index_base
  store float %14, float* %16, align 4
  %17 = add i32 %8, 0
  %18 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %17
  %19 = load float, float* %18, align 4, !invariant.load !22
  %20 = bitcast [256 x float]* %3 to float*
  %21 = getelementptr inbounds float, float* %20, i32 %linear_index1
  store float %19, float* %21, align 4
  %22 = add i32 %9, 0
  %23 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %22
  %24 = load float, float* %23, align 4, !invariant.load !22
  %25 = bitcast [256 x float]* %3 to float*
  %26 = getelementptr inbounds float, float* %25, i32 %linear_index2
  store float %24, float* %26, align 4
  %27 = add i32 %10, 0
  %28 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %27
  %29 = load float, float* %28, align 4, !invariant.load !22
  %30 = bitcast [256 x float]* %3 to float*
  %31 = getelementptr inbounds float, float* %30, i32 %linear_index3
  store float %29, float* %31, align 4
  br label %slice_17.in_bounds-after
}

define void @slice_18(i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [512 x float]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 3072
  %3 = bitcast i8* %2 to [256 x float]*
  %4 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !23
  %5 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !25
  %6 = mul nuw nsw i32 %4, 64
  %linear_index = add nuw nsw i32 %6, %5
  %linear_index_in_range = icmp ult i32 %linear_index, 64
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %7 = udiv i32 %linear_index_base, 1
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %8 = udiv i32 %linear_index1, 1
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %9 = udiv i32 %linear_index2, 1
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %10 = udiv i32 %linear_index3, 1
  %11 = icmp ult i32 %linear_index_base, 256
  br i1 %11, label %slice_18.in_bounds-true, label %slice_18.in_bounds-after

slice_18.in_bounds-after:                         ; preds = %slice_18.in_bounds-true, %entry
  ret void

slice_18.in_bounds-true:                          ; preds = %entry
  %12 = add i32 %7, 256
  %13 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %12
  %14 = load float, float* %13, align 4, !invariant.load !22
  %15 = bitcast [256 x float]* %3 to float*
  %16 = getelementptr inbounds float, float* %15, i32 %linear_index_base
  store float %14, float* %16, align 4
  %17 = add i32 %8, 256
  %18 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %17
  %19 = load float, float* %18, align 4, !invariant.load !22
  %20 = bitcast [256 x float]* %3 to float*
  %21 = getelementptr inbounds float, float* %20, i32 %linear_index1
  store float %19, float* %21, align 4
  %22 = add i32 %9, 256
  %23 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %22
  %24 = load float, float* %23, align 4, !invariant.load !22
  %25 = bitcast [256 x float]* %3 to float*
  %26 = getelementptr inbounds float, float* %25, i32 %linear_index2
  store float %24, float* %26, align 4
  %27 = add i32 %10, 256
  %28 = getelementptr inbounds [512 x float], [512 x float]* %1, i32 0, i32 %27
  %29 = load float, float* %28, align 4, !invariant.load !22
  %30 = bitcast [256 x float]* %3 to float*
  %31 = getelementptr inbounds float, float* %30, i32 %linear_index3
  store float %29, float* %31, align 4
  br label %slice_18.in_bounds-after
}

define void @fusion_7(i8* noalias align 16 dereferenceable(1024) %alloc1, i8* noalias align 16 dereferenceable(1024) %alloc2, i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %alloc1, i64 0
  %1 = bitcast i8* %0 to [16 x [16 x float]]*
  %2 = getelementptr inbounds i8, i8* %temp_buf, i64 2048
  %3 = bitcast i8* %2 to [16 x [16 x float]]*
  %4 = getelementptr inbounds i8, i8* %alloc2, i64 0
  %5 = bitcast i8* %4 to [16 x [16 x float]]*
  %6 = getelementptr inbounds i8, i8* %temp_buf, i64 3072
  %7 = bitcast i8* %6 to [16 x [16 x float]]*
  %8 = getelementptr inbounds i8, i8* %alloc1, i64 0
  %9 = bitcast i8* %8 to [256 x float]*
  %10 = getelementptr inbounds i8, i8* %alloc2, i64 0
  %11 = bitcast i8* %10 to [256 x float]*
  %12 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !23
  %13 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !26
  %14 = mul nuw nsw i32 %12, 512
  %linear_index = add nuw nsw i32 %14, %13
  %linear_index_in_range = icmp ult i32 %linear_index, 512
  call void @llvm.assume(i1 %linear_index_in_range)
  %15 = udiv i32 %linear_index, 1
  %16 = icmp ult i32 %linear_index, 512
  br i1 %16, label %fusion_7.in_bounds-true, label %fusion_7.in_bounds-after

fusion_7.in_bounds-after:                         ; preds = %slice1-after, %entry
  ret void

fusion_7.in_bounds-true:                          ; preds = %entry
  br label %concatenate.pivot.256.

concat_index_from_operand_id0:                    ; preds = %concatenate.pivot.0.
  %17 = phi i32 [ 0, %concatenate.pivot.0. ]
  %18 = sub nsw i32 %15, %17
  %19 = mul nuw nsw i32 %18, 1
  %20 = add nuw nsw i32 0, %19
  %21 = urem i32 %20, 16
  %22 = udiv i32 %20, 16
  %23 = udiv i32 %22, 16
  %24 = getelementptr inbounds [16 x [16 x float]], [16 x [16 x float]]* %1, i32 0, i32 %22, i32 %21
  %25 = load float, float* %24, align 4
  %26 = getelementptr inbounds [16 x [16 x float]], [16 x [16 x float]]* %3, i32 0, i32 %22, i32 %21
  %27 = load float, float* %26, align 4, !invariant.load !22
  %region_0_17_constant_5 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %multiply.7 = fmul float %27, %region_0_17_constant_5
  %subtract.8 = fsub float %25, %multiply.7
  br label %concatenate.13.merge

concat_index_from_operand_id1:                    ; preds = %concatenate.pivot.256.2
  %28 = phi i32 [ 256, %concatenate.pivot.256.2 ]
  %29 = sub nsw i32 %15, %28
  %30 = mul nuw nsw i32 %29, 1
  %31 = add nuw nsw i32 0, %30
  %32 = urem i32 %31, 16
  %33 = udiv i32 %31, 16
  %34 = udiv i32 %33, 16
  %35 = getelementptr inbounds [16 x [16 x float]], [16 x [16 x float]]* %5, i32 0, i32 %33, i32 %32
  %36 = load float, float* %35, align 4
  %37 = getelementptr inbounds [16 x [16 x float]], [16 x [16 x float]]* %7, i32 0, i32 %33, i32 %32
  %38 = load float, float* %37, align 4, !invariant.load !22
  %region_0_17_constant_51 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %multiply.10 = fmul float %38, %region_0_17_constant_51
  %subtract.11 = fsub float %36, %multiply.10
  br label %concatenate.13.merge

concatenate.pivot.256.:                           ; preds = %fusion_7.in_bounds-true
  %39 = icmp ult i32 %15, 256
  br i1 %39, label %concatenate.pivot.0., label %concatenate.pivot.256.2

concatenate.pivot.0.:                             ; preds = %concatenate.pivot.256.
  br label %concat_index_from_operand_id0

concatenate.pivot.256.2:                          ; preds = %concatenate.pivot.256.
  br label %concat_index_from_operand_id1

concatenate.13.merge:                             ; preds = %concat_index_from_operand_id1, %concat_index_from_operand_id0
  %40 = phi float [ %subtract.8, %concat_index_from_operand_id0 ], [ %subtract.11, %concat_index_from_operand_id1 ]
  %41 = icmp sge i32 %15, 0
  %42 = icmp slt i32 %15, 256
  %43 = and i1 %41, %42
  br i1 %43, label %slice0-true, label %slice0-after

slice0-after:                                     ; preds = %slice0-true, %concatenate.13.merge
  %44 = icmp sge i32 %15, 256
  %45 = icmp slt i32 %15, 512
  %46 = and i1 %44, %45
  br i1 %46, label %slice1-true, label %slice1-after

slice1-after:                                     ; preds = %slice1-true, %slice0-after
  br label %fusion_7.in_bounds-after

slice0-true:                                      ; preds = %concatenate.13.merge
  %47 = sub i32 %15, 0
  %48 = getelementptr inbounds [256 x float], [256 x float]* %9, i32 0, i32 %47
  store float %40, float* %48, align 4
  br label %slice0-after

slice1-true:                                      ; preds = %slice0-after
  %49 = sub i32 %15, 256
  %50 = getelementptr inbounds [256 x float], [256 x float]* %11, i32 0, i32 %49
  store float %40, float* %50, align 4
  br label %slice1-after
}

define void @add_2(i8* noalias align 16 dereferenceable(4) %alloc0, i8* noalias align 128 dereferenceable(50320) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %alloc0, i64 0
  %1 = bitcast i8* %0 to i32*
  %2 = getelementptr inbounds i8, i8* %alloc0, i64 0
  %3 = bitcast i8* %2 to i32*
  %4 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !23
  %5 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !23
  %6 = mul nuw nsw i32 %4, 1
  %linear_index = add nuw nsw i32 %6, %5
  %linear_index_in_range = icmp ult i32 %linear_index, 1
  call void @llvm.assume(i1 %linear_index_in_range)
  %7 = icmp ult i32 %linear_index, 1
  br i1 %7, label %add_2.in_bounds-true, label %add_2.in_bounds-after

add_2.in_bounds-after:                            ; preds = %add_2.in_bounds-true, %entry
  ret void

add_2.in_bounds-true:                             ; preds = %entry
  %8 = load i32, i32* %1, align 4
  %9 = load i32, i32* bitcast ([4 x i8]* @buffer_for_constant_5 to i32*), align 4, !invariant.load !22
  %10 = add i32 %8, %9
  store i32 %10, i32* %3, align 4
  br label %add_2.in_bounds-after
}

attributes #0 = { nounwind readnone speculatable }
attributes #1 = { inaccessiblememonly nocallback nofree nosync nounwind willreturn }

!nvvm.annotations = !{!0, !1, !2, !3, !4, !5, !6, !7, !8, !9, !10, !11, !12, !13, !14, !15, !16, !17, !18, !19}

!0 = !{void (i8*)* @rng_get_and_update_state, !"kernel", i32 1}
!1 = !{void (i8*)* @rng_get_and_update_state, !"reqntidx", i32 1}
!2 = !{void (i8*)* @fusion_5, !"kernel", i32 1}
!3 = !{void (i8*)* @fusion_5, !"reqntidx", i32 256}
!4 = !{void (i8*, i8*)* @fusion_4, !"kernel", i32 1}
!5 = !{void (i8*, i8*)* @fusion_4, !"reqntidx", i32 256}
!6 = !{void (i8*)* @fusion_1, !"kernel", i32 1}
!7 = !{void (i8*)* @fusion_1, !"reqntidx", i32 256}
!8 = !{void (i8*)* @fusion_3, !"kernel", i32 1}
!9 = !{void (i8*)* @fusion_3, !"reqntidx", i32 256}
!10 = !{void (i8*)* @concatenate_8, !"kernel", i32 1}
!11 = !{void (i8*)* @concatenate_8, !"reqntidx", i32 128}
!12 = !{void (i8*)* @slice_17, !"kernel", i32 1}
!13 = !{void (i8*)* @slice_17, !"reqntidx", i32 64}
!14 = !{void (i8*)* @slice_18, !"kernel", i32 1}
!15 = !{void (i8*)* @slice_18, !"reqntidx", i32 64}
!16 = !{void (i8*, i8*, i8*)* @fusion_7, !"kernel", i32 1}
!17 = !{void (i8*, i8*, i8*)* @fusion_7, !"reqntidx", i32 512}
!18 = !{void (i8*, i8*)* @add_2, !"kernel", i32 1}
!19 = !{void (i8*, i8*)* @add_2, !"reqntidx", i32 1}
!20 = !{i32 0, i32 4}
!21 = !{i32 0, i32 256}
!22 = !{}
!23 = !{i32 0, i32 1}
!24 = !{i32 0, i32 128}
!25 = !{i32 0, i32 64}
!26 = !{i32 0, i32 512}
