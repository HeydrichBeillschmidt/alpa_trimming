BufferAssignment:
allocation 0: 0x15c1e9f0, size 16384, parameter 3, shape |f32[8,32,16]| at ShapeIndex {}:
 value: <8 param.3 @0> (size=16384,offset=0): f32[8,32,16]{2,1,0}
allocation 1: 0x15c1eaa0, size 16384, parameter 4, shape |f32[8,32,16]| at ShapeIndex {}:
 value: <10 param.5 @0> (size=16384,offset=0): f32[8,32,16]{2,1,0}
allocation 2: 0x15c1eb50, size 1024, parameter 1, shape |f32[16,16]| at ShapeIndex {}, maybe-live-out:
 value: <7 param.2 @0> (size=1024,offset=0): f32[16,16]{1,0}
 value: <23 fusion.7{0} @0> (size=1024,offset=0): f32[256]{0}
allocation 3: 0x15c1ec00, size 1024, parameter 2, shape |f32[16,16]| at ShapeIndex {}, maybe-live-out:
 value: <9 param.4 @0> (size=1024,offset=0): f32[16,16]{1,0}
 value: <24 fusion.7{1} @0> (size=1024,offset=0): f32[256]{0}
allocation 4: 0x15c1ecb0, size 24, output shape is |(s32[], f32[16,16], f32[16,16])|, maybe-live-out:
 value: <29 tuple.2{} @0> (size=24,offset=0): (s32[], f32[16,16]{1,0}, f32[16,16]{1,0})
allocation 5: 0x15c1ed60, size 4, thread-local:
 value: <2 add @0> (size=4,offset=0): f32[]
allocation 6: 0x15c1ee10, size 8, parameter 5, shape |u32[2]| at ShapeIndex {}:
 value: <3 param @0> (size=8,offset=0): u32[2]{0}
allocation 7: 0x15c1eec0, size 8, constant:
 value: <12 constant_3 @0> (size=8,offset=0): u64[]
allocation 8: 0x15c1ef70, size 4, parameter 0, shape |s32[]| at ShapeIndex {}, maybe-live-out:
 value: <4 param.1 @0> (size=4,offset=0): s32[]
 value: <6 add.2 @0> (size=4,offset=0): s32[]
allocation 9: 0x15c1f020, size 4, constant:
 value: <5 constant_5 @0> (size=4,offset=0): s32[]
allocation 10: 0x15c1f0d0, size 4, thread-local:
 value: <1 y @0> (size=4,offset=0): f32[]
allocation 11: 0x15c1f180, size 4, thread-local:
 value: <0 x @0> (size=4,offset=0): f32[]
allocation 12: 0x15c1f230, size 50320, preallocated-temp:
 value: <11 rng-get-and-update-state @0> (size=16,offset=50176): u64[2]{0}
 value: <13 cublas-gemm.1 @0> (size=16384,offset=0): f32[256,16]{1,0}
 value: <14 cublas-gemm.3 @0> (size=16384,offset=32768): f32[256,16]{1,0}
 value: <15 cublas-gemm.5 @0> (size=16384,offset=16384): f32[256,16]{1,0}
 value: <16 cublas-gemm.7 @0> (size=1024,offset=16384): f32[16,16]{1,0}
 value: <17 cublas-gemm.9 @0> (size=1024,offset=49152): f32[16,16]{1,0}
 value: <18 fusion.1 @0> (size=16384,offset=32768): f32[16,256]{0,1}
 value: <19 fusion.3 @0> (size=16384,offset=0): f32[16,256]{0,1}
 value: <20 fusion.4 @0> (size=16384,offset=0): f32[8,32,16]{2,1,0}
 value: <21 fusion.5 @0> (size=16384,offset=16384): f32[256,16]{1,0}
 value: <22 fusion.7{} @0> (size=16,offset=50304): (f32[256]{0}, f32[256]{0})
 value: <25 concatenate.8 @0> (size=2048,offset=0): f32[512]{0}
 value: <26 all-reduce.2 @0> (size=2048,offset=0): f32[512]{0}
 value: <27 slice.17 @0> (size=1024,offset=2048): f32[256]{0}
 value: <28 slice.18 @0> (size=1024,offset=3072): f32[256]{0}

Total bytes used: 85196 (83.2KiB)

Used values:
<0 x @0>
 positions:
  x
 uses:
  add, operand 0
 from instruction:%x = f32[] parameter(0)
<1 y @0>
 positions:
  y
 uses:
  add, operand 1
 from instruction:%y = f32[] parameter(1)
<2 add @0>
 positions:
  add
 uses:
 from instruction:%add = f32[] add(f32[] %x, f32[] %y)
<3 param @0>
 positions:
  param
 uses:
 from instruction:%param = u32[2]{0} parameter(5), sharding={replicated}
<4 param.1 @0>
 positions:
  param.1
 uses:
  add.2, operand 0
 from instruction:%param.1 = s32[] parameter(0), sharding={replicated}
<5 constant_5 @0>
 positions:
  constant_5
 uses:
  add.2, operand 1
 from instruction:%constant_5 = s32[] constant(1), metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
<6 add.2 @0>
 positions:
  add.2
  tuple.2 {0}
 uses:
  tuple.2, operand 0
 from instruction:%add.2 = s32[] add(s32[] %param.1, s32[] %constant_5), metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
<7 param.2 @0>
 positions:
  param.2
 uses:
  fusion.7, operand 0
  cublas-gemm.1, operand 1
 from instruction:%param.2 = f32[16,16]{1,0} parameter(1), sharding={replicated}
<8 param.3 @0>
 positions:
  param.3
  bitcast
 uses:
  bitcast, operand 0
  cublas-gemm.1, operand 0
  cublas-gemm.7, operand 0
 from instruction:%param.3 = f32[8,32,16]{2,1,0} parameter(3), sharding={devices=[4,1,1]0,1,2,3}
<9 param.4 @0>
 positions:
  param.4
 uses:
  fusion.7, operand 2
  cublas-gemm.5, operand 1
  cublas-gemm.3, operand 1
 from instruction:%param.4 = f32[16,16]{1,0} parameter(2), sharding={replicated}
<10 param.5 @0>
 positions:
  param.5
 uses:
  fusion.4, operand 0
 from instruction:%param.5 = f32[8,32,16]{2,1,0} parameter(4), sharding={devices=[4,1,1]0,1,2,3}
<11 rng-get-and-update-state @0>
 positions:
  rng-get-and-update-state
 uses:
  fusion.5, operand 2
  fusion.3, operand 2
 from instruction:%rng-get-and-update-state = u64[2]{0} rng-get-and-update-state(), delta=4096
<12 constant_3 @0>
 positions:
  constant_3
 uses:
  fusion.5, operand 1
  fusion.3, operand 1
 from instruction:%constant_3 = u64[] constant(32)
<13 cublas-gemm.1 @0>
 positions:
  cublas-gemm.1
 uses:
  fusion.5, operand 0
 from instruction:%cublas-gemm.1 = f32[256,16]{1,0} custom-call(f32[256,16]{1,0} %bitcast, f32[16,16]{1,0} %param.2), custom_call_target="__cublas$gemm", metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}, backend_config="{\"alpha_real\":1.1111111640930176,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"256\",\"selected_algorithm\":\"2\"}"
<14 cublas-gemm.3 @0>
 positions:
  cublas-gemm.3
 uses:
  fusion.4, operand 1
 from instruction:%cublas-gemm.3 = f32[256,16]{1,0} custom-call(f32[256,16]{1,0} %fusion.5, f32[16,16]{1,0} %param.4), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"256\",\"selected_algorithm\":\"-1\"}"
<15 cublas-gemm.5 @0>
 positions:
  cublas-gemm.5
 uses:
  fusion.3, operand 0
 from instruction:%cublas-gemm.5 = f32[256,16]{1,0} custom-call(f32[256,16]{1,0} %bitcast.11, f32[16,16]{1,0} %param.4), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"256\",\"selected_algorithm\":\"-1\"}"
<16 cublas-gemm.7 @0>
 positions:
  cublas-gemm.7
  bitcast.58
 uses:
  bitcast.58, operand 0
  concatenate.8, operand 0
 from instruction:%cublas-gemm.7 = f32[16,16]{1,0} custom-call(f32[256,16]{1,0} %bitcast, f32[16,256]{0,1} %fusion.3), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"4096\",\"selected_algorithm\":\"-1\"}"
<17 cublas-gemm.9 @0>
 positions:
  cublas-gemm.9
  bitcast.59
 uses:
  bitcast.59, operand 0
  concatenate.8, operand 1
 from instruction:%cublas-gemm.9 = f32[16,16]{1,0} custom-call(f32[256,16]{1,0} %fusion.5, f32[16,256]{0,1} %fusion.1), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"4096\",\"selected_algorithm\":\"-1\"}"
<18 fusion.1 @0>
 positions:
  fusion.1
 uses:
  cublas-gemm.9, operand 1
 from instruction:%fusion.1 = f32[16,256]{0,1} fusion(f32[8,32,16]{2,1,0} %fusion.4), kind=kLoop, calls=%fused_computation.1
<19 fusion.3 @0>
 positions:
  fusion.3
 uses:
  cublas-gemm.7, operand 1
 from instruction:%fusion.3 = f32[16,256]{0,1} fusion(f32[256,16]{1,0} %cublas-gemm.5, u64[] %constant_3, u64[2]{0} %rng-get-and-update-state), kind=kLoop, calls=%fused_computation.3
<20 fusion.4 @0>
 positions:
  fusion.4
  bitcast.11
 uses:
  bitcast.11, operand 0
  fusion.1, operand 0
  cublas-gemm.5, operand 0
 from instruction:%fusion.4 = f32[8,32,16]{2,1,0} fusion(f32[8,32,16]{2,1,0} %param.5, f32[256,16]{1,0} %cublas-gemm.3), kind=kLoop, calls=%fused_computation.4, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=103}
<21 fusion.5 @0>
 positions:
  fusion.5
 uses:
  cublas-gemm.3, operand 0
  cublas-gemm.9, operand 0
 from instruction:%fusion.5 = f32[256,16]{1,0} fusion(f32[256,16]{1,0} %cublas-gemm.1, u64[] %constant_3, u64[2]{0} %rng-get-and-update-state), kind=kLoop, calls=%fused_computation.5
<22 fusion.7{} @0>
 positions:
  fusion.7 {}
 uses:
  get-tuple-element, operand 0 {}
  get-tuple-element.1, operand 0 {}
 from instruction:%fusion.7 = (f32[256]{0}, f32[256]{0}) fusion(f32[16,16]{1,0} %param.2, f32[16,16]{1,0} %bitcast.60, f32[16,16]{1,0} %param.4, f32[16,16]{1,0} %bitcast.61), kind=kInput, calls=%horizontally_fused_computation
<23 fusion.7{0} @0>
 positions:
  fusion.7 {0}
  get-tuple-element
  bitcast.56
  tuple.2 {1}
 uses:
  bitcast.56, operand 0
  tuple.2, operand 1
 from instruction:%fusion.7 = (f32[256]{0}, f32[256]{0}) fusion(f32[16,16]{1,0} %param.2, f32[16,16]{1,0} %bitcast.60, f32[16,16]{1,0} %param.4, f32[16,16]{1,0} %bitcast.61), kind=kInput, calls=%horizontally_fused_computation
<24 fusion.7{1} @0>
 positions:
  fusion.7 {1}
  get-tuple-element.1
  bitcast.57
  tuple.2 {2}
 uses:
  bitcast.57, operand 0
  tuple.2, operand 2
 from instruction:%fusion.7 = (f32[256]{0}, f32[256]{0}) fusion(f32[16,16]{1,0} %param.2, f32[16,16]{1,0} %bitcast.60, f32[16,16]{1,0} %param.4, f32[16,16]{1,0} %bitcast.61), kind=kInput, calls=%horizontally_fused_computation
<25 concatenate.8 @0>
 positions:
  concatenate.8
 uses:
  all-reduce.2, operand 0
 from instruction:%concatenate.8 = f32[512]{0} concatenate(f32[256]{0} %bitcast.58, f32[256]{0} %bitcast.59), dimensions={0}
<26 all-reduce.2 @0>
 positions:
  all-reduce.2
 uses:
  slice.17, operand 0
  slice.18, operand 0
 from instruction:%all-reduce.2 = f32[512]{0} all-reduce(f32[512]{0} %concatenate.8), channel_id=1, replica_groups={{0}}, to_apply=%add
<27 slice.17 @0>
 positions:
  slice.17
  bitcast.60
 uses:
  bitcast.60, operand 0
  fusion.7, operand 1
 from instruction:%slice.17 = f32[256]{0} slice(f32[512]{0} %all-reduce.2), slice={[0:256]}
<28 slice.18 @0>
 positions:
  slice.18
  bitcast.61
 uses:
  bitcast.61, operand 0
  fusion.7, operand 3
 from instruction:%slice.18 = f32[256]{0} slice(f32[512]{0} %all-reduce.2), slice={[256:512]}
<29 tuple.2{} @0>
 positions:
  tuple.2 {}
 uses:
 from instruction:%tuple.2 = (s32[], f32[16,16]{1,0}, f32[16,16]{1,0}) tuple(s32[] %add.2, f32[16,16]{1,0} %bitcast.56, f32[16,16]{1,0} %bitcast.57)


HloLiveRange (max 35):
  InstructionSequence:
    0:param
    1:param.3
    2:bitcast
    3:param.2
    4:cublas-gemm.1
    5:rng-get-and-update-state
    6:constant_3
    7:fusion.5
    8:param.4
    9:cublas-gemm.3
    10:param.5
    11:fusion.4
    12:fusion.1
    13:cublas-gemm.9
    14:bitcast.59
    15:bitcast.11
    16:cublas-gemm.5
    17:fusion.3
    18:cublas-gemm.7
    19:bitcast.58
    20:concatenate.8
    21:all-reduce.2
    22:slice.17
    23:bitcast.60
    24:slice.18
    25:bitcast.61
    26:fusion.7
    27:get-tuple-element
    28:bitcast.56
    29:get-tuple-element.1
    30:bitcast.57
    31:constant_5
    32:param.1
    33:add.2
    34:tuple.2
  BufferLiveRange:
    param{}:0-35
    param.1{}:0-32
    constant_5{}:31-33
    add.2{}:33-35
    param.2{}:0-25
    param.3{}:0-35
    param.4{}:0-25
    param.5{}:0-35
    rng-get-and-update-state{}:5-17
    constant_3{}:6-17
    cublas-gemm.1{}:4-7
    cublas-gemm.3{}:9-11
    cublas-gemm.5{}:16-17
    cublas-gemm.7{}:18-20
    cublas-gemm.9{}:13-20
    fusion.1{}:12-13
    fusion.3{}:17-18
    fusion.4{}:11-16
    fusion.5{}:7-13
    fusion.7{}:26-29
    fusion.7{0}:26-35
    fusion.7{1}:26-35
    concatenate.8{}:20-21
    all-reduce.2{}:21-24
    slice.17{}:22-26
    slice.18{}:24-26
    tuple.2{}:34-35
  Live ranges at 12 (peak):
    param: 8 bytes
    param.1: 4 bytes
    param.2: 1024 bytes
    param.3: 16384 bytes
    param.4: 1024 bytes
    param.5: 16384 bytes
    rng-get-and-update-state: 16 bytes
    constant_3: 8 bytes
    fusion.1: 16384 bytes
    fusion.4: 16384 bytes
    fusion.5: 16384 bytes
