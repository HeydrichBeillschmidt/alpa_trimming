HloModule func_shard_parallel__6.6

scalar_add_computation {
  scalar_lhs = f32[] parameter(0)
  scalar_rhs = f32[] parameter(1)
  ROOT add = f32[] add(scalar_lhs, scalar_rhs)
}

ENTRY func_shard_parallel__6.6_spmd {
  param = f32[128]{0} parameter(0), sharding={replicated}
  broadcast = f32[64,128]{0,1} broadcast(param), dimensions={1}
  param.1 = f32[128,64]{1,0} parameter(1), sharding={devices=[1,4]0,1,2,3}
  transpose = f32[64,128]{0,1} transpose(param.1), dimensions={1,0}
  multiply = f32[64,128]{0,1} multiply(broadcast, transpose)
  constant = f32[] constant(0)
  reduce = f32[64]{0} reduce(multiply, constant), dimensions={1}, to_apply=scalar_add_computation, metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="test_auto_sharding_basic.py" source_line=201}
  ROOT tuple = (f32[64]{0}) tuple(reduce)
}

