HloModule func_shard_parallel__2.68, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias) }

ENTRY func_shard_parallel__2.68 {
  parameter.6 = u32[2]{0} parameter(5), sharding={replicated}
  parameter.1 = s32[] parameter(0), sharding={replicated}
  constant.65 = s32[] constant(1), sharding={replicated}, metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
  add.66 = s32[] add(parameter.1, constant.65), sharding={replicated}, metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
  parameter.2 = f32[16,16]{1,0} parameter(1), sharding={replicated}
  parameter.4 = f32[32,32,16]{2,1,0} parameter(3), sharding={devices=[4,1,1]0,1,2,3}
  reshape.13 = f32[1024,16]{1,0} reshape(parameter.4), sharding={devices=[4,1]0,1,2,3}
  constant.9 = f32[] constant(0), sharding={replicated}, metadata={op_type="rng_uniform" op_name="parallelize(func_shard_parallel)/rng_uniform[shape=(32, 32, 16)]" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=61}
  constant.10 = f32[] constant(1), sharding={replicated}, metadata={op_type="rng_uniform" op_name="parallelize(func_shard_parallel)/rng_uniform[shape=(32, 32, 16)]" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=61}
  rng.11 = f32[32,32,16] rng(constant.9, constant.10), distribution=rng_uniform, sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="rng_uniform" op_name="parallelize(func_shard_parallel)/rng_uniform[shape=(32, 32, 16)]" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=61}
  constant.12 = f32[] constant(0.9), sharding={replicated}, metadata={op_type="lt" op_name="parallelize(func_shard_parallel)/lt" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=122}
  broadcast.13 = f32[32,32,16]{2,1,0} broadcast(constant.12), dimensions={}, sharding={replicated}, metadata={op_type="lt" op_name="parallelize(func_shard_parallel)/lt" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=122}
  compare.14 = pred[32,32,16]{2,1,0} compare(rng.11, broadcast.13), direction=LT, sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="lt" op_name="parallelize(func_shard_parallel)/lt" source_file="/home/ubuntu/apjax/alpa/alpa/monkey_patch.py" source_line=122}
  constant.48 = pred[] constant(true), sharding={replicated}, metadata={op_type="eq" op_name="parallelize(func_shard_parallel)/eq" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  broadcast.49 = pred[32,32,16]{2,1,0} broadcast(constant.48), dimensions={}, sharding={replicated}, metadata={op_type="eq" op_name="parallelize(func_shard_parallel)/eq" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  compare.50 = pred[32,32,16]{2,1,0} compare(compare.14, broadcast.49), direction=EQ, sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="eq" op_name="parallelize(func_shard_parallel)/eq" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  dot = f32[1024,16]{1,0} dot(reshape.13, parameter.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={devices=[4,1]0,1,2,3}
  constant = f32[] constant(1.11111116), sharding={replicated}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  broadcast.3 = f32[1024,16]{1,0} broadcast(constant), dimensions={}, sharding={replicated}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  multiply.5 = f32[1024,16]{1,0} multiply(dot, broadcast.3), sharding={devices=[4,1]0,1,2,3}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  reshape.15 = f32[32,32,16]{2,1,0} reshape(multiply.5), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  broadcast.19 = f32[32,32,16]{2,1,0} broadcast(constant.9), dimensions={}, sharding={replicated}, metadata={op_type="broadcast_in_dim" op_name="parallelize(func_shard_parallel)/broadcast_in_dim[shape=(32, 32, 16) broadcast_dimensions=()]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  select.20 = f32[32,32,16]{2,1,0} select(compare.14, reshape.15, broadcast.19), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="select_n" op_name="parallelize(func_shard_parallel)/select_n" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  reshape.3 = f32[1024,16]{1,0} reshape(select.20), sharding={devices=[4,1]0,1,2,3}
  parameter.3 = f32[16,16]{1,0} parameter(2), sharding={replicated}
  dot.1 = f32[1024,16]{1,0} dot(reshape.3, parameter.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={devices=[4,1]0,1,2,3}
  reshape.5 = f32[32,32,16]{2,1,0} reshape(dot.1), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  parameter.5 = f32[32,32,16]{2,1,0} parameter(4), sharding={devices=[4,1,1]0,1,2,3}
  subtract.24 = f32[32,32,16]{2,1,0} subtract(reshape.5, parameter.5), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="test_auto_sharding_basic.py" source_line=103}
  constant.4 = f32[] constant(0.000122070312), sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=103}
  broadcast.1 = f32[32,32,16]{2,1,0} broadcast(constant.4), dimensions={}, sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=103}
  multiply.42 = f32[32,32,16]{2,1,0} multiply(subtract.24, broadcast.1), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=103}
  reshape.9 = f32[1024,16]{1,0} reshape(multiply.42), sharding={devices=[4,1]0,1,2,3}
  dot.3 = f32[1024,16]{1,0} dot(reshape.9, parameter.3), lhs_contracting_dims={1}, rhs_contracting_dims={1}, sharding={devices=[4,1]0,1,2,3}
  reshape.11 = f32[32,32,16]{2,1,0} reshape(dot.3), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  select.51 = f32[32,32,16]{2,1,0} select(compare.50, reshape.11, broadcast.19), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="select_n" op_name="parallelize(func_shard_parallel)/select_n" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  broadcast.2 = f32[32,32,16]{2,1,0} broadcast(constant), dimensions={}, sharding={replicated}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  multiply.4 = f32[32,32,16]{2,1,0} multiply(select.51, broadcast.2), sharding={devices=[4,1,1]0,1,2,3}, metadata={op_type="div" op_name="parallelize(func_shard_parallel)/div" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/stochastic.py" source_line=68}
  transpose.8 = f32[16,32,32]{0,2,1} transpose(multiply.4), dimensions={2,0,1}, sharding={devices=[1,4,1]0,1,2,3}
  reshape.12 = f32[16,1024]{1,0} reshape(transpose.8), sharding={devices=[1,4]0,1,2,3}
  dot.5 = f32[16,16]{0,1} dot(reshape.13, reshape.12), lhs_contracting_dims={0}, rhs_contracting_dims={1}, sharding={replicated}, metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  constant.57 = f32[] constant(0.01), sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  broadcast.58 = f32[16,16]{1,0} broadcast(constant.57), dimensions={}, sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  multiply.59 = f32[16,16]{1,0} multiply(dot.5, broadcast.58), sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  subtract.60 = f32[16,16]{1,0} subtract(parameter.2, multiply.59), sharding={replicated}, metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  transpose.4 = f32[16,32,32]{0,2,1} transpose(multiply.42), dimensions={2,0,1}, sharding={devices=[1,4,1]0,1,2,3}
  reshape.6 = f32[16,1024]{1,0} reshape(transpose.4), sharding={devices=[1,4]0,1,2,3}
  dot.6 = f32[16,16]{0,1} dot(reshape.3, reshape.6), lhs_contracting_dims={0}, rhs_contracting_dims={1}, sharding={replicated}, metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  multiply.63 = f32[16,16]{1,0} multiply(dot.6, broadcast.58), sharding={replicated}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  subtract.64 = f32[16,16]{1,0} subtract(parameter.3, multiply.63), sharding={replicated}, metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  ROOT tuple.67 = (s32[], f32[16,16]{1,0}, f32[16,16]{1,0}) tuple(add.66, subtract.60, subtract.64), sharding={{replicated}, {replicated}, {replicated}}
}

