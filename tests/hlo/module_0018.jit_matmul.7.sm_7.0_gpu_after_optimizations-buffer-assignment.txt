BufferAssignment:
allocation 0: 0x3227e80, size 262144, parameter 0, shape |f32[64,1024]| at ShapeIndex {}:
 value: <0 Arg_0.1 @0> (size=262144,offset=0): f32[64,1024]{1,0}
allocation 1: 0x3227f30, size 262144, parameter 1, shape |f32[1024,64]| at ShapeIndex {}:
 value: <1 Arg_1.2 @0> (size=262144,offset=0): f32[1024,64]{1,0}
allocation 2: 0x3227fe0, size 16384, output shape is |f32[64,64]|, maybe-live-out:
 value: <2 cublas-gemm.1 @0> (size=16384,offset=0): f32[64,64]{1,0}

Total bytes used: 540672 (528.0KiB)

Used values:
<0 Arg_0.1 @0>
 positions:
  Arg_0.1
 uses:
  cublas-gemm.1, operand 0
 from instruction:%Arg_0.1 = f32[64,1024]{1,0} parameter(0)
<1 Arg_1.2 @0>
 positions:
  Arg_1.2
 uses:
  cublas-gemm.1, operand 1
 from instruction:%Arg_1.2 = f32[1024,64]{1,0} parameter(1)
<2 cublas-gemm.1 @0>
 positions:
  cublas-gemm.1
 uses:
 from instruction:%cublas-gemm.1 = f32[64,64]{1,0} custom-call(f32[64,1024]{1,0} %Arg_0.1, f32[1024,64]{1,0} %Arg_1.2), custom_call_target="__cublas$gemm", metadata={op_name="jit(matmul)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="test_auto_sharding_basic.py" source_line=52}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"65536\",\"rhs_stride\":\"65536\",\"selected_algorithm\":\"21\"}"


HloLiveRange (max 3):
  InstructionSequence:
    0:Arg_1.2
    1:Arg_0.1
    2:cublas-gemm.1
  BufferLiveRange:
    Arg_0.1{}:0-3
    Arg_1.2{}:0-3
    cublas-gemm.1{}:2-3
  Live ranges at 2 (peak):
    Arg_0.1: 262144 bytes
    Arg_1.2: 262144 bytes
    cublas-gemm.1: 16384 bytes
