target datalayout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64"
target triple = "nvptx64-nvidia-cuda"

@buffer_for_constant_185 = external constant [8 x i8], align 128
@rng_state = external dso_local addrspace(1) global i128
@0 = external dso_local unnamed_addr constant [4 x i8]
@1 = external dso_local unnamed_addr constant [4 x i8]
@2 = external dso_local unnamed_addr constant [4 x i8]
@3 = external dso_local unnamed_addr constant [4 x i8]
@4 = external dso_local unnamed_addr constant [4 x i8]
@5 = external dso_local unnamed_addr constant [4 x i8]
@6 = external dso_local unnamed_addr constant [4 x i8]
@7 = external dso_local unnamed_addr constant [4 x i8]
@8 = external dso_local unnamed_addr constant [4 x i8]
@9 = external dso_local unnamed_addr constant [4 x i8]
@10 = external dso_local unnamed_addr constant [4 x i8]
@11 = external dso_local unnamed_addr constant [4 x i8]
@12 = external dso_local unnamed_addr constant [4 x i8]
@13 = external dso_local unnamed_addr constant [4 x i8]
@14 = external dso_local unnamed_addr constant [4 x i8]
@15 = external dso_local unnamed_addr constant [4 x i8]
@16 = external dso_local unnamed_addr constant [4 x i8]
@17 = external dso_local unnamed_addr constant [4 x i8]
@18 = external dso_local unnamed_addr constant [4 x i8]
@19 = external dso_local unnamed_addr constant [8 x i8]
@20 = external dso_local unnamed_addr constant [4 x i8]
@21 = external dso_local unnamed_addr constant [8 x i8]
@22 = external dso_local unnamed_addr constant [4 x i8]
@23 = external dso_local unnamed_addr constant [4 x i8]
@24 = external dso_local unnamed_addr constant [4 x i8]
@25 = external dso_local unnamed_addr constant [4 x i8]
@26 = external dso_local unnamed_addr constant [8 x i8]
@27 = external dso_local unnamed_addr constant [4 x i8]
@28 = external dso_local unnamed_addr constant [4 x i8]
@29 = external dso_local unnamed_addr constant [4 x i8]
@30 = external dso_local unnamed_addr constant [4 x i8]
@31 = external dso_local unnamed_addr constant [4 x i8]
@32 = external dso_local unnamed_addr constant [4 x i8]
@33 = external dso_local unnamed_addr constant [4 x i8]
@34 = external dso_local unnamed_addr constant [4 x i8]
@35 = external dso_local unnamed_addr constant [4 x i8]
@36 = external dso_local unnamed_addr constant [4 x i8]
@37 = external dso_local unnamed_addr constant [4 x i8]
@38 = external dso_local unnamed_addr constant [4 x i8]
@39 = external dso_local unnamed_addr constant [4 x i8]
@40 = external dso_local unnamed_addr constant [4 x i8]
@41 = external dso_local unnamed_addr constant [4 x i8]
@42 = external dso_local unnamed_addr constant [4 x i8]
@43 = external dso_local unnamed_addr constant [4 x i8]
@44 = external dso_local unnamed_addr constant [4 x i8]
@45 = external dso_local unnamed_addr constant [4 x i8]
@46 = external dso_local unnamed_addr constant [4 x i8]
@47 = external dso_local unnamed_addr constant [4 x i8]
@48 = external dso_local unnamed_addr constant [4 x i8]
@49 = external dso_local unnamed_addr constant [4 x i8]
@50 = external dso_local unnamed_addr constant [4 x i8]
@51 = external dso_local unnamed_addr constant [4 x i8]
@52 = external dso_local unnamed_addr constant [4 x i8]
@53 = external dso_local unnamed_addr constant [4 x i8]
@54 = external dso_local unnamed_addr constant [4 x i8]
@55 = external dso_local unnamed_addr constant [4 x i8]
@56 = external dso_local unnamed_addr constant [4 x i8]
@57 = external dso_local unnamed_addr constant [4 x i8]
@58 = external dso_local unnamed_addr constant [4 x i8]
@59 = external dso_local unnamed_addr constant [4 x i8]
@60 = external dso_local unnamed_addr constant [4 x i8]
@61 = external dso_local unnamed_addr constant [4 x i8]
@62 = external dso_local unnamed_addr constant [4 x i8]
@63 = external dso_local unnamed_addr constant [4 x i8]
@64 = external dso_local unnamed_addr constant [4 x i8]
@65 = external dso_local unnamed_addr constant [4 x i8]
@66 = external dso_local unnamed_addr constant [4 x i8]
@67 = external dso_local unnamed_addr constant [4 x i8]
@68 = external dso_local unnamed_addr constant [4 x i8]
@69 = external dso_local unnamed_addr constant [4 x i8]
@70 = external dso_local unnamed_addr constant [4 x i8]
@71 = external dso_local unnamed_addr constant [4 x i8]
@72 = external dso_local unnamed_addr constant [4 x i8]
@73 = external dso_local unnamed_addr constant [4 x i8]
@74 = external dso_local unnamed_addr constant [4 x i8]

define void @rng_get_and_update_state(i8* noalias align 128 dereferenceable(16) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [2 x i64]*
  %load_state = load i128, i128 addrspace(1)* @rng_state, align 16
  %2 = add i128 %load_state, 256
  store i128 %2, i128 addrspace(1)* @rng_state, align 16
  %3 = bitcast [2 x i64]* %1 to i64*
  %rng_state_address = getelementptr inbounds i64, i64* %3, i64 0
  %4 = bitcast i64* %rng_state_address to i128*
  store i128 %load_state, i128* %4, align 16
  ret void
}

define void @fusion(i8* noalias align 16 dereferenceable(4) %alloc0, i8* noalias align 16 dereferenceable(4) %alloc1, i8* noalias align 128 dereferenceable(1024) %alloc2, i8* noalias align 128 dereferenceable(16) %temp_buf) {
entry:
  %0 = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %1 = bitcast i8* %0 to [2 x i64]*
  %2 = getelementptr inbounds i8, i8* %alloc0, i64 0
  %3 = bitcast i8* %2 to i32*
  %4 = getelementptr inbounds i8, i8* %alloc1, i64 0
  %5 = bitcast i8* %4 to i32*
  %6 = getelementptr inbounds i8, i8* %alloc2, i64 0
  %7 = bitcast i8* %6 to [16 x [16 x float]]*
  %8 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !4
  %9 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !5
  %10 = mul nuw nsw i32 %8, 64
  %linear_index = add nuw nsw i32 %10, %9
  %linear_index_in_range = icmp ult i32 %linear_index, 64
  call void @llvm.assume(i1 %linear_index_in_range)
  %linear_index_base = mul nuw nsw i32 %linear_index, 4
  %11 = udiv i32 %linear_index_base, 1
  %12 = urem i32 %11, 16
  %13 = udiv i32 %linear_index_base, 16
  %linear_index1 = add nuw nsw i32 %linear_index_base, 1
  %14 = udiv i32 %linear_index1, 1
  %15 = urem i32 %14, 16
  %16 = udiv i32 %linear_index1, 16
  %linear_index2 = add nuw nsw i32 %linear_index_base, 2
  %17 = udiv i32 %linear_index2, 1
  %18 = urem i32 %17, 16
  %19 = udiv i32 %linear_index2, 16
  %linear_index3 = add nuw nsw i32 %linear_index_base, 3
  %20 = udiv i32 %linear_index3, 1
  %21 = urem i32 %20, 16
  %22 = udiv i32 %linear_index3, 16
  %23 = icmp ult i32 %linear_index_base, 256
  br i1 %23, label %fusion.in_bounds-true, label %fusion.in_bounds-after

fusion.in_bounds-after:                           ; preds = %concatenate.272.merge914, %entry
  ret void

fusion.in_bounds-true:                            ; preds = %entry
  %24 = load i32, i32* %5, align 4, !invariant.load !6
  %25 = sitofp i32 %24 to float
  %compare.6 = fcmp une float %25, %25
  %26 = zext i1 %compare.6 to i8
  %region_0_446_constant_7 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_8 = load float, float* bitcast ([4 x i8]* @74 to float*), align 4
  %compare.9 = fcmp oeq float %25, %region_0_446_constant_8
  %27 = zext i1 %compare.9 to i8
  %region_0_446_constant_10 = load i32, i32* bitcast ([4 x i8]* @73 to i32*), align 4
  %28 = bitcast float %25 to i32
  %region_0_446_constant_12 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %29 = and i32 %28, %region_0_446_constant_12
  %region_0_446_constant_14 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %30 = icmp eq i32 %29, %region_0_446_constant_14
  %31 = zext i1 %30 to i8
  %region_0_446_constant_16 = load i32, i32* bitcast ([4 x i8]* @72 to i32*), align 4
  %region_0_446_constant_17 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %32 = icmp sgt i32 %29, %region_0_446_constant_17
  %33 = zext i1 %32 to i8
  %region_0_446_constant_19 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %34 = and i32 %28, %region_0_446_constant_19
  %region_0_446_constant_191 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %35 = icmp ne i32 %34, %region_0_446_constant_191
  %36 = zext i1 %35 to i8
  %37 = or i8 %33, %36
  %region_0_446_constant_23 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %38 = trunc i8 %37 to i1
  %39 = select i1 %38, i32 %region_0_446_constant_23, i32 %region_0_446_constant_24
  %40 = add i32 %28, %39
  %41 = trunc i8 %31 to i1
  %42 = select i1 %41, i32 %region_0_446_constant_16, i32 %40
  %43 = trunc i8 %27 to i1
  %44 = select i1 %43, i32 %region_0_446_constant_10, i32 %42
  %45 = trunc i8 %26 to i1
  %46 = select i1 %45, i32 %region_0_446_constant_7, i32 %44
  %47 = bitcast i32 %46 to float
  %48 = load i32, i32* %3, align 4, !invariant.load !6
  %49 = sitofp i32 %48 to float
  %compare.33 = fcmp une float %49, %49
  %50 = zext i1 %compare.33 to i8
  %region_0_446_constant_72 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_34 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %compare.35 = fcmp oeq float %49, %region_0_446_constant_34
  %51 = zext i1 %compare.35 to i8
  %region_0_446_constant_173 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %52 = bitcast float %49 to i32
  %region_0_446_constant_124 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %53 = and i32 %52, %region_0_446_constant_124
  %region_0_446_constant_145 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %54 = icmp eq i32 %53, %region_0_446_constant_145
  %55 = zext i1 %54 to i8
  %region_0_446_constant_246 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %region_0_446_constant_177 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %56 = icmp sgt i32 %53, %region_0_446_constant_177
  %57 = zext i1 %56 to i8
  %region_0_446_constant_198 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %58 = and i32 %52, %region_0_446_constant_198
  %region_0_446_constant_149 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %59 = icmp ne i32 %58, %region_0_446_constant_149
  %60 = zext i1 %59 to i8
  %61 = or i8 %57, %60
  %region_0_446_constant_2310 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_2411 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %62 = trunc i8 %61 to i1
  %63 = select i1 %62, i32 %region_0_446_constant_2310, i32 %region_0_446_constant_2411
  %64 = add i32 %52, %63
  %65 = trunc i8 %55 to i1
  %66 = select i1 %65, i32 %region_0_446_constant_246, i32 %64
  %67 = trunc i8 %51 to i1
  %68 = select i1 %67, i32 %region_0_446_constant_173, i32 %66
  %69 = trunc i8 %50 to i1
  %70 = select i1 %69, i32 %region_0_446_constant_72, i32 %68
  %71 = bitcast i32 %70 to float
  %72 = mul nuw nsw i32 %12, 1
  %73 = add nuw nsw i32 0, %72
  %74 = mul nuw nsw i32 %13, 16
  %75 = add nuw nsw i32 %73, %74
  %76 = urem i32 %75, 4
  %77 = udiv i32 %75, 4
  %78 = udiv i32 %77, 64
  br label %concatenate.pivot.2.

concat_index_from_operand_id0:                    ; preds = %concatenate.pivot.0.
  %79 = phi i32 [ 0, %concatenate.pivot.0. ]
  %80 = sub nsw i32 %76, %79
  %81 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %82 = load i64, i64* %81, align 8, !invariant.load !6
  %83 = trunc i64 %82 to i32
  %84 = zext i32 %83 to i64
  %85 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %86 = lshr i64 %82, %85
  %shft.chk = icmp ult i64 %85, 64
  %87 = select i1 %shft.chk, i64 %86, i64 0
  %88 = trunc i64 %87 to i32
  %89 = zext i32 %88 to i64
  %90 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %91 = shl i64 %89, %90
  %shft.chk12 = icmp ult i64 %90, 64
  %92 = select i1 %shft.chk12, i64 %91, i64 0
  %93 = or i64 %84, %92
  %94 = mul nuw nsw i32 %77, 1
  %95 = add nuw nsw i32 0, %94
  %96 = zext i32 %95 to i64
  %97 = add i64 %93, %96
  %98 = trunc i64 %97 to i32
  %99 = zext i32 %98 to i64
  %region_0_446_constant_64 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %100 = mul i64 %99, %region_0_446_constant_64
  %101 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %102 = lshr i64 %100, %101
  %shft.chk13 = icmp ult i64 %101, 64
  %103 = select i1 %shft.chk13, i64 %102, i64 0
  %104 = trunc i64 %103 to i32
  %105 = icmp ult i64 %97, %93
  %106 = zext i1 %105 to i8
  %107 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %108 = load i64, i64* %107, align 8, !invariant.load !6
  %109 = trunc i64 %108 to i32
  %110 = zext i32 %109 to i64
  %111 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %112 = lshr i64 %108, %111
  %shft.chk14 = icmp ult i64 %111, 64
  %113 = select i1 %shft.chk14, i64 %112, i64 0
  %114 = trunc i64 %113 to i32
  %115 = zext i32 %114 to i64
  %116 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %117 = shl i64 %115, %116
  %shft.chk15 = icmp ult i64 %116, 64
  %118 = select i1 %shft.chk15, i64 %117, i64 0
  %119 = or i64 %110, %118
  %region_0_446_constant_80 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %120 = add i64 %119, %region_0_446_constant_80
  %121 = trunc i8 %106 to i1
  %122 = select i1 %121, i64 %120, i64 %119
  %123 = lshr i64 %122, %101
  %shft.chk16 = icmp ult i64 %101, 64
  %124 = select i1 %shft.chk16, i64 %123, i64 0
  %125 = trunc i64 %124 to i32
  %126 = xor i32 %104, %125
  %region_0_446_constant_88 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %127 = xor i32 %126, %region_0_446_constant_88
  %128 = zext i32 %127 to i64
  %region_0_446_constant_92 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %129 = mul i64 %128, %region_0_446_constant_92
  %130 = lshr i64 %129, %101
  %shft.chk17 = icmp ult i64 %101, 64
  %131 = select i1 %shft.chk17, i64 %130, i64 0
  %132 = trunc i64 %131 to i32
  %133 = trunc i64 %122 to i32
  %134 = zext i32 %133 to i64
  %135 = mul i64 %134, %region_0_446_constant_92
  %136 = trunc i64 %135 to i32
  %137 = xor i32 %132, %136
  %region_0_446_constant_102 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %138 = xor i32 %137, %region_0_446_constant_102
  %139 = zext i32 %138 to i64
  %140 = mul i64 %139, %region_0_446_constant_64
  %141 = lshr i64 %140, %101
  %shft.chk18 = icmp ult i64 %101, 64
  %142 = select i1 %shft.chk18, i64 %141, i64 0
  %143 = trunc i64 %142 to i32
  %144 = lshr i64 %135, %101
  %shft.chk19 = icmp ult i64 %101, 64
  %145 = select i1 %shft.chk19, i64 %144, i64 0
  %146 = trunc i64 %145 to i32
  %147 = lshr i64 %97, %101
  %shft.chk20 = icmp ult i64 %101, 64
  %148 = select i1 %shft.chk20, i64 %147, i64 0
  %149 = trunc i64 %148 to i32
  %150 = xor i32 %146, %149
  %region_0_446_constant_114 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %151 = xor i32 %150, %region_0_446_constant_114
  %152 = zext i32 %151 to i64
  %153 = mul i64 %152, %region_0_446_constant_64
  %154 = trunc i64 %153 to i32
  %155 = xor i32 %143, %154
  %region_0_446_constant_121 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %156 = xor i32 %155, %region_0_446_constant_121
  %157 = zext i32 %156 to i64
  %158 = mul i64 %157, %region_0_446_constant_92
  %159 = lshr i64 %158, %101
  %shft.chk21 = icmp ult i64 %101, 64
  %160 = select i1 %shft.chk21, i64 %159, i64 0
  %161 = trunc i64 %160 to i32
  %162 = lshr i64 %153, %101
  %shft.chk22 = icmp ult i64 %101, 64
  %163 = select i1 %shft.chk22, i64 %162, i64 0
  %164 = trunc i64 %163 to i32
  %165 = trunc i64 %100 to i32
  %166 = xor i32 %164, %165
  %region_0_446_constant_132 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %167 = xor i32 %166, %region_0_446_constant_132
  %168 = zext i32 %167 to i64
  %169 = mul i64 %168, %region_0_446_constant_92
  %170 = trunc i64 %169 to i32
  %171 = xor i32 %161, %170
  %region_0_446_constant_139 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %172 = xor i32 %171, %region_0_446_constant_139
  %173 = zext i32 %172 to i64
  %174 = mul i64 %173, %region_0_446_constant_64
  %175 = lshr i64 %174, %101
  %shft.chk23 = icmp ult i64 %101, 64
  %176 = select i1 %shft.chk23, i64 %175, i64 0
  %177 = trunc i64 %176 to i32
  %178 = lshr i64 %169, %101
  %shft.chk24 = icmp ult i64 %101, 64
  %179 = select i1 %shft.chk24, i64 %178, i64 0
  %180 = trunc i64 %179 to i32
  %181 = trunc i64 %129 to i32
  %182 = xor i32 %180, %181
  %region_0_446_constant_150 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %183 = xor i32 %182, %region_0_446_constant_150
  %184 = zext i32 %183 to i64
  %185 = mul i64 %184, %region_0_446_constant_64
  %186 = trunc i64 %185 to i32
  %187 = xor i32 %177, %186
  %region_0_446_constant_157 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %188 = xor i32 %187, %region_0_446_constant_157
  %189 = zext i32 %188 to i64
  %190 = mul i64 %189, %region_0_446_constant_92
  %191 = lshr i64 %190, %101
  %shft.chk25 = icmp ult i64 %101, 64
  %192 = select i1 %shft.chk25, i64 %191, i64 0
  %193 = trunc i64 %192 to i32
  %194 = lshr i64 %185, %101
  %shft.chk26 = icmp ult i64 %101, 64
  %195 = select i1 %shft.chk26, i64 %194, i64 0
  %196 = trunc i64 %195 to i32
  %197 = trunc i64 %140 to i32
  %198 = xor i32 %196, %197
  %region_0_446_constant_168 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %199 = xor i32 %198, %region_0_446_constant_168
  %200 = zext i32 %199 to i64
  %201 = mul i64 %200, %region_0_446_constant_92
  %202 = trunc i64 %201 to i32
  %203 = xor i32 %193, %202
  %region_0_446_constant_175 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %204 = xor i32 %203, %region_0_446_constant_175
  %205 = zext i32 %204 to i64
  %206 = mul i64 %205, %region_0_446_constant_64
  %207 = lshr i64 %206, %101
  %shft.chk27 = icmp ult i64 %101, 64
  %208 = select i1 %shft.chk27, i64 %207, i64 0
  %209 = trunc i64 %208 to i32
  %210 = lshr i64 %201, %101
  %shft.chk28 = icmp ult i64 %101, 64
  %211 = select i1 %shft.chk28, i64 %210, i64 0
  %212 = trunc i64 %211 to i32
  %213 = trunc i64 %158 to i32
  %214 = xor i32 %212, %213
  %region_0_446_constant_186 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %215 = xor i32 %214, %region_0_446_constant_186
  %216 = zext i32 %215 to i64
  %217 = mul i64 %216, %region_0_446_constant_64
  %218 = trunc i64 %217 to i32
  %219 = xor i32 %209, %218
  %region_0_446_constant_193 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %220 = xor i32 %219, %region_0_446_constant_193
  %221 = zext i32 %220 to i64
  %222 = mul i64 %221, %region_0_446_constant_92
  %223 = lshr i64 %222, %101
  %shft.chk29 = icmp ult i64 %101, 64
  %224 = select i1 %shft.chk29, i64 %223, i64 0
  %225 = trunc i64 %224 to i32
  %226 = lshr i64 %217, %101
  %shft.chk30 = icmp ult i64 %101, 64
  %227 = select i1 %shft.chk30, i64 %226, i64 0
  %228 = trunc i64 %227 to i32
  %229 = trunc i64 %174 to i32
  %230 = xor i32 %228, %229
  %region_0_446_constant_204 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %231 = xor i32 %230, %region_0_446_constant_204
  %232 = zext i32 %231 to i64
  %233 = mul i64 %232, %region_0_446_constant_92
  %234 = trunc i64 %233 to i32
  %235 = xor i32 %225, %234
  %region_0_446_constant_211 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %236 = xor i32 %235, %region_0_446_constant_211
  %237 = zext i32 %236 to i64
  %238 = mul i64 %237, %region_0_446_constant_64
  %239 = lshr i64 %238, %101
  %shft.chk31 = icmp ult i64 %101, 64
  %240 = select i1 %shft.chk31, i64 %239, i64 0
  %241 = trunc i64 %240 to i32
  %242 = lshr i64 %233, %101
  %shft.chk32 = icmp ult i64 %101, 64
  %243 = select i1 %shft.chk32, i64 %242, i64 0
  %244 = trunc i64 %243 to i32
  %245 = trunc i64 %190 to i32
  %246 = xor i32 %244, %245
  %region_0_446_constant_222 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %247 = xor i32 %246, %region_0_446_constant_222
  %248 = zext i32 %247 to i64
  %249 = mul i64 %248, %region_0_446_constant_64
  %250 = trunc i64 %249 to i32
  %251 = xor i32 %241, %250
  %region_0_446_constant_229 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %252 = xor i32 %251, %region_0_446_constant_229
  %253 = zext i32 %252 to i64
  %254 = mul i64 %253, %region_0_446_constant_92
  %255 = lshr i64 %254, %101
  %shft.chk33 = icmp ult i64 %101, 64
  %256 = select i1 %shft.chk33, i64 %255, i64 0
  %257 = trunc i64 %256 to i32
  %258 = lshr i64 %249, %101
  %shft.chk34 = icmp ult i64 %101, 64
  %259 = select i1 %shft.chk34, i64 %258, i64 0
  %260 = trunc i64 %259 to i32
  %261 = trunc i64 %206 to i32
  %262 = xor i32 %260, %261
  %region_0_446_constant_240 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %263 = xor i32 %262, %region_0_446_constant_240
  %264 = zext i32 %263 to i64
  %265 = mul i64 %264, %region_0_446_constant_92
  %266 = trunc i64 %265 to i32
  %267 = xor i32 %257, %266
  %region_0_446_constant_247 = load i32, i32* bitcast ([4 x i8]* @41 to i32*), align 4
  %268 = xor i32 %267, %region_0_446_constant_247
  br label %concatenate.272.merge

concat_index_from_operand_id1:                    ; preds = %concatenate.pivot.1.160
  %269 = phi i32 [ 1, %concatenate.pivot.1.160 ]
  %270 = sub nsw i32 %76, %269
  %271 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %272 = load i64, i64* %271, align 8, !invariant.load !6
  %273 = trunc i64 %272 to i32
  %274 = zext i32 %273 to i64
  %275 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %276 = lshr i64 %272, %275
  %shft.chk35 = icmp ult i64 %275, 64
  %277 = select i1 %shft.chk35, i64 %276, i64 0
  %278 = trunc i64 %277 to i32
  %279 = zext i32 %278 to i64
  %280 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %281 = shl i64 %279, %280
  %shft.chk36 = icmp ult i64 %280, 64
  %282 = select i1 %shft.chk36, i64 %281, i64 0
  %283 = or i64 %274, %282
  %284 = mul nuw nsw i32 %77, 1
  %285 = add nuw nsw i32 0, %284
  %286 = zext i32 %285 to i64
  %287 = add i64 %283, %286
  %288 = trunc i64 %287 to i32
  %289 = zext i32 %288 to i64
  %region_0_446_constant_6437 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %290 = mul i64 %289, %region_0_446_constant_6437
  %291 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %292 = lshr i64 %290, %291
  %shft.chk38 = icmp ult i64 %291, 64
  %293 = select i1 %shft.chk38, i64 %292, i64 0
  %294 = trunc i64 %293 to i32
  %295 = icmp ult i64 %287, %283
  %296 = zext i1 %295 to i8
  %297 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %298 = load i64, i64* %297, align 8, !invariant.load !6
  %299 = trunc i64 %298 to i32
  %300 = zext i32 %299 to i64
  %301 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %302 = lshr i64 %298, %301
  %shft.chk39 = icmp ult i64 %301, 64
  %303 = select i1 %shft.chk39, i64 %302, i64 0
  %304 = trunc i64 %303 to i32
  %305 = zext i32 %304 to i64
  %306 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %307 = shl i64 %305, %306
  %shft.chk40 = icmp ult i64 %306, 64
  %308 = select i1 %shft.chk40, i64 %307, i64 0
  %309 = or i64 %300, %308
  %region_0_446_constant_8041 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %310 = add i64 %309, %region_0_446_constant_8041
  %311 = trunc i8 %296 to i1
  %312 = select i1 %311, i64 %310, i64 %309
  %313 = lshr i64 %312, %291
  %shft.chk42 = icmp ult i64 %291, 64
  %314 = select i1 %shft.chk42, i64 %313, i64 0
  %315 = trunc i64 %314 to i32
  %316 = xor i32 %294, %315
  %region_0_446_constant_8843 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %317 = xor i32 %316, %region_0_446_constant_8843
  %318 = zext i32 %317 to i64
  %region_0_446_constant_9244 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %319 = mul i64 %318, %region_0_446_constant_9244
  %320 = lshr i64 %319, %291
  %shft.chk45 = icmp ult i64 %291, 64
  %321 = select i1 %shft.chk45, i64 %320, i64 0
  %322 = trunc i64 %321 to i32
  %323 = trunc i64 %312 to i32
  %324 = zext i32 %323 to i64
  %325 = mul i64 %324, %region_0_446_constant_9244
  %326 = trunc i64 %325 to i32
  %327 = xor i32 %322, %326
  %region_0_446_constant_10246 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %328 = xor i32 %327, %region_0_446_constant_10246
  %329 = zext i32 %328 to i64
  %330 = mul i64 %329, %region_0_446_constant_6437
  %331 = lshr i64 %330, %291
  %shft.chk47 = icmp ult i64 %291, 64
  %332 = select i1 %shft.chk47, i64 %331, i64 0
  %333 = trunc i64 %332 to i32
  %334 = lshr i64 %325, %291
  %shft.chk48 = icmp ult i64 %291, 64
  %335 = select i1 %shft.chk48, i64 %334, i64 0
  %336 = trunc i64 %335 to i32
  %337 = lshr i64 %287, %291
  %shft.chk49 = icmp ult i64 %291, 64
  %338 = select i1 %shft.chk49, i64 %337, i64 0
  %339 = trunc i64 %338 to i32
  %340 = xor i32 %336, %339
  %region_0_446_constant_11450 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %341 = xor i32 %340, %region_0_446_constant_11450
  %342 = zext i32 %341 to i64
  %343 = mul i64 %342, %region_0_446_constant_6437
  %344 = trunc i64 %343 to i32
  %345 = xor i32 %333, %344
  %region_0_446_constant_12151 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %346 = xor i32 %345, %region_0_446_constant_12151
  %347 = zext i32 %346 to i64
  %348 = mul i64 %347, %region_0_446_constant_9244
  %349 = lshr i64 %348, %291
  %shft.chk52 = icmp ult i64 %291, 64
  %350 = select i1 %shft.chk52, i64 %349, i64 0
  %351 = trunc i64 %350 to i32
  %352 = lshr i64 %343, %291
  %shft.chk53 = icmp ult i64 %291, 64
  %353 = select i1 %shft.chk53, i64 %352, i64 0
  %354 = trunc i64 %353 to i32
  %355 = trunc i64 %290 to i32
  %356 = xor i32 %354, %355
  %region_0_446_constant_13254 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %357 = xor i32 %356, %region_0_446_constant_13254
  %358 = zext i32 %357 to i64
  %359 = mul i64 %358, %region_0_446_constant_9244
  %360 = trunc i64 %359 to i32
  %361 = xor i32 %351, %360
  %region_0_446_constant_13955 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %362 = xor i32 %361, %region_0_446_constant_13955
  %363 = zext i32 %362 to i64
  %364 = mul i64 %363, %region_0_446_constant_6437
  %365 = lshr i64 %364, %291
  %shft.chk56 = icmp ult i64 %291, 64
  %366 = select i1 %shft.chk56, i64 %365, i64 0
  %367 = trunc i64 %366 to i32
  %368 = lshr i64 %359, %291
  %shft.chk57 = icmp ult i64 %291, 64
  %369 = select i1 %shft.chk57, i64 %368, i64 0
  %370 = trunc i64 %369 to i32
  %371 = trunc i64 %319 to i32
  %372 = xor i32 %370, %371
  %region_0_446_constant_15058 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %373 = xor i32 %372, %region_0_446_constant_15058
  %374 = zext i32 %373 to i64
  %375 = mul i64 %374, %region_0_446_constant_6437
  %376 = trunc i64 %375 to i32
  %377 = xor i32 %367, %376
  %region_0_446_constant_15759 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %378 = xor i32 %377, %region_0_446_constant_15759
  %379 = zext i32 %378 to i64
  %380 = mul i64 %379, %region_0_446_constant_9244
  %381 = lshr i64 %380, %291
  %shft.chk60 = icmp ult i64 %291, 64
  %382 = select i1 %shft.chk60, i64 %381, i64 0
  %383 = trunc i64 %382 to i32
  %384 = lshr i64 %375, %291
  %shft.chk61 = icmp ult i64 %291, 64
  %385 = select i1 %shft.chk61, i64 %384, i64 0
  %386 = trunc i64 %385 to i32
  %387 = trunc i64 %330 to i32
  %388 = xor i32 %386, %387
  %region_0_446_constant_16862 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %389 = xor i32 %388, %region_0_446_constant_16862
  %390 = zext i32 %389 to i64
  %391 = mul i64 %390, %region_0_446_constant_9244
  %392 = trunc i64 %391 to i32
  %393 = xor i32 %383, %392
  %region_0_446_constant_17563 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %394 = xor i32 %393, %region_0_446_constant_17563
  %395 = zext i32 %394 to i64
  %396 = mul i64 %395, %region_0_446_constant_6437
  %397 = lshr i64 %396, %291
  %shft.chk64 = icmp ult i64 %291, 64
  %398 = select i1 %shft.chk64, i64 %397, i64 0
  %399 = trunc i64 %398 to i32
  %400 = lshr i64 %391, %291
  %shft.chk65 = icmp ult i64 %291, 64
  %401 = select i1 %shft.chk65, i64 %400, i64 0
  %402 = trunc i64 %401 to i32
  %403 = trunc i64 %348 to i32
  %404 = xor i32 %402, %403
  %region_0_446_constant_18666 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %405 = xor i32 %404, %region_0_446_constant_18666
  %406 = zext i32 %405 to i64
  %407 = mul i64 %406, %region_0_446_constant_6437
  %408 = trunc i64 %407 to i32
  %409 = xor i32 %399, %408
  %region_0_446_constant_19367 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %410 = xor i32 %409, %region_0_446_constant_19367
  %411 = zext i32 %410 to i64
  %412 = mul i64 %411, %region_0_446_constant_9244
  %413 = lshr i64 %412, %291
  %shft.chk68 = icmp ult i64 %291, 64
  %414 = select i1 %shft.chk68, i64 %413, i64 0
  %415 = trunc i64 %414 to i32
  %416 = lshr i64 %407, %291
  %shft.chk69 = icmp ult i64 %291, 64
  %417 = select i1 %shft.chk69, i64 %416, i64 0
  %418 = trunc i64 %417 to i32
  %419 = trunc i64 %364 to i32
  %420 = xor i32 %418, %419
  %region_0_446_constant_20470 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %421 = xor i32 %420, %region_0_446_constant_20470
  %422 = zext i32 %421 to i64
  %423 = mul i64 %422, %region_0_446_constant_9244
  %424 = trunc i64 %423 to i32
  %425 = xor i32 %415, %424
  %region_0_446_constant_21171 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %426 = xor i32 %425, %region_0_446_constant_21171
  %427 = zext i32 %426 to i64
  %428 = mul i64 %427, %region_0_446_constant_6437
  %429 = lshr i64 %428, %291
  %shft.chk72 = icmp ult i64 %291, 64
  %430 = select i1 %shft.chk72, i64 %429, i64 0
  %431 = trunc i64 %430 to i32
  %432 = lshr i64 %423, %291
  %shft.chk73 = icmp ult i64 %291, 64
  %433 = select i1 %shft.chk73, i64 %432, i64 0
  %434 = trunc i64 %433 to i32
  %435 = trunc i64 %380 to i32
  %436 = xor i32 %434, %435
  %region_0_446_constant_22274 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %437 = xor i32 %436, %region_0_446_constant_22274
  %438 = zext i32 %437 to i64
  %439 = mul i64 %438, %region_0_446_constant_6437
  %440 = trunc i64 %439 to i32
  %441 = xor i32 %431, %440
  %region_0_446_constant_22975 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %442 = xor i32 %441, %region_0_446_constant_22975
  %443 = zext i32 %442 to i64
  %444 = mul i64 %443, %region_0_446_constant_9244
  %445 = trunc i64 %444 to i32
  br label %concatenate.272.merge

concat_index_from_operand_id2:                    ; preds = %concatenate.pivot.2.161
  %446 = phi i32 [ 2, %concatenate.pivot.2.161 ]
  %447 = sub nsw i32 %76, %446
  %448 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %449 = load i64, i64* %448, align 8, !invariant.load !6
  %450 = trunc i64 %449 to i32
  %451 = zext i32 %450 to i64
  %452 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %453 = lshr i64 %449, %452
  %shft.chk76 = icmp ult i64 %452, 64
  %454 = select i1 %shft.chk76, i64 %453, i64 0
  %455 = trunc i64 %454 to i32
  %456 = zext i32 %455 to i64
  %457 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %458 = shl i64 %456, %457
  %shft.chk77 = icmp ult i64 %457, 64
  %459 = select i1 %shft.chk77, i64 %458, i64 0
  %460 = or i64 %451, %459
  %461 = mul nuw nsw i32 %77, 1
  %462 = add nuw nsw i32 0, %461
  %463 = zext i32 %462 to i64
  %464 = add i64 %460, %463
  %465 = icmp ult i64 %464, %460
  %466 = zext i1 %465 to i8
  %467 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %468 = load i64, i64* %467, align 8, !invariant.load !6
  %469 = trunc i64 %468 to i32
  %470 = zext i32 %469 to i64
  %471 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %472 = lshr i64 %468, %471
  %shft.chk78 = icmp ult i64 %471, 64
  %473 = select i1 %shft.chk78, i64 %472, i64 0
  %474 = trunc i64 %473 to i32
  %475 = zext i32 %474 to i64
  %476 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %477 = shl i64 %475, %476
  %shft.chk79 = icmp ult i64 %476, 64
  %478 = select i1 %shft.chk79, i64 %477, i64 0
  %479 = or i64 %470, %478
  %region_0_446_constant_8080 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %480 = add i64 %479, %region_0_446_constant_8080
  %481 = trunc i8 %466 to i1
  %482 = select i1 %481, i64 %480, i64 %479
  %483 = trunc i64 %482 to i32
  %484 = zext i32 %483 to i64
  %region_0_446_constant_9281 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %485 = mul i64 %484, %region_0_446_constant_9281
  %486 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %487 = lshr i64 %485, %486
  %shft.chk82 = icmp ult i64 %486, 64
  %488 = select i1 %shft.chk82, i64 %487, i64 0
  %489 = trunc i64 %488 to i32
  %490 = lshr i64 %464, %486
  %shft.chk83 = icmp ult i64 %486, 64
  %491 = select i1 %shft.chk83, i64 %490, i64 0
  %492 = trunc i64 %491 to i32
  %493 = xor i32 %489, %492
  %region_0_446_constant_11484 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %494 = xor i32 %493, %region_0_446_constant_11484
  %495 = zext i32 %494 to i64
  %region_0_446_constant_6485 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %496 = mul i64 %495, %region_0_446_constant_6485
  %497 = lshr i64 %496, %486
  %shft.chk86 = icmp ult i64 %486, 64
  %498 = select i1 %shft.chk86, i64 %497, i64 0
  %499 = trunc i64 %498 to i32
  %500 = trunc i64 %464 to i32
  %501 = zext i32 %500 to i64
  %502 = mul i64 %501, %region_0_446_constant_6485
  %503 = trunc i64 %502 to i32
  %504 = xor i32 %499, %503
  %region_0_446_constant_13287 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %505 = xor i32 %504, %region_0_446_constant_13287
  %506 = zext i32 %505 to i64
  %507 = mul i64 %506, %region_0_446_constant_9281
  %508 = lshr i64 %507, %486
  %shft.chk88 = icmp ult i64 %486, 64
  %509 = select i1 %shft.chk88, i64 %508, i64 0
  %510 = trunc i64 %509 to i32
  %511 = lshr i64 %502, %486
  %shft.chk89 = icmp ult i64 %486, 64
  %512 = select i1 %shft.chk89, i64 %511, i64 0
  %513 = trunc i64 %512 to i32
  %514 = lshr i64 %482, %486
  %shft.chk90 = icmp ult i64 %486, 64
  %515 = select i1 %shft.chk90, i64 %514, i64 0
  %516 = trunc i64 %515 to i32
  %517 = xor i32 %513, %516
  %region_0_446_constant_8891 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %518 = xor i32 %517, %region_0_446_constant_8891
  %519 = zext i32 %518 to i64
  %520 = mul i64 %519, %region_0_446_constant_9281
  %521 = trunc i64 %520 to i32
  %522 = xor i32 %510, %521
  %region_0_446_constant_15092 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %523 = xor i32 %522, %region_0_446_constant_15092
  %524 = zext i32 %523 to i64
  %525 = mul i64 %524, %region_0_446_constant_6485
  %526 = lshr i64 %525, %486
  %shft.chk93 = icmp ult i64 %486, 64
  %527 = select i1 %shft.chk93, i64 %526, i64 0
  %528 = trunc i64 %527 to i32
  %529 = lshr i64 %520, %486
  %shft.chk94 = icmp ult i64 %486, 64
  %530 = select i1 %shft.chk94, i64 %529, i64 0
  %531 = trunc i64 %530 to i32
  %532 = trunc i64 %485 to i32
  %533 = xor i32 %531, %532
  %region_0_446_constant_10295 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %534 = xor i32 %533, %region_0_446_constant_10295
  %535 = zext i32 %534 to i64
  %536 = mul i64 %535, %region_0_446_constant_6485
  %537 = trunc i64 %536 to i32
  %538 = xor i32 %528, %537
  %region_0_446_constant_16896 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %539 = xor i32 %538, %region_0_446_constant_16896
  %540 = zext i32 %539 to i64
  %541 = mul i64 %540, %region_0_446_constant_9281
  %542 = lshr i64 %541, %486
  %shft.chk97 = icmp ult i64 %486, 64
  %543 = select i1 %shft.chk97, i64 %542, i64 0
  %544 = trunc i64 %543 to i32
  %545 = lshr i64 %536, %486
  %shft.chk98 = icmp ult i64 %486, 64
  %546 = select i1 %shft.chk98, i64 %545, i64 0
  %547 = trunc i64 %546 to i32
  %548 = trunc i64 %496 to i32
  %549 = xor i32 %547, %548
  %region_0_446_constant_12199 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %550 = xor i32 %549, %region_0_446_constant_12199
  %551 = zext i32 %550 to i64
  %552 = mul i64 %551, %region_0_446_constant_9281
  %553 = trunc i64 %552 to i32
  %554 = xor i32 %544, %553
  %region_0_446_constant_186100 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %555 = xor i32 %554, %region_0_446_constant_186100
  %556 = zext i32 %555 to i64
  %557 = mul i64 %556, %region_0_446_constant_6485
  %558 = lshr i64 %557, %486
  %shft.chk101 = icmp ult i64 %486, 64
  %559 = select i1 %shft.chk101, i64 %558, i64 0
  %560 = trunc i64 %559 to i32
  %561 = lshr i64 %552, %486
  %shft.chk102 = icmp ult i64 %486, 64
  %562 = select i1 %shft.chk102, i64 %561, i64 0
  %563 = trunc i64 %562 to i32
  %564 = trunc i64 %507 to i32
  %565 = xor i32 %563, %564
  %region_0_446_constant_139103 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %566 = xor i32 %565, %region_0_446_constant_139103
  %567 = zext i32 %566 to i64
  %568 = mul i64 %567, %region_0_446_constant_6485
  %569 = trunc i64 %568 to i32
  %570 = xor i32 %560, %569
  %region_0_446_constant_204104 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %571 = xor i32 %570, %region_0_446_constant_204104
  %572 = zext i32 %571 to i64
  %573 = mul i64 %572, %region_0_446_constant_9281
  %574 = lshr i64 %573, %486
  %shft.chk105 = icmp ult i64 %486, 64
  %575 = select i1 %shft.chk105, i64 %574, i64 0
  %576 = trunc i64 %575 to i32
  %577 = lshr i64 %568, %486
  %shft.chk106 = icmp ult i64 %486, 64
  %578 = select i1 %shft.chk106, i64 %577, i64 0
  %579 = trunc i64 %578 to i32
  %580 = trunc i64 %525 to i32
  %581 = xor i32 %579, %580
  %region_0_446_constant_157107 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %582 = xor i32 %581, %region_0_446_constant_157107
  %583 = zext i32 %582 to i64
  %584 = mul i64 %583, %region_0_446_constant_9281
  %585 = trunc i64 %584 to i32
  %586 = xor i32 %576, %585
  %region_0_446_constant_222108 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %587 = xor i32 %586, %region_0_446_constant_222108
  %588 = zext i32 %587 to i64
  %589 = mul i64 %588, %region_0_446_constant_6485
  %590 = lshr i64 %589, %486
  %shft.chk109 = icmp ult i64 %486, 64
  %591 = select i1 %shft.chk109, i64 %590, i64 0
  %592 = trunc i64 %591 to i32
  %593 = lshr i64 %584, %486
  %shft.chk110 = icmp ult i64 %486, 64
  %594 = select i1 %shft.chk110, i64 %593, i64 0
  %595 = trunc i64 %594 to i32
  %596 = trunc i64 %541 to i32
  %597 = xor i32 %595, %596
  %region_0_446_constant_175111 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %598 = xor i32 %597, %region_0_446_constant_175111
  %599 = zext i32 %598 to i64
  %600 = mul i64 %599, %region_0_446_constant_6485
  %601 = trunc i64 %600 to i32
  %602 = xor i32 %592, %601
  %region_0_446_constant_240112 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %603 = xor i32 %602, %region_0_446_constant_240112
  %604 = zext i32 %603 to i64
  %605 = mul i64 %604, %region_0_446_constant_9281
  %606 = lshr i64 %605, %486
  %shft.chk113 = icmp ult i64 %486, 64
  %607 = select i1 %shft.chk113, i64 %606, i64 0
  %608 = trunc i64 %607 to i32
  %609 = lshr i64 %600, %486
  %shft.chk114 = icmp ult i64 %486, 64
  %610 = select i1 %shft.chk114, i64 %609, i64 0
  %611 = trunc i64 %610 to i32
  %612 = trunc i64 %557 to i32
  %613 = xor i32 %611, %612
  %region_0_446_constant_193115 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %614 = xor i32 %613, %region_0_446_constant_193115
  %615 = zext i32 %614 to i64
  %616 = mul i64 %615, %region_0_446_constant_9281
  %617 = trunc i64 %616 to i32
  %618 = xor i32 %608, %617
  %region_0_446_constant_257 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %619 = xor i32 %618, %region_0_446_constant_257
  %620 = zext i32 %619 to i64
  %621 = mul i64 %620, %region_0_446_constant_6485
  %622 = lshr i64 %621, %486
  %shft.chk116 = icmp ult i64 %486, 64
  %623 = select i1 %shft.chk116, i64 %622, i64 0
  %624 = trunc i64 %623 to i32
  %625 = lshr i64 %616, %486
  %shft.chk117 = icmp ult i64 %486, 64
  %626 = select i1 %shft.chk117, i64 %625, i64 0
  %627 = trunc i64 %626 to i32
  %628 = trunc i64 %573 to i32
  %629 = xor i32 %627, %628
  %region_0_446_constant_211118 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %630 = xor i32 %629, %region_0_446_constant_211118
  %631 = zext i32 %630 to i64
  %632 = mul i64 %631, %region_0_446_constant_6485
  %633 = trunc i64 %632 to i32
  %634 = xor i32 %624, %633
  %region_0_446_constant_266 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %635 = xor i32 %634, %region_0_446_constant_266
  br label %concatenate.272.merge

concat_index_from_operand_id3:                    ; preds = %concatenate.pivot.3.162
  %636 = phi i32 [ 3, %concatenate.pivot.3.162 ]
  %637 = sub nsw i32 %76, %636
  %638 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %639 = load i64, i64* %638, align 8, !invariant.load !6
  %640 = trunc i64 %639 to i32
  %641 = zext i32 %640 to i64
  %642 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %643 = lshr i64 %639, %642
  %shft.chk119 = icmp ult i64 %642, 64
  %644 = select i1 %shft.chk119, i64 %643, i64 0
  %645 = trunc i64 %644 to i32
  %646 = zext i32 %645 to i64
  %647 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %648 = shl i64 %646, %647
  %shft.chk120 = icmp ult i64 %647, 64
  %649 = select i1 %shft.chk120, i64 %648, i64 0
  %650 = or i64 %641, %649
  %651 = mul nuw nsw i32 %77, 1
  %652 = add nuw nsw i32 0, %651
  %653 = zext i32 %652 to i64
  %654 = add i64 %650, %653
  %655 = icmp ult i64 %654, %650
  %656 = zext i1 %655 to i8
  %657 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %658 = load i64, i64* %657, align 8, !invariant.load !6
  %659 = trunc i64 %658 to i32
  %660 = zext i32 %659 to i64
  %661 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %662 = lshr i64 %658, %661
  %shft.chk121 = icmp ult i64 %661, 64
  %663 = select i1 %shft.chk121, i64 %662, i64 0
  %664 = trunc i64 %663 to i32
  %665 = zext i32 %664 to i64
  %666 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %667 = shl i64 %665, %666
  %shft.chk122 = icmp ult i64 %666, 64
  %668 = select i1 %shft.chk122, i64 %667, i64 0
  %669 = or i64 %660, %668
  %region_0_446_constant_80123 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %670 = add i64 %669, %region_0_446_constant_80123
  %671 = trunc i8 %656 to i1
  %672 = select i1 %671, i64 %670, i64 %669
  %673 = trunc i64 %672 to i32
  %674 = zext i32 %673 to i64
  %region_0_446_constant_92124 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %675 = mul i64 %674, %region_0_446_constant_92124
  %676 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %677 = lshr i64 %675, %676
  %shft.chk125 = icmp ult i64 %676, 64
  %678 = select i1 %shft.chk125, i64 %677, i64 0
  %679 = trunc i64 %678 to i32
  %680 = lshr i64 %654, %676
  %shft.chk126 = icmp ult i64 %676, 64
  %681 = select i1 %shft.chk126, i64 %680, i64 0
  %682 = trunc i64 %681 to i32
  %683 = xor i32 %679, %682
  %region_0_446_constant_114127 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %684 = xor i32 %683, %region_0_446_constant_114127
  %685 = zext i32 %684 to i64
  %region_0_446_constant_64128 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %686 = mul i64 %685, %region_0_446_constant_64128
  %687 = lshr i64 %686, %676
  %shft.chk129 = icmp ult i64 %676, 64
  %688 = select i1 %shft.chk129, i64 %687, i64 0
  %689 = trunc i64 %688 to i32
  %690 = trunc i64 %654 to i32
  %691 = zext i32 %690 to i64
  %692 = mul i64 %691, %region_0_446_constant_64128
  %693 = trunc i64 %692 to i32
  %694 = xor i32 %689, %693
  %region_0_446_constant_132130 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %695 = xor i32 %694, %region_0_446_constant_132130
  %696 = zext i32 %695 to i64
  %697 = mul i64 %696, %region_0_446_constant_92124
  %698 = lshr i64 %697, %676
  %shft.chk131 = icmp ult i64 %676, 64
  %699 = select i1 %shft.chk131, i64 %698, i64 0
  %700 = trunc i64 %699 to i32
  %701 = lshr i64 %692, %676
  %shft.chk132 = icmp ult i64 %676, 64
  %702 = select i1 %shft.chk132, i64 %701, i64 0
  %703 = trunc i64 %702 to i32
  %704 = lshr i64 %672, %676
  %shft.chk133 = icmp ult i64 %676, 64
  %705 = select i1 %shft.chk133, i64 %704, i64 0
  %706 = trunc i64 %705 to i32
  %707 = xor i32 %703, %706
  %region_0_446_constant_88134 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %708 = xor i32 %707, %region_0_446_constant_88134
  %709 = zext i32 %708 to i64
  %710 = mul i64 %709, %region_0_446_constant_92124
  %711 = trunc i64 %710 to i32
  %712 = xor i32 %700, %711
  %region_0_446_constant_150135 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %713 = xor i32 %712, %region_0_446_constant_150135
  %714 = zext i32 %713 to i64
  %715 = mul i64 %714, %region_0_446_constant_64128
  %716 = lshr i64 %715, %676
  %shft.chk136 = icmp ult i64 %676, 64
  %717 = select i1 %shft.chk136, i64 %716, i64 0
  %718 = trunc i64 %717 to i32
  %719 = lshr i64 %710, %676
  %shft.chk137 = icmp ult i64 %676, 64
  %720 = select i1 %shft.chk137, i64 %719, i64 0
  %721 = trunc i64 %720 to i32
  %722 = trunc i64 %675 to i32
  %723 = xor i32 %721, %722
  %region_0_446_constant_102138 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %724 = xor i32 %723, %region_0_446_constant_102138
  %725 = zext i32 %724 to i64
  %726 = mul i64 %725, %region_0_446_constant_64128
  %727 = trunc i64 %726 to i32
  %728 = xor i32 %718, %727
  %region_0_446_constant_168139 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %729 = xor i32 %728, %region_0_446_constant_168139
  %730 = zext i32 %729 to i64
  %731 = mul i64 %730, %region_0_446_constant_92124
  %732 = lshr i64 %731, %676
  %shft.chk140 = icmp ult i64 %676, 64
  %733 = select i1 %shft.chk140, i64 %732, i64 0
  %734 = trunc i64 %733 to i32
  %735 = lshr i64 %726, %676
  %shft.chk141 = icmp ult i64 %676, 64
  %736 = select i1 %shft.chk141, i64 %735, i64 0
  %737 = trunc i64 %736 to i32
  %738 = trunc i64 %686 to i32
  %739 = xor i32 %737, %738
  %region_0_446_constant_121142 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %740 = xor i32 %739, %region_0_446_constant_121142
  %741 = zext i32 %740 to i64
  %742 = mul i64 %741, %region_0_446_constant_92124
  %743 = trunc i64 %742 to i32
  %744 = xor i32 %734, %743
  %region_0_446_constant_186143 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %745 = xor i32 %744, %region_0_446_constant_186143
  %746 = zext i32 %745 to i64
  %747 = mul i64 %746, %region_0_446_constant_64128
  %748 = lshr i64 %747, %676
  %shft.chk144 = icmp ult i64 %676, 64
  %749 = select i1 %shft.chk144, i64 %748, i64 0
  %750 = trunc i64 %749 to i32
  %751 = lshr i64 %742, %676
  %shft.chk145 = icmp ult i64 %676, 64
  %752 = select i1 %shft.chk145, i64 %751, i64 0
  %753 = trunc i64 %752 to i32
  %754 = trunc i64 %697 to i32
  %755 = xor i32 %753, %754
  %region_0_446_constant_139146 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %756 = xor i32 %755, %region_0_446_constant_139146
  %757 = zext i32 %756 to i64
  %758 = mul i64 %757, %region_0_446_constant_64128
  %759 = trunc i64 %758 to i32
  %760 = xor i32 %750, %759
  %region_0_446_constant_204147 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %761 = xor i32 %760, %region_0_446_constant_204147
  %762 = zext i32 %761 to i64
  %763 = mul i64 %762, %region_0_446_constant_92124
  %764 = lshr i64 %763, %676
  %shft.chk148 = icmp ult i64 %676, 64
  %765 = select i1 %shft.chk148, i64 %764, i64 0
  %766 = trunc i64 %765 to i32
  %767 = lshr i64 %758, %676
  %shft.chk149 = icmp ult i64 %676, 64
  %768 = select i1 %shft.chk149, i64 %767, i64 0
  %769 = trunc i64 %768 to i32
  %770 = trunc i64 %715 to i32
  %771 = xor i32 %769, %770
  %region_0_446_constant_157150 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %772 = xor i32 %771, %region_0_446_constant_157150
  %773 = zext i32 %772 to i64
  %774 = mul i64 %773, %region_0_446_constant_92124
  %775 = trunc i64 %774 to i32
  %776 = xor i32 %766, %775
  %region_0_446_constant_222151 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %777 = xor i32 %776, %region_0_446_constant_222151
  %778 = zext i32 %777 to i64
  %779 = mul i64 %778, %region_0_446_constant_64128
  %780 = lshr i64 %779, %676
  %shft.chk152 = icmp ult i64 %676, 64
  %781 = select i1 %shft.chk152, i64 %780, i64 0
  %782 = trunc i64 %781 to i32
  %783 = lshr i64 %774, %676
  %shft.chk153 = icmp ult i64 %676, 64
  %784 = select i1 %shft.chk153, i64 %783, i64 0
  %785 = trunc i64 %784 to i32
  %786 = trunc i64 %731 to i32
  %787 = xor i32 %785, %786
  %region_0_446_constant_175154 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %788 = xor i32 %787, %region_0_446_constant_175154
  %789 = zext i32 %788 to i64
  %790 = mul i64 %789, %region_0_446_constant_64128
  %791 = trunc i64 %790 to i32
  %792 = xor i32 %782, %791
  %region_0_446_constant_240155 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %793 = xor i32 %792, %region_0_446_constant_240155
  %794 = zext i32 %793 to i64
  %795 = mul i64 %794, %region_0_446_constant_92124
  %796 = lshr i64 %795, %676
  %shft.chk156 = icmp ult i64 %676, 64
  %797 = select i1 %shft.chk156, i64 %796, i64 0
  %798 = trunc i64 %797 to i32
  %799 = lshr i64 %790, %676
  %shft.chk157 = icmp ult i64 %676, 64
  %800 = select i1 %shft.chk157, i64 %799, i64 0
  %801 = trunc i64 %800 to i32
  %802 = trunc i64 %747 to i32
  %803 = xor i32 %801, %802
  %region_0_446_constant_193158 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %804 = xor i32 %803, %region_0_446_constant_193158
  %805 = zext i32 %804 to i64
  %806 = mul i64 %805, %region_0_446_constant_92124
  %807 = trunc i64 %806 to i32
  %808 = xor i32 %798, %807
  %region_0_446_constant_257159 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %809 = xor i32 %808, %region_0_446_constant_257159
  %810 = zext i32 %809 to i64
  %811 = mul i64 %810, %region_0_446_constant_64128
  %812 = trunc i64 %811 to i32
  br label %concatenate.272.merge

concatenate.pivot.2.:                             ; preds = %fusion.in_bounds-true
  %813 = icmp ult i32 %76, 2
  br i1 %813, label %concatenate.pivot.1., label %concatenate.pivot.3.

concatenate.pivot.1.:                             ; preds = %concatenate.pivot.2.
  %814 = icmp ult i32 %76, 1
  br i1 %814, label %concatenate.pivot.0., label %concatenate.pivot.1.160

concatenate.pivot.0.:                             ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id0

concatenate.pivot.1.160:                          ; preds = %concatenate.pivot.1.
  br label %concat_index_from_operand_id1

concatenate.pivot.3.:                             ; preds = %concatenate.pivot.2.
  %815 = icmp ult i32 %76, 3
  br i1 %815, label %concatenate.pivot.2.161, label %concatenate.pivot.3.162

concatenate.pivot.2.161:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id2

concatenate.pivot.3.162:                          ; preds = %concatenate.pivot.3.
  br label %concat_index_from_operand_id3

concatenate.272.merge:                            ; preds = %concat_index_from_operand_id3, %concat_index_from_operand_id2, %concat_index_from_operand_id1, %concat_index_from_operand_id0
  %816 = phi i32 [ %268, %concat_index_from_operand_id0 ], [ %445, %concat_index_from_operand_id1 ], [ %635, %concat_index_from_operand_id2 ], [ %812, %concat_index_from_operand_id3 ]
  %region_0_446_constant_273 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %817 = lshr i32 %816, %region_0_446_constant_273
  %shft.chk163 = icmp ult i32 %region_0_446_constant_273, 32
  %818 = select i1 %shft.chk163, i32 %817, i32 0
  %819 = uitofp i32 %818 to float
  %region_0_446_constant_277 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %820 = load i32, i32* %5, align 4, !invariant.load !6
  %821 = sitofp i32 %820 to float
  %region_0_446_constant_278 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.279 = fmul float %821, %region_0_446_constant_278
  %region_0_446_constant_280 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %822 = fcmp oge float %region_0_446_constant_277, %multiply.279
  %823 = fcmp une float %region_0_446_constant_277, %region_0_446_constant_277
  %824 = or i1 %822, %823
  %825 = select i1 %824, float %region_0_446_constant_277, float %multiply.279
  %826 = fcmp ole float %region_0_446_constant_280, %825
  %827 = fcmp une float %region_0_446_constant_280, %region_0_446_constant_280
  %828 = or i1 %826, %827
  %829 = select i1 %828, float %region_0_446_constant_280, float %825
  %multiply.282 = fmul float %829, %829
  %region_0_446_constant_283 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.284 = fmul float %multiply.282, %region_0_446_constant_283
  %region_0_446_constant_285 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.286 = fadd float %multiply.284, %region_0_446_constant_285
  %multiply.287 = fmul float %add.286, %multiply.282
  %region_0_446_constant_288 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.289 = fadd float %multiply.287, %region_0_446_constant_288
  %multiply.290 = fmul float %add.289, %multiply.282
  %region_0_446_constant_291 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.292 = fadd float %multiply.290, %region_0_446_constant_291
  %multiply.293 = fmul float %add.292, %multiply.282
  %region_0_446_constant_294 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.295 = fadd float %multiply.293, %region_0_446_constant_294
  %multiply.296 = fmul float %add.295, %multiply.282
  %region_0_446_constant_297 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.298 = fadd float %multiply.296, %region_0_446_constant_297
  %multiply.299 = fmul float %add.298, %multiply.282
  %region_0_446_constant_300 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.301 = fadd float %multiply.299, %region_0_446_constant_300
  %multiply.302 = fmul float %add.301, %multiply.282
  %region_0_446_constant_303 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.304 = fadd float %multiply.302, %region_0_446_constant_303
  %multiply.305 = fmul float %829, %add.304
  %region_0_446_constant_306 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.307 = fadd float %multiply.284, %region_0_446_constant_306
  %multiply.308 = fmul float %add.307, %multiply.282
  %region_0_446_constant_309 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.310 = fadd float %multiply.308, %region_0_446_constant_309
  %multiply.311 = fmul float %add.310, %multiply.282
  %region_0_446_constant_312 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.313 = fadd float %multiply.311, %region_0_446_constant_312
  %multiply.314 = fmul float %add.313, %multiply.282
  %region_0_446_constant_315 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.316 = fadd float %multiply.314, %region_0_446_constant_315
  %multiply.317 = fmul float %add.316, %multiply.282
  %region_0_446_constant_318 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.319 = fadd float %multiply.317, %region_0_446_constant_318
  %divide.320 = fdiv float %multiply.305, %add.319
  %region_0_446_constant_277164 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %830 = load i32, i32* %3, align 4, !invariant.load !6
  %831 = sitofp i32 %830 to float
  %region_0_446_constant_278165 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.321 = fmul float %831, %region_0_446_constant_278165
  %region_0_446_constant_280166 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %832 = fcmp oge float %region_0_446_constant_277164, %multiply.321
  %833 = fcmp une float %region_0_446_constant_277164, %region_0_446_constant_277164
  %834 = or i1 %832, %833
  %835 = select i1 %834, float %region_0_446_constant_277164, float %multiply.321
  %836 = fcmp ole float %region_0_446_constant_280166, %835
  %837 = fcmp une float %region_0_446_constant_280166, %region_0_446_constant_280166
  %838 = or i1 %836, %837
  %839 = select i1 %838, float %region_0_446_constant_280166, float %835
  %multiply.323 = fmul float %839, %839
  %region_0_446_constant_283167 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.324 = fmul float %multiply.323, %region_0_446_constant_283167
  %region_0_446_constant_285168 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.325 = fadd float %multiply.324, %region_0_446_constant_285168
  %multiply.326 = fmul float %add.325, %multiply.323
  %region_0_446_constant_288169 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.327 = fadd float %multiply.326, %region_0_446_constant_288169
  %multiply.328 = fmul float %add.327, %multiply.323
  %region_0_446_constant_291170 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.329 = fadd float %multiply.328, %region_0_446_constant_291170
  %multiply.330 = fmul float %add.329, %multiply.323
  %region_0_446_constant_294171 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.331 = fadd float %multiply.330, %region_0_446_constant_294171
  %multiply.332 = fmul float %add.331, %multiply.323
  %region_0_446_constant_297172 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.333 = fadd float %multiply.332, %region_0_446_constant_297172
  %multiply.334 = fmul float %add.333, %multiply.323
  %region_0_446_constant_300173 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.335 = fadd float %multiply.334, %region_0_446_constant_300173
  %multiply.336 = fmul float %add.335, %multiply.323
  %region_0_446_constant_303174 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.337 = fadd float %multiply.336, %region_0_446_constant_303174
  %multiply.338 = fmul float %839, %add.337
  %region_0_446_constant_306175 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.339 = fadd float %multiply.324, %region_0_446_constant_306175
  %multiply.340 = fmul float %add.339, %multiply.323
  %region_0_446_constant_309176 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.341 = fadd float %multiply.340, %region_0_446_constant_309176
  %multiply.342 = fmul float %add.341, %multiply.323
  %region_0_446_constant_312177 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.343 = fadd float %multiply.342, %region_0_446_constant_312177
  %multiply.344 = fmul float %add.343, %multiply.323
  %region_0_446_constant_315178 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.345 = fadd float %multiply.344, %region_0_446_constant_315178
  %multiply.346 = fmul float %add.345, %multiply.323
  %region_0_446_constant_318179 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.347 = fadd float %multiply.346, %region_0_446_constant_318179
  %divide.348 = fdiv float %multiply.338, %add.347
  %subtract.349 = fsub float %divide.320, %divide.348
  %region_0_446_constant_350 = load float, float* bitcast ([4 x i8]* @17 to float*), align 4
  %multiply.351 = fmul float %subtract.349, %region_0_446_constant_350
  %multiply.353 = fmul float %819, %multiply.351
  %add.355 = fadd float %multiply.353, %divide.348
  %840 = call float @llvm.fabs.f32(float %add.355)
  %region_0_446_constant_358 = load float, float* bitcast ([4 x i8]* @64 to float*), align 4
  %compare.360 = fcmp oeq float %840, %region_0_446_constant_358
  %841 = zext i1 %compare.360 to i8
  %region_0_446_constant_34180 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %multiply.362 = fmul float %add.355, %region_0_446_constant_34180
  %842 = fneg float %add.355
  %multiply.364 = fmul float %842, %add.355
  %843 = call float @__nv_log1pf(float %multiply.364)
  %844 = fneg float %843
  %region_0_446_constant_367 = load float, float* bitcast ([4 x i8]* @44 to float*), align 4
  %compare.369 = fcmp olt float %844, %region_0_446_constant_367
  %845 = zext i1 %compare.369 to i8
  %region_0_446_constant_370 = load float, float* bitcast ([4 x i8]* @62 to float*), align 4
  %region_0_446_constant_372 = load float, float* bitcast ([4 x i8]* @61 to float*), align 4
  %846 = trunc i8 %845 to i1
  %847 = select i1 %846, float %region_0_446_constant_370, float %region_0_446_constant_372
  %region_0_446_constant_375 = load float, float* bitcast ([4 x i8]* @60 to float*), align 4
  %region_0_446_constant_377 = load float, float* bitcast ([4 x i8]* @59 to float*), align 4
  %848 = trunc i8 %845 to i1
  %849 = select i1 %848, float %region_0_446_constant_375, float %region_0_446_constant_377
  %region_0_446_constant_380 = load float, float* bitcast ([4 x i8]* @58 to float*), align 4
  %region_0_446_constant_382 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %850 = trunc i8 %845 to i1
  %851 = select i1 %850, float %region_0_446_constant_380, float %region_0_446_constant_382
  %region_0_446_constant_385 = load float, float* bitcast ([4 x i8]* @56 to float*), align 4
  %region_0_446_constant_387 = load float, float* bitcast ([4 x i8]* @55 to float*), align 4
  %852 = trunc i8 %845 to i1
  %853 = select i1 %852, float %region_0_446_constant_385, float %region_0_446_constant_387
  %region_0_446_constant_390 = load float, float* bitcast ([4 x i8]* @54 to float*), align 4
  %region_0_446_constant_392 = load float, float* bitcast ([4 x i8]* @53 to float*), align 4
  %854 = trunc i8 %845 to i1
  %855 = select i1 %854, float %region_0_446_constant_390, float %region_0_446_constant_392
  %region_0_446_constant_395 = load float, float* bitcast ([4 x i8]* @52 to float*), align 4
  %region_0_446_constant_397 = load float, float* bitcast ([4 x i8]* @51 to float*), align 4
  %856 = trunc i8 %845 to i1
  %857 = select i1 %856, float %region_0_446_constant_395, float %region_0_446_constant_397
  %region_0_446_constant_400 = load float, float* bitcast ([4 x i8]* @50 to float*), align 4
  %region_0_446_constant_402 = load float, float* bitcast ([4 x i8]* @49 to float*), align 4
  %858 = trunc i8 %845 to i1
  %859 = select i1 %858, float %region_0_446_constant_400, float %region_0_446_constant_402
  %region_0_446_constant_405 = load float, float* bitcast ([4 x i8]* @48 to float*), align 4
  %region_0_446_constant_407 = load float, float* bitcast ([4 x i8]* @47 to float*), align 4
  %860 = trunc i8 %845 to i1
  %861 = select i1 %860, float %region_0_446_constant_405, float %region_0_446_constant_407
  %region_0_446_constant_410 = load float, float* bitcast ([4 x i8]* @46 to float*), align 4
  %region_0_446_constant_412 = load float, float* bitcast ([4 x i8]* @45 to float*), align 4
  %862 = trunc i8 %845 to i1
  %863 = select i1 %862, float %region_0_446_constant_410, float %region_0_446_constant_412
  %region_0_446_constant_415 = load float, float* bitcast ([4 x i8]* @43 to float*), align 4
  %add.417 = fadd float %844, %region_0_446_constant_415
  %864 = call float @__nv_sqrtf(float %844)
  %region_0_446_constant_419 = load float, float* bitcast ([4 x i8]* @42 to float*), align 4
  %add.421 = fadd float %864, %region_0_446_constant_419
  %865 = trunc i8 %845 to i1
  %866 = select i1 %865, float %add.417, float %add.421
  %multiply.423 = fmul float %863, %866
  %add.424 = fadd float %861, %multiply.423
  %multiply.425 = fmul float %add.424, %866
  %add.426 = fadd float %859, %multiply.425
  %multiply.427 = fmul float %add.426, %866
  %add.428 = fadd float %857, %multiply.427
  %multiply.429 = fmul float %add.428, %866
  %add.430 = fadd float %855, %multiply.429
  %multiply.431 = fmul float %add.430, %866
  %add.432 = fadd float %853, %multiply.431
  %multiply.433 = fmul float %add.432, %866
  %add.434 = fadd float %851, %multiply.433
  %multiply.435 = fmul float %add.434, %866
  %add.436 = fadd float %849, %multiply.435
  %multiply.437 = fmul float %add.436, %866
  %add.438 = fadd float %847, %multiply.437
  %multiply.439 = fmul float %add.438, %add.355
  %867 = trunc i8 %841 to i1
  %868 = select i1 %867, float %multiply.362, float %multiply.439
  %region_0_446_constant_441 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %multiply.443 = fmul float %868, %region_0_446_constant_441
  %869 = fcmp oge float %71, %multiply.443
  %870 = fcmp une float %71, %71
  %871 = or i1 %869, %870
  %maximum.444 = select i1 %871, float %71, float %multiply.443
  %872 = fcmp ole float %47, %maximum.444
  %873 = fcmp une float %47, %47
  %874 = or i1 %872, %873
  %minimum.445 = select i1 %874, float %47, float %maximum.444
  %875 = bitcast [16 x [16 x float]]* %7 to float*
  %876 = getelementptr inbounds float, float* %875, i32 %linear_index_base
  store float %minimum.445, float* %876, align 4
  %compare.6181 = fcmp une float %821, %821
  %877 = zext i1 %compare.6181 to i8
  %region_0_446_constant_7182 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_8183 = load float, float* bitcast ([4 x i8]* @74 to float*), align 4
  %compare.9184 = fcmp oeq float %821, %region_0_446_constant_8183
  %878 = zext i1 %compare.9184 to i8
  %region_0_446_constant_10185 = load i32, i32* bitcast ([4 x i8]* @73 to i32*), align 4
  %879 = bitcast float %821 to i32
  %region_0_446_constant_12186 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %880 = and i32 %879, %region_0_446_constant_12186
  %region_0_446_constant_14187 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %881 = icmp eq i32 %880, %region_0_446_constant_14187
  %882 = zext i1 %881 to i8
  %region_0_446_constant_16188 = load i32, i32* bitcast ([4 x i8]* @72 to i32*), align 4
  %region_0_446_constant_17189 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %883 = icmp sgt i32 %880, %region_0_446_constant_17189
  %884 = zext i1 %883 to i8
  %region_0_446_constant_19190 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %885 = and i32 %879, %region_0_446_constant_19190
  %region_0_446_constant_19191 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %886 = icmp ne i32 %885, %region_0_446_constant_19191
  %887 = zext i1 %886 to i8
  %888 = or i8 %884, %887
  %region_0_446_constant_23192 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24193 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %889 = trunc i8 %888 to i1
  %890 = select i1 %889, i32 %region_0_446_constant_23192, i32 %region_0_446_constant_24193
  %891 = add i32 %879, %890
  %892 = trunc i8 %882 to i1
  %893 = select i1 %892, i32 %region_0_446_constant_16188, i32 %891
  %894 = trunc i8 %878 to i1
  %895 = select i1 %894, i32 %region_0_446_constant_10185, i32 %893
  %896 = trunc i8 %877 to i1
  %897 = select i1 %896, i32 %region_0_446_constant_7182, i32 %895
  %898 = bitcast i32 %897 to float
  %compare.33194 = fcmp une float %831, %831
  %899 = zext i1 %compare.33194 to i8
  %region_0_446_constant_7195 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_34196 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %compare.35197 = fcmp oeq float %831, %region_0_446_constant_34196
  %900 = zext i1 %compare.35197 to i8
  %region_0_446_constant_17198 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %901 = bitcast float %831 to i32
  %region_0_446_constant_12200 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %902 = and i32 %901, %region_0_446_constant_12200
  %region_0_446_constant_14201 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %903 = icmp eq i32 %902, %region_0_446_constant_14201
  %904 = zext i1 %903 to i8
  %region_0_446_constant_24202 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %region_0_446_constant_17203 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %905 = icmp sgt i32 %902, %region_0_446_constant_17203
  %906 = zext i1 %905 to i8
  %region_0_446_constant_19204 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %907 = and i32 %901, %region_0_446_constant_19204
  %region_0_446_constant_14205 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %908 = icmp ne i32 %907, %region_0_446_constant_14205
  %909 = zext i1 %908 to i8
  %910 = or i8 %906, %909
  %region_0_446_constant_23206 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24207 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %911 = trunc i8 %910 to i1
  %912 = select i1 %911, i32 %region_0_446_constant_23206, i32 %region_0_446_constant_24207
  %913 = add i32 %901, %912
  %914 = trunc i8 %904 to i1
  %915 = select i1 %914, i32 %region_0_446_constant_24202, i32 %913
  %916 = trunc i8 %900 to i1
  %917 = select i1 %916, i32 %region_0_446_constant_17198, i32 %915
  %918 = trunc i8 %899 to i1
  %919 = select i1 %918, i32 %region_0_446_constant_7195, i32 %917
  %920 = bitcast i32 %919 to float
  %921 = mul nuw nsw i32 %15, 1
  %922 = add nuw nsw i32 0, %921
  %923 = mul nuw nsw i32 %16, 16
  %924 = add nuw nsw i32 %922, %923
  %925 = urem i32 %924, 4
  %926 = udiv i32 %924, 4
  %927 = udiv i32 %926, 64
  br label %concatenate.pivot.2.385

concat_index_from_operand_id0209:                 ; preds = %concatenate.pivot.0.387
  %928 = phi i32 [ 0, %concatenate.pivot.0.387 ]
  %929 = sub nsw i32 %925, %928
  %930 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %931 = load i64, i64* %930, align 8, !invariant.load !6
  %932 = trunc i64 %931 to i32
  %933 = zext i32 %932 to i64
  %934 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %935 = lshr i64 %931, %934
  %shft.chk210 = icmp ult i64 %934, 64
  %936 = select i1 %shft.chk210, i64 %935, i64 0
  %937 = trunc i64 %936 to i32
  %938 = zext i32 %937 to i64
  %939 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %940 = shl i64 %938, %939
  %shft.chk211 = icmp ult i64 %939, 64
  %941 = select i1 %shft.chk211, i64 %940, i64 0
  %942 = or i64 %933, %941
  %943 = mul nuw nsw i32 %926, 1
  %944 = add nuw nsw i32 0, %943
  %945 = zext i32 %944 to i64
  %946 = add i64 %942, %945
  %947 = trunc i64 %946 to i32
  %948 = zext i32 %947 to i64
  %region_0_446_constant_64212 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %949 = mul i64 %948, %region_0_446_constant_64212
  %950 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %951 = lshr i64 %949, %950
  %shft.chk213 = icmp ult i64 %950, 64
  %952 = select i1 %shft.chk213, i64 %951, i64 0
  %953 = trunc i64 %952 to i32
  %954 = icmp ult i64 %946, %942
  %955 = zext i1 %954 to i8
  %956 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %957 = load i64, i64* %956, align 8, !invariant.load !6
  %958 = trunc i64 %957 to i32
  %959 = zext i32 %958 to i64
  %960 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %961 = lshr i64 %957, %960
  %shft.chk214 = icmp ult i64 %960, 64
  %962 = select i1 %shft.chk214, i64 %961, i64 0
  %963 = trunc i64 %962 to i32
  %964 = zext i32 %963 to i64
  %965 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %966 = shl i64 %964, %965
  %shft.chk215 = icmp ult i64 %965, 64
  %967 = select i1 %shft.chk215, i64 %966, i64 0
  %968 = or i64 %959, %967
  %region_0_446_constant_80216 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %969 = add i64 %968, %region_0_446_constant_80216
  %970 = trunc i8 %955 to i1
  %971 = select i1 %970, i64 %969, i64 %968
  %972 = lshr i64 %971, %950
  %shft.chk217 = icmp ult i64 %950, 64
  %973 = select i1 %shft.chk217, i64 %972, i64 0
  %974 = trunc i64 %973 to i32
  %975 = xor i32 %953, %974
  %region_0_446_constant_88218 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %976 = xor i32 %975, %region_0_446_constant_88218
  %977 = zext i32 %976 to i64
  %region_0_446_constant_92219 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %978 = mul i64 %977, %region_0_446_constant_92219
  %979 = lshr i64 %978, %950
  %shft.chk220 = icmp ult i64 %950, 64
  %980 = select i1 %shft.chk220, i64 %979, i64 0
  %981 = trunc i64 %980 to i32
  %982 = trunc i64 %971 to i32
  %983 = zext i32 %982 to i64
  %984 = mul i64 %983, %region_0_446_constant_92219
  %985 = trunc i64 %984 to i32
  %986 = xor i32 %981, %985
  %region_0_446_constant_102221 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %987 = xor i32 %986, %region_0_446_constant_102221
  %988 = zext i32 %987 to i64
  %989 = mul i64 %988, %region_0_446_constant_64212
  %990 = lshr i64 %989, %950
  %shft.chk222 = icmp ult i64 %950, 64
  %991 = select i1 %shft.chk222, i64 %990, i64 0
  %992 = trunc i64 %991 to i32
  %993 = lshr i64 %984, %950
  %shft.chk223 = icmp ult i64 %950, 64
  %994 = select i1 %shft.chk223, i64 %993, i64 0
  %995 = trunc i64 %994 to i32
  %996 = lshr i64 %946, %950
  %shft.chk224 = icmp ult i64 %950, 64
  %997 = select i1 %shft.chk224, i64 %996, i64 0
  %998 = trunc i64 %997 to i32
  %999 = xor i32 %995, %998
  %region_0_446_constant_114225 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1000 = xor i32 %999, %region_0_446_constant_114225
  %1001 = zext i32 %1000 to i64
  %1002 = mul i64 %1001, %region_0_446_constant_64212
  %1003 = trunc i64 %1002 to i32
  %1004 = xor i32 %992, %1003
  %region_0_446_constant_121226 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1005 = xor i32 %1004, %region_0_446_constant_121226
  %1006 = zext i32 %1005 to i64
  %1007 = mul i64 %1006, %region_0_446_constant_92219
  %1008 = lshr i64 %1007, %950
  %shft.chk227 = icmp ult i64 %950, 64
  %1009 = select i1 %shft.chk227, i64 %1008, i64 0
  %1010 = trunc i64 %1009 to i32
  %1011 = lshr i64 %1002, %950
  %shft.chk228 = icmp ult i64 %950, 64
  %1012 = select i1 %shft.chk228, i64 %1011, i64 0
  %1013 = trunc i64 %1012 to i32
  %1014 = trunc i64 %949 to i32
  %1015 = xor i32 %1013, %1014
  %region_0_446_constant_132229 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %1016 = xor i32 %1015, %region_0_446_constant_132229
  %1017 = zext i32 %1016 to i64
  %1018 = mul i64 %1017, %region_0_446_constant_92219
  %1019 = trunc i64 %1018 to i32
  %1020 = xor i32 %1010, %1019
  %region_0_446_constant_139230 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %1021 = xor i32 %1020, %region_0_446_constant_139230
  %1022 = zext i32 %1021 to i64
  %1023 = mul i64 %1022, %region_0_446_constant_64212
  %1024 = lshr i64 %1023, %950
  %shft.chk231 = icmp ult i64 %950, 64
  %1025 = select i1 %shft.chk231, i64 %1024, i64 0
  %1026 = trunc i64 %1025 to i32
  %1027 = lshr i64 %1018, %950
  %shft.chk232 = icmp ult i64 %950, 64
  %1028 = select i1 %shft.chk232, i64 %1027, i64 0
  %1029 = trunc i64 %1028 to i32
  %1030 = trunc i64 %978 to i32
  %1031 = xor i32 %1029, %1030
  %region_0_446_constant_150233 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %1032 = xor i32 %1031, %region_0_446_constant_150233
  %1033 = zext i32 %1032 to i64
  %1034 = mul i64 %1033, %region_0_446_constant_64212
  %1035 = trunc i64 %1034 to i32
  %1036 = xor i32 %1026, %1035
  %region_0_446_constant_157234 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1037 = xor i32 %1036, %region_0_446_constant_157234
  %1038 = zext i32 %1037 to i64
  %1039 = mul i64 %1038, %region_0_446_constant_92219
  %1040 = lshr i64 %1039, %950
  %shft.chk235 = icmp ult i64 %950, 64
  %1041 = select i1 %shft.chk235, i64 %1040, i64 0
  %1042 = trunc i64 %1041 to i32
  %1043 = lshr i64 %1034, %950
  %shft.chk236 = icmp ult i64 %950, 64
  %1044 = select i1 %shft.chk236, i64 %1043, i64 0
  %1045 = trunc i64 %1044 to i32
  %1046 = trunc i64 %989 to i32
  %1047 = xor i32 %1045, %1046
  %region_0_446_constant_168237 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %1048 = xor i32 %1047, %region_0_446_constant_168237
  %1049 = zext i32 %1048 to i64
  %1050 = mul i64 %1049, %region_0_446_constant_92219
  %1051 = trunc i64 %1050 to i32
  %1052 = xor i32 %1042, %1051
  %region_0_446_constant_175238 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %1053 = xor i32 %1052, %region_0_446_constant_175238
  %1054 = zext i32 %1053 to i64
  %1055 = mul i64 %1054, %region_0_446_constant_64212
  %1056 = lshr i64 %1055, %950
  %shft.chk239 = icmp ult i64 %950, 64
  %1057 = select i1 %shft.chk239, i64 %1056, i64 0
  %1058 = trunc i64 %1057 to i32
  %1059 = lshr i64 %1050, %950
  %shft.chk240 = icmp ult i64 %950, 64
  %1060 = select i1 %shft.chk240, i64 %1059, i64 0
  %1061 = trunc i64 %1060 to i32
  %1062 = trunc i64 %1007 to i32
  %1063 = xor i32 %1061, %1062
  %region_0_446_constant_186241 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1064 = xor i32 %1063, %region_0_446_constant_186241
  %1065 = zext i32 %1064 to i64
  %1066 = mul i64 %1065, %region_0_446_constant_64212
  %1067 = trunc i64 %1066 to i32
  %1068 = xor i32 %1058, %1067
  %region_0_446_constant_193242 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1069 = xor i32 %1068, %region_0_446_constant_193242
  %1070 = zext i32 %1069 to i64
  %1071 = mul i64 %1070, %region_0_446_constant_92219
  %1072 = lshr i64 %1071, %950
  %shft.chk243 = icmp ult i64 %950, 64
  %1073 = select i1 %shft.chk243, i64 %1072, i64 0
  %1074 = trunc i64 %1073 to i32
  %1075 = lshr i64 %1066, %950
  %shft.chk244 = icmp ult i64 %950, 64
  %1076 = select i1 %shft.chk244, i64 %1075, i64 0
  %1077 = trunc i64 %1076 to i32
  %1078 = trunc i64 %1023 to i32
  %1079 = xor i32 %1077, %1078
  %region_0_446_constant_204245 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1080 = xor i32 %1079, %region_0_446_constant_204245
  %1081 = zext i32 %1080 to i64
  %1082 = mul i64 %1081, %region_0_446_constant_92219
  %1083 = trunc i64 %1082 to i32
  %1084 = xor i32 %1074, %1083
  %region_0_446_constant_211246 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1085 = xor i32 %1084, %region_0_446_constant_211246
  %1086 = zext i32 %1085 to i64
  %1087 = mul i64 %1086, %region_0_446_constant_64212
  %1088 = lshr i64 %1087, %950
  %shft.chk247 = icmp ult i64 %950, 64
  %1089 = select i1 %shft.chk247, i64 %1088, i64 0
  %1090 = trunc i64 %1089 to i32
  %1091 = lshr i64 %1082, %950
  %shft.chk248 = icmp ult i64 %950, 64
  %1092 = select i1 %shft.chk248, i64 %1091, i64 0
  %1093 = trunc i64 %1092 to i32
  %1094 = trunc i64 %1039 to i32
  %1095 = xor i32 %1093, %1094
  %region_0_446_constant_222249 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %1096 = xor i32 %1095, %region_0_446_constant_222249
  %1097 = zext i32 %1096 to i64
  %1098 = mul i64 %1097, %region_0_446_constant_64212
  %1099 = trunc i64 %1098 to i32
  %1100 = xor i32 %1090, %1099
  %region_0_446_constant_229250 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1101 = xor i32 %1100, %region_0_446_constant_229250
  %1102 = zext i32 %1101 to i64
  %1103 = mul i64 %1102, %region_0_446_constant_92219
  %1104 = lshr i64 %1103, %950
  %shft.chk251 = icmp ult i64 %950, 64
  %1105 = select i1 %shft.chk251, i64 %1104, i64 0
  %1106 = trunc i64 %1105 to i32
  %1107 = lshr i64 %1098, %950
  %shft.chk252 = icmp ult i64 %950, 64
  %1108 = select i1 %shft.chk252, i64 %1107, i64 0
  %1109 = trunc i64 %1108 to i32
  %1110 = trunc i64 %1055 to i32
  %1111 = xor i32 %1109, %1110
  %region_0_446_constant_240253 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1112 = xor i32 %1111, %region_0_446_constant_240253
  %1113 = zext i32 %1112 to i64
  %1114 = mul i64 %1113, %region_0_446_constant_92219
  %1115 = trunc i64 %1114 to i32
  %1116 = xor i32 %1106, %1115
  %region_0_446_constant_247254 = load i32, i32* bitcast ([4 x i8]* @41 to i32*), align 4
  %1117 = xor i32 %1116, %region_0_446_constant_247254
  br label %concatenate.272.merge208

concat_index_from_operand_id1255:                 ; preds = %concatenate.pivot.1.388
  %1118 = phi i32 [ 1, %concatenate.pivot.1.388 ]
  %1119 = sub nsw i32 %925, %1118
  %1120 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %1121 = load i64, i64* %1120, align 8, !invariant.load !6
  %1122 = trunc i64 %1121 to i32
  %1123 = zext i32 %1122 to i64
  %1124 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1125 = lshr i64 %1121, %1124
  %shft.chk256 = icmp ult i64 %1124, 64
  %1126 = select i1 %shft.chk256, i64 %1125, i64 0
  %1127 = trunc i64 %1126 to i32
  %1128 = zext i32 %1127 to i64
  %1129 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1130 = shl i64 %1128, %1129
  %shft.chk257 = icmp ult i64 %1129, 64
  %1131 = select i1 %shft.chk257, i64 %1130, i64 0
  %1132 = or i64 %1123, %1131
  %1133 = mul nuw nsw i32 %926, 1
  %1134 = add nuw nsw i32 0, %1133
  %1135 = zext i32 %1134 to i64
  %1136 = add i64 %1132, %1135
  %1137 = trunc i64 %1136 to i32
  %1138 = zext i32 %1137 to i64
  %region_0_446_constant_64258 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %1139 = mul i64 %1138, %region_0_446_constant_64258
  %1140 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1141 = lshr i64 %1139, %1140
  %shft.chk259 = icmp ult i64 %1140, 64
  %1142 = select i1 %shft.chk259, i64 %1141, i64 0
  %1143 = trunc i64 %1142 to i32
  %1144 = icmp ult i64 %1136, %1132
  %1145 = zext i1 %1144 to i8
  %1146 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %1147 = load i64, i64* %1146, align 8, !invariant.load !6
  %1148 = trunc i64 %1147 to i32
  %1149 = zext i32 %1148 to i64
  %1150 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1151 = lshr i64 %1147, %1150
  %shft.chk260 = icmp ult i64 %1150, 64
  %1152 = select i1 %shft.chk260, i64 %1151, i64 0
  %1153 = trunc i64 %1152 to i32
  %1154 = zext i32 %1153 to i64
  %1155 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1156 = shl i64 %1154, %1155
  %shft.chk261 = icmp ult i64 %1155, 64
  %1157 = select i1 %shft.chk261, i64 %1156, i64 0
  %1158 = or i64 %1149, %1157
  %region_0_446_constant_80262 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %1159 = add i64 %1158, %region_0_446_constant_80262
  %1160 = trunc i8 %1145 to i1
  %1161 = select i1 %1160, i64 %1159, i64 %1158
  %1162 = lshr i64 %1161, %1140
  %shft.chk263 = icmp ult i64 %1140, 64
  %1163 = select i1 %shft.chk263, i64 %1162, i64 0
  %1164 = trunc i64 %1163 to i32
  %1165 = xor i32 %1143, %1164
  %region_0_446_constant_88264 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %1166 = xor i32 %1165, %region_0_446_constant_88264
  %1167 = zext i32 %1166 to i64
  %region_0_446_constant_92265 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %1168 = mul i64 %1167, %region_0_446_constant_92265
  %1169 = lshr i64 %1168, %1140
  %shft.chk266 = icmp ult i64 %1140, 64
  %1170 = select i1 %shft.chk266, i64 %1169, i64 0
  %1171 = trunc i64 %1170 to i32
  %1172 = trunc i64 %1161 to i32
  %1173 = zext i32 %1172 to i64
  %1174 = mul i64 %1173, %region_0_446_constant_92265
  %1175 = trunc i64 %1174 to i32
  %1176 = xor i32 %1171, %1175
  %region_0_446_constant_102267 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %1177 = xor i32 %1176, %region_0_446_constant_102267
  %1178 = zext i32 %1177 to i64
  %1179 = mul i64 %1178, %region_0_446_constant_64258
  %1180 = lshr i64 %1179, %1140
  %shft.chk268 = icmp ult i64 %1140, 64
  %1181 = select i1 %shft.chk268, i64 %1180, i64 0
  %1182 = trunc i64 %1181 to i32
  %1183 = lshr i64 %1174, %1140
  %shft.chk269 = icmp ult i64 %1140, 64
  %1184 = select i1 %shft.chk269, i64 %1183, i64 0
  %1185 = trunc i64 %1184 to i32
  %1186 = lshr i64 %1136, %1140
  %shft.chk270 = icmp ult i64 %1140, 64
  %1187 = select i1 %shft.chk270, i64 %1186, i64 0
  %1188 = trunc i64 %1187 to i32
  %1189 = xor i32 %1185, %1188
  %region_0_446_constant_114271 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1190 = xor i32 %1189, %region_0_446_constant_114271
  %1191 = zext i32 %1190 to i64
  %1192 = mul i64 %1191, %region_0_446_constant_64258
  %1193 = trunc i64 %1192 to i32
  %1194 = xor i32 %1182, %1193
  %region_0_446_constant_121272 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1195 = xor i32 %1194, %region_0_446_constant_121272
  %1196 = zext i32 %1195 to i64
  %1197 = mul i64 %1196, %region_0_446_constant_92265
  %1198 = lshr i64 %1197, %1140
  %shft.chk273 = icmp ult i64 %1140, 64
  %1199 = select i1 %shft.chk273, i64 %1198, i64 0
  %1200 = trunc i64 %1199 to i32
  %1201 = lshr i64 %1192, %1140
  %shft.chk274 = icmp ult i64 %1140, 64
  %1202 = select i1 %shft.chk274, i64 %1201, i64 0
  %1203 = trunc i64 %1202 to i32
  %1204 = trunc i64 %1139 to i32
  %1205 = xor i32 %1203, %1204
  %region_0_446_constant_132275 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %1206 = xor i32 %1205, %region_0_446_constant_132275
  %1207 = zext i32 %1206 to i64
  %1208 = mul i64 %1207, %region_0_446_constant_92265
  %1209 = trunc i64 %1208 to i32
  %1210 = xor i32 %1200, %1209
  %region_0_446_constant_139276 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %1211 = xor i32 %1210, %region_0_446_constant_139276
  %1212 = zext i32 %1211 to i64
  %1213 = mul i64 %1212, %region_0_446_constant_64258
  %1214 = lshr i64 %1213, %1140
  %shft.chk277 = icmp ult i64 %1140, 64
  %1215 = select i1 %shft.chk277, i64 %1214, i64 0
  %1216 = trunc i64 %1215 to i32
  %1217 = lshr i64 %1208, %1140
  %shft.chk278 = icmp ult i64 %1140, 64
  %1218 = select i1 %shft.chk278, i64 %1217, i64 0
  %1219 = trunc i64 %1218 to i32
  %1220 = trunc i64 %1168 to i32
  %1221 = xor i32 %1219, %1220
  %region_0_446_constant_150279 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %1222 = xor i32 %1221, %region_0_446_constant_150279
  %1223 = zext i32 %1222 to i64
  %1224 = mul i64 %1223, %region_0_446_constant_64258
  %1225 = trunc i64 %1224 to i32
  %1226 = xor i32 %1216, %1225
  %region_0_446_constant_157280 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1227 = xor i32 %1226, %region_0_446_constant_157280
  %1228 = zext i32 %1227 to i64
  %1229 = mul i64 %1228, %region_0_446_constant_92265
  %1230 = lshr i64 %1229, %1140
  %shft.chk281 = icmp ult i64 %1140, 64
  %1231 = select i1 %shft.chk281, i64 %1230, i64 0
  %1232 = trunc i64 %1231 to i32
  %1233 = lshr i64 %1224, %1140
  %shft.chk282 = icmp ult i64 %1140, 64
  %1234 = select i1 %shft.chk282, i64 %1233, i64 0
  %1235 = trunc i64 %1234 to i32
  %1236 = trunc i64 %1179 to i32
  %1237 = xor i32 %1235, %1236
  %region_0_446_constant_168283 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %1238 = xor i32 %1237, %region_0_446_constant_168283
  %1239 = zext i32 %1238 to i64
  %1240 = mul i64 %1239, %region_0_446_constant_92265
  %1241 = trunc i64 %1240 to i32
  %1242 = xor i32 %1232, %1241
  %region_0_446_constant_175284 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %1243 = xor i32 %1242, %region_0_446_constant_175284
  %1244 = zext i32 %1243 to i64
  %1245 = mul i64 %1244, %region_0_446_constant_64258
  %1246 = lshr i64 %1245, %1140
  %shft.chk285 = icmp ult i64 %1140, 64
  %1247 = select i1 %shft.chk285, i64 %1246, i64 0
  %1248 = trunc i64 %1247 to i32
  %1249 = lshr i64 %1240, %1140
  %shft.chk286 = icmp ult i64 %1140, 64
  %1250 = select i1 %shft.chk286, i64 %1249, i64 0
  %1251 = trunc i64 %1250 to i32
  %1252 = trunc i64 %1197 to i32
  %1253 = xor i32 %1251, %1252
  %region_0_446_constant_186287 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1254 = xor i32 %1253, %region_0_446_constant_186287
  %1255 = zext i32 %1254 to i64
  %1256 = mul i64 %1255, %region_0_446_constant_64258
  %1257 = trunc i64 %1256 to i32
  %1258 = xor i32 %1248, %1257
  %region_0_446_constant_193288 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1259 = xor i32 %1258, %region_0_446_constant_193288
  %1260 = zext i32 %1259 to i64
  %1261 = mul i64 %1260, %region_0_446_constant_92265
  %1262 = lshr i64 %1261, %1140
  %shft.chk289 = icmp ult i64 %1140, 64
  %1263 = select i1 %shft.chk289, i64 %1262, i64 0
  %1264 = trunc i64 %1263 to i32
  %1265 = lshr i64 %1256, %1140
  %shft.chk290 = icmp ult i64 %1140, 64
  %1266 = select i1 %shft.chk290, i64 %1265, i64 0
  %1267 = trunc i64 %1266 to i32
  %1268 = trunc i64 %1213 to i32
  %1269 = xor i32 %1267, %1268
  %region_0_446_constant_204291 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1270 = xor i32 %1269, %region_0_446_constant_204291
  %1271 = zext i32 %1270 to i64
  %1272 = mul i64 %1271, %region_0_446_constant_92265
  %1273 = trunc i64 %1272 to i32
  %1274 = xor i32 %1264, %1273
  %region_0_446_constant_211292 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1275 = xor i32 %1274, %region_0_446_constant_211292
  %1276 = zext i32 %1275 to i64
  %1277 = mul i64 %1276, %region_0_446_constant_64258
  %1278 = lshr i64 %1277, %1140
  %shft.chk293 = icmp ult i64 %1140, 64
  %1279 = select i1 %shft.chk293, i64 %1278, i64 0
  %1280 = trunc i64 %1279 to i32
  %1281 = lshr i64 %1272, %1140
  %shft.chk294 = icmp ult i64 %1140, 64
  %1282 = select i1 %shft.chk294, i64 %1281, i64 0
  %1283 = trunc i64 %1282 to i32
  %1284 = trunc i64 %1229 to i32
  %1285 = xor i32 %1283, %1284
  %region_0_446_constant_222295 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %1286 = xor i32 %1285, %region_0_446_constant_222295
  %1287 = zext i32 %1286 to i64
  %1288 = mul i64 %1287, %region_0_446_constant_64258
  %1289 = trunc i64 %1288 to i32
  %1290 = xor i32 %1280, %1289
  %region_0_446_constant_229296 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1291 = xor i32 %1290, %region_0_446_constant_229296
  %1292 = zext i32 %1291 to i64
  %1293 = mul i64 %1292, %region_0_446_constant_92265
  %1294 = trunc i64 %1293 to i32
  br label %concatenate.272.merge208

concat_index_from_operand_id2297:                 ; preds = %concatenate.pivot.2.390
  %1295 = phi i32 [ 2, %concatenate.pivot.2.390 ]
  %1296 = sub nsw i32 %925, %1295
  %1297 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %1298 = load i64, i64* %1297, align 8, !invariant.load !6
  %1299 = trunc i64 %1298 to i32
  %1300 = zext i32 %1299 to i64
  %1301 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1302 = lshr i64 %1298, %1301
  %shft.chk298 = icmp ult i64 %1301, 64
  %1303 = select i1 %shft.chk298, i64 %1302, i64 0
  %1304 = trunc i64 %1303 to i32
  %1305 = zext i32 %1304 to i64
  %1306 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1307 = shl i64 %1305, %1306
  %shft.chk299 = icmp ult i64 %1306, 64
  %1308 = select i1 %shft.chk299, i64 %1307, i64 0
  %1309 = or i64 %1300, %1308
  %1310 = mul nuw nsw i32 %926, 1
  %1311 = add nuw nsw i32 0, %1310
  %1312 = zext i32 %1311 to i64
  %1313 = add i64 %1309, %1312
  %1314 = icmp ult i64 %1313, %1309
  %1315 = zext i1 %1314 to i8
  %1316 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %1317 = load i64, i64* %1316, align 8, !invariant.load !6
  %1318 = trunc i64 %1317 to i32
  %1319 = zext i32 %1318 to i64
  %1320 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1321 = lshr i64 %1317, %1320
  %shft.chk300 = icmp ult i64 %1320, 64
  %1322 = select i1 %shft.chk300, i64 %1321, i64 0
  %1323 = trunc i64 %1322 to i32
  %1324 = zext i32 %1323 to i64
  %1325 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1326 = shl i64 %1324, %1325
  %shft.chk301 = icmp ult i64 %1325, 64
  %1327 = select i1 %shft.chk301, i64 %1326, i64 0
  %1328 = or i64 %1319, %1327
  %region_0_446_constant_80302 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %1329 = add i64 %1328, %region_0_446_constant_80302
  %1330 = trunc i8 %1315 to i1
  %1331 = select i1 %1330, i64 %1329, i64 %1328
  %1332 = trunc i64 %1331 to i32
  %1333 = zext i32 %1332 to i64
  %region_0_446_constant_92303 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %1334 = mul i64 %1333, %region_0_446_constant_92303
  %1335 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1336 = lshr i64 %1334, %1335
  %shft.chk304 = icmp ult i64 %1335, 64
  %1337 = select i1 %shft.chk304, i64 %1336, i64 0
  %1338 = trunc i64 %1337 to i32
  %1339 = lshr i64 %1313, %1335
  %shft.chk305 = icmp ult i64 %1335, 64
  %1340 = select i1 %shft.chk305, i64 %1339, i64 0
  %1341 = trunc i64 %1340 to i32
  %1342 = xor i32 %1338, %1341
  %region_0_446_constant_114306 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1343 = xor i32 %1342, %region_0_446_constant_114306
  %1344 = zext i32 %1343 to i64
  %region_0_446_constant_64307 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %1345 = mul i64 %1344, %region_0_446_constant_64307
  %1346 = lshr i64 %1345, %1335
  %shft.chk308 = icmp ult i64 %1335, 64
  %1347 = select i1 %shft.chk308, i64 %1346, i64 0
  %1348 = trunc i64 %1347 to i32
  %1349 = trunc i64 %1313 to i32
  %1350 = zext i32 %1349 to i64
  %1351 = mul i64 %1350, %region_0_446_constant_64307
  %1352 = trunc i64 %1351 to i32
  %1353 = xor i32 %1348, %1352
  %region_0_446_constant_132309 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %1354 = xor i32 %1353, %region_0_446_constant_132309
  %1355 = zext i32 %1354 to i64
  %1356 = mul i64 %1355, %region_0_446_constant_92303
  %1357 = lshr i64 %1356, %1335
  %shft.chk310 = icmp ult i64 %1335, 64
  %1358 = select i1 %shft.chk310, i64 %1357, i64 0
  %1359 = trunc i64 %1358 to i32
  %1360 = lshr i64 %1351, %1335
  %shft.chk311 = icmp ult i64 %1335, 64
  %1361 = select i1 %shft.chk311, i64 %1360, i64 0
  %1362 = trunc i64 %1361 to i32
  %1363 = lshr i64 %1331, %1335
  %shft.chk312 = icmp ult i64 %1335, 64
  %1364 = select i1 %shft.chk312, i64 %1363, i64 0
  %1365 = trunc i64 %1364 to i32
  %1366 = xor i32 %1362, %1365
  %region_0_446_constant_88313 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %1367 = xor i32 %1366, %region_0_446_constant_88313
  %1368 = zext i32 %1367 to i64
  %1369 = mul i64 %1368, %region_0_446_constant_92303
  %1370 = trunc i64 %1369 to i32
  %1371 = xor i32 %1359, %1370
  %region_0_446_constant_150314 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %1372 = xor i32 %1371, %region_0_446_constant_150314
  %1373 = zext i32 %1372 to i64
  %1374 = mul i64 %1373, %region_0_446_constant_64307
  %1375 = lshr i64 %1374, %1335
  %shft.chk315 = icmp ult i64 %1335, 64
  %1376 = select i1 %shft.chk315, i64 %1375, i64 0
  %1377 = trunc i64 %1376 to i32
  %1378 = lshr i64 %1369, %1335
  %shft.chk316 = icmp ult i64 %1335, 64
  %1379 = select i1 %shft.chk316, i64 %1378, i64 0
  %1380 = trunc i64 %1379 to i32
  %1381 = trunc i64 %1334 to i32
  %1382 = xor i32 %1380, %1381
  %region_0_446_constant_102317 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %1383 = xor i32 %1382, %region_0_446_constant_102317
  %1384 = zext i32 %1383 to i64
  %1385 = mul i64 %1384, %region_0_446_constant_64307
  %1386 = trunc i64 %1385 to i32
  %1387 = xor i32 %1377, %1386
  %region_0_446_constant_168318 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %1388 = xor i32 %1387, %region_0_446_constant_168318
  %1389 = zext i32 %1388 to i64
  %1390 = mul i64 %1389, %region_0_446_constant_92303
  %1391 = lshr i64 %1390, %1335
  %shft.chk319 = icmp ult i64 %1335, 64
  %1392 = select i1 %shft.chk319, i64 %1391, i64 0
  %1393 = trunc i64 %1392 to i32
  %1394 = lshr i64 %1385, %1335
  %shft.chk320 = icmp ult i64 %1335, 64
  %1395 = select i1 %shft.chk320, i64 %1394, i64 0
  %1396 = trunc i64 %1395 to i32
  %1397 = trunc i64 %1345 to i32
  %1398 = xor i32 %1396, %1397
  %region_0_446_constant_121321 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1399 = xor i32 %1398, %region_0_446_constant_121321
  %1400 = zext i32 %1399 to i64
  %1401 = mul i64 %1400, %region_0_446_constant_92303
  %1402 = trunc i64 %1401 to i32
  %1403 = xor i32 %1393, %1402
  %region_0_446_constant_186322 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1404 = xor i32 %1403, %region_0_446_constant_186322
  %1405 = zext i32 %1404 to i64
  %1406 = mul i64 %1405, %region_0_446_constant_64307
  %1407 = lshr i64 %1406, %1335
  %shft.chk323 = icmp ult i64 %1335, 64
  %1408 = select i1 %shft.chk323, i64 %1407, i64 0
  %1409 = trunc i64 %1408 to i32
  %1410 = lshr i64 %1401, %1335
  %shft.chk324 = icmp ult i64 %1335, 64
  %1411 = select i1 %shft.chk324, i64 %1410, i64 0
  %1412 = trunc i64 %1411 to i32
  %1413 = trunc i64 %1356 to i32
  %1414 = xor i32 %1412, %1413
  %region_0_446_constant_139325 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %1415 = xor i32 %1414, %region_0_446_constant_139325
  %1416 = zext i32 %1415 to i64
  %1417 = mul i64 %1416, %region_0_446_constant_64307
  %1418 = trunc i64 %1417 to i32
  %1419 = xor i32 %1409, %1418
  %region_0_446_constant_204326 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1420 = xor i32 %1419, %region_0_446_constant_204326
  %1421 = zext i32 %1420 to i64
  %1422 = mul i64 %1421, %region_0_446_constant_92303
  %1423 = lshr i64 %1422, %1335
  %shft.chk327 = icmp ult i64 %1335, 64
  %1424 = select i1 %shft.chk327, i64 %1423, i64 0
  %1425 = trunc i64 %1424 to i32
  %1426 = lshr i64 %1417, %1335
  %shft.chk328 = icmp ult i64 %1335, 64
  %1427 = select i1 %shft.chk328, i64 %1426, i64 0
  %1428 = trunc i64 %1427 to i32
  %1429 = trunc i64 %1374 to i32
  %1430 = xor i32 %1428, %1429
  %region_0_446_constant_157329 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1431 = xor i32 %1430, %region_0_446_constant_157329
  %1432 = zext i32 %1431 to i64
  %1433 = mul i64 %1432, %region_0_446_constant_92303
  %1434 = trunc i64 %1433 to i32
  %1435 = xor i32 %1425, %1434
  %region_0_446_constant_222330 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %1436 = xor i32 %1435, %region_0_446_constant_222330
  %1437 = zext i32 %1436 to i64
  %1438 = mul i64 %1437, %region_0_446_constant_64307
  %1439 = lshr i64 %1438, %1335
  %shft.chk331 = icmp ult i64 %1335, 64
  %1440 = select i1 %shft.chk331, i64 %1439, i64 0
  %1441 = trunc i64 %1440 to i32
  %1442 = lshr i64 %1433, %1335
  %shft.chk332 = icmp ult i64 %1335, 64
  %1443 = select i1 %shft.chk332, i64 %1442, i64 0
  %1444 = trunc i64 %1443 to i32
  %1445 = trunc i64 %1390 to i32
  %1446 = xor i32 %1444, %1445
  %region_0_446_constant_175333 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %1447 = xor i32 %1446, %region_0_446_constant_175333
  %1448 = zext i32 %1447 to i64
  %1449 = mul i64 %1448, %region_0_446_constant_64307
  %1450 = trunc i64 %1449 to i32
  %1451 = xor i32 %1441, %1450
  %region_0_446_constant_240334 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1452 = xor i32 %1451, %region_0_446_constant_240334
  %1453 = zext i32 %1452 to i64
  %1454 = mul i64 %1453, %region_0_446_constant_92303
  %1455 = lshr i64 %1454, %1335
  %shft.chk335 = icmp ult i64 %1335, 64
  %1456 = select i1 %shft.chk335, i64 %1455, i64 0
  %1457 = trunc i64 %1456 to i32
  %1458 = lshr i64 %1449, %1335
  %shft.chk336 = icmp ult i64 %1335, 64
  %1459 = select i1 %shft.chk336, i64 %1458, i64 0
  %1460 = trunc i64 %1459 to i32
  %1461 = trunc i64 %1406 to i32
  %1462 = xor i32 %1460, %1461
  %region_0_446_constant_193337 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1463 = xor i32 %1462, %region_0_446_constant_193337
  %1464 = zext i32 %1463 to i64
  %1465 = mul i64 %1464, %region_0_446_constant_92303
  %1466 = trunc i64 %1465 to i32
  %1467 = xor i32 %1457, %1466
  %region_0_446_constant_257338 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %1468 = xor i32 %1467, %region_0_446_constant_257338
  %1469 = zext i32 %1468 to i64
  %1470 = mul i64 %1469, %region_0_446_constant_64307
  %1471 = lshr i64 %1470, %1335
  %shft.chk339 = icmp ult i64 %1335, 64
  %1472 = select i1 %shft.chk339, i64 %1471, i64 0
  %1473 = trunc i64 %1472 to i32
  %1474 = lshr i64 %1465, %1335
  %shft.chk340 = icmp ult i64 %1335, 64
  %1475 = select i1 %shft.chk340, i64 %1474, i64 0
  %1476 = trunc i64 %1475 to i32
  %1477 = trunc i64 %1422 to i32
  %1478 = xor i32 %1476, %1477
  %region_0_446_constant_211341 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1479 = xor i32 %1478, %region_0_446_constant_211341
  %1480 = zext i32 %1479 to i64
  %1481 = mul i64 %1480, %region_0_446_constant_64307
  %1482 = trunc i64 %1481 to i32
  %1483 = xor i32 %1473, %1482
  %region_0_446_constant_266342 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %1484 = xor i32 %1483, %region_0_446_constant_266342
  br label %concatenate.272.merge208

concat_index_from_operand_id3343:                 ; preds = %concatenate.pivot.3.391
  %1485 = phi i32 [ 3, %concatenate.pivot.3.391 ]
  %1486 = sub nsw i32 %925, %1485
  %1487 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %1488 = load i64, i64* %1487, align 8, !invariant.load !6
  %1489 = trunc i64 %1488 to i32
  %1490 = zext i32 %1489 to i64
  %1491 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1492 = lshr i64 %1488, %1491
  %shft.chk344 = icmp ult i64 %1491, 64
  %1493 = select i1 %shft.chk344, i64 %1492, i64 0
  %1494 = trunc i64 %1493 to i32
  %1495 = zext i32 %1494 to i64
  %1496 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1497 = shl i64 %1495, %1496
  %shft.chk345 = icmp ult i64 %1496, 64
  %1498 = select i1 %shft.chk345, i64 %1497, i64 0
  %1499 = or i64 %1490, %1498
  %1500 = mul nuw nsw i32 %926, 1
  %1501 = add nuw nsw i32 0, %1500
  %1502 = zext i32 %1501 to i64
  %1503 = add i64 %1499, %1502
  %1504 = icmp ult i64 %1503, %1499
  %1505 = zext i1 %1504 to i8
  %1506 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %1507 = load i64, i64* %1506, align 8, !invariant.load !6
  %1508 = trunc i64 %1507 to i32
  %1509 = zext i32 %1508 to i64
  %1510 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1511 = lshr i64 %1507, %1510
  %shft.chk346 = icmp ult i64 %1510, 64
  %1512 = select i1 %shft.chk346, i64 %1511, i64 0
  %1513 = trunc i64 %1512 to i32
  %1514 = zext i32 %1513 to i64
  %1515 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1516 = shl i64 %1514, %1515
  %shft.chk347 = icmp ult i64 %1515, 64
  %1517 = select i1 %shft.chk347, i64 %1516, i64 0
  %1518 = or i64 %1509, %1517
  %region_0_446_constant_80348 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %1519 = add i64 %1518, %region_0_446_constant_80348
  %1520 = trunc i8 %1505 to i1
  %1521 = select i1 %1520, i64 %1519, i64 %1518
  %1522 = trunc i64 %1521 to i32
  %1523 = zext i32 %1522 to i64
  %region_0_446_constant_92349 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %1524 = mul i64 %1523, %region_0_446_constant_92349
  %1525 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1526 = lshr i64 %1524, %1525
  %shft.chk350 = icmp ult i64 %1525, 64
  %1527 = select i1 %shft.chk350, i64 %1526, i64 0
  %1528 = trunc i64 %1527 to i32
  %1529 = lshr i64 %1503, %1525
  %shft.chk351 = icmp ult i64 %1525, 64
  %1530 = select i1 %shft.chk351, i64 %1529, i64 0
  %1531 = trunc i64 %1530 to i32
  %1532 = xor i32 %1528, %1531
  %region_0_446_constant_114352 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1533 = xor i32 %1532, %region_0_446_constant_114352
  %1534 = zext i32 %1533 to i64
  %region_0_446_constant_64353 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %1535 = mul i64 %1534, %region_0_446_constant_64353
  %1536 = lshr i64 %1535, %1525
  %shft.chk354 = icmp ult i64 %1525, 64
  %1537 = select i1 %shft.chk354, i64 %1536, i64 0
  %1538 = trunc i64 %1537 to i32
  %1539 = trunc i64 %1503 to i32
  %1540 = zext i32 %1539 to i64
  %1541 = mul i64 %1540, %region_0_446_constant_64353
  %1542 = trunc i64 %1541 to i32
  %1543 = xor i32 %1538, %1542
  %region_0_446_constant_132355 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %1544 = xor i32 %1543, %region_0_446_constant_132355
  %1545 = zext i32 %1544 to i64
  %1546 = mul i64 %1545, %region_0_446_constant_92349
  %1547 = lshr i64 %1546, %1525
  %shft.chk356 = icmp ult i64 %1525, 64
  %1548 = select i1 %shft.chk356, i64 %1547, i64 0
  %1549 = trunc i64 %1548 to i32
  %1550 = lshr i64 %1541, %1525
  %shft.chk357 = icmp ult i64 %1525, 64
  %1551 = select i1 %shft.chk357, i64 %1550, i64 0
  %1552 = trunc i64 %1551 to i32
  %1553 = lshr i64 %1521, %1525
  %shft.chk358 = icmp ult i64 %1525, 64
  %1554 = select i1 %shft.chk358, i64 %1553, i64 0
  %1555 = trunc i64 %1554 to i32
  %1556 = xor i32 %1552, %1555
  %region_0_446_constant_88359 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %1557 = xor i32 %1556, %region_0_446_constant_88359
  %1558 = zext i32 %1557 to i64
  %1559 = mul i64 %1558, %region_0_446_constant_92349
  %1560 = trunc i64 %1559 to i32
  %1561 = xor i32 %1549, %1560
  %region_0_446_constant_150360 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %1562 = xor i32 %1561, %region_0_446_constant_150360
  %1563 = zext i32 %1562 to i64
  %1564 = mul i64 %1563, %region_0_446_constant_64353
  %1565 = lshr i64 %1564, %1525
  %shft.chk361 = icmp ult i64 %1525, 64
  %1566 = select i1 %shft.chk361, i64 %1565, i64 0
  %1567 = trunc i64 %1566 to i32
  %1568 = lshr i64 %1559, %1525
  %shft.chk362 = icmp ult i64 %1525, 64
  %1569 = select i1 %shft.chk362, i64 %1568, i64 0
  %1570 = trunc i64 %1569 to i32
  %1571 = trunc i64 %1524 to i32
  %1572 = xor i32 %1570, %1571
  %region_0_446_constant_102363 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %1573 = xor i32 %1572, %region_0_446_constant_102363
  %1574 = zext i32 %1573 to i64
  %1575 = mul i64 %1574, %region_0_446_constant_64353
  %1576 = trunc i64 %1575 to i32
  %1577 = xor i32 %1567, %1576
  %region_0_446_constant_168364 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %1578 = xor i32 %1577, %region_0_446_constant_168364
  %1579 = zext i32 %1578 to i64
  %1580 = mul i64 %1579, %region_0_446_constant_92349
  %1581 = lshr i64 %1580, %1525
  %shft.chk365 = icmp ult i64 %1525, 64
  %1582 = select i1 %shft.chk365, i64 %1581, i64 0
  %1583 = trunc i64 %1582 to i32
  %1584 = lshr i64 %1575, %1525
  %shft.chk366 = icmp ult i64 %1525, 64
  %1585 = select i1 %shft.chk366, i64 %1584, i64 0
  %1586 = trunc i64 %1585 to i32
  %1587 = trunc i64 %1535 to i32
  %1588 = xor i32 %1586, %1587
  %region_0_446_constant_121367 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1589 = xor i32 %1588, %region_0_446_constant_121367
  %1590 = zext i32 %1589 to i64
  %1591 = mul i64 %1590, %region_0_446_constant_92349
  %1592 = trunc i64 %1591 to i32
  %1593 = xor i32 %1583, %1592
  %region_0_446_constant_186368 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1594 = xor i32 %1593, %region_0_446_constant_186368
  %1595 = zext i32 %1594 to i64
  %1596 = mul i64 %1595, %region_0_446_constant_64353
  %1597 = lshr i64 %1596, %1525
  %shft.chk369 = icmp ult i64 %1525, 64
  %1598 = select i1 %shft.chk369, i64 %1597, i64 0
  %1599 = trunc i64 %1598 to i32
  %1600 = lshr i64 %1591, %1525
  %shft.chk370 = icmp ult i64 %1525, 64
  %1601 = select i1 %shft.chk370, i64 %1600, i64 0
  %1602 = trunc i64 %1601 to i32
  %1603 = trunc i64 %1546 to i32
  %1604 = xor i32 %1602, %1603
  %region_0_446_constant_139371 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %1605 = xor i32 %1604, %region_0_446_constant_139371
  %1606 = zext i32 %1605 to i64
  %1607 = mul i64 %1606, %region_0_446_constant_64353
  %1608 = trunc i64 %1607 to i32
  %1609 = xor i32 %1599, %1608
  %region_0_446_constant_204372 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1610 = xor i32 %1609, %region_0_446_constant_204372
  %1611 = zext i32 %1610 to i64
  %1612 = mul i64 %1611, %region_0_446_constant_92349
  %1613 = lshr i64 %1612, %1525
  %shft.chk373 = icmp ult i64 %1525, 64
  %1614 = select i1 %shft.chk373, i64 %1613, i64 0
  %1615 = trunc i64 %1614 to i32
  %1616 = lshr i64 %1607, %1525
  %shft.chk374 = icmp ult i64 %1525, 64
  %1617 = select i1 %shft.chk374, i64 %1616, i64 0
  %1618 = trunc i64 %1617 to i32
  %1619 = trunc i64 %1564 to i32
  %1620 = xor i32 %1618, %1619
  %region_0_446_constant_157375 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1621 = xor i32 %1620, %region_0_446_constant_157375
  %1622 = zext i32 %1621 to i64
  %1623 = mul i64 %1622, %region_0_446_constant_92349
  %1624 = trunc i64 %1623 to i32
  %1625 = xor i32 %1615, %1624
  %region_0_446_constant_222376 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %1626 = xor i32 %1625, %region_0_446_constant_222376
  %1627 = zext i32 %1626 to i64
  %1628 = mul i64 %1627, %region_0_446_constant_64353
  %1629 = lshr i64 %1628, %1525
  %shft.chk377 = icmp ult i64 %1525, 64
  %1630 = select i1 %shft.chk377, i64 %1629, i64 0
  %1631 = trunc i64 %1630 to i32
  %1632 = lshr i64 %1623, %1525
  %shft.chk378 = icmp ult i64 %1525, 64
  %1633 = select i1 %shft.chk378, i64 %1632, i64 0
  %1634 = trunc i64 %1633 to i32
  %1635 = trunc i64 %1580 to i32
  %1636 = xor i32 %1634, %1635
  %region_0_446_constant_175379 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %1637 = xor i32 %1636, %region_0_446_constant_175379
  %1638 = zext i32 %1637 to i64
  %1639 = mul i64 %1638, %region_0_446_constant_64353
  %1640 = trunc i64 %1639 to i32
  %1641 = xor i32 %1631, %1640
  %region_0_446_constant_240380 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1642 = xor i32 %1641, %region_0_446_constant_240380
  %1643 = zext i32 %1642 to i64
  %1644 = mul i64 %1643, %region_0_446_constant_92349
  %1645 = lshr i64 %1644, %1525
  %shft.chk381 = icmp ult i64 %1525, 64
  %1646 = select i1 %shft.chk381, i64 %1645, i64 0
  %1647 = trunc i64 %1646 to i32
  %1648 = lshr i64 %1639, %1525
  %shft.chk382 = icmp ult i64 %1525, 64
  %1649 = select i1 %shft.chk382, i64 %1648, i64 0
  %1650 = trunc i64 %1649 to i32
  %1651 = trunc i64 %1596 to i32
  %1652 = xor i32 %1650, %1651
  %region_0_446_constant_193383 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1653 = xor i32 %1652, %region_0_446_constant_193383
  %1654 = zext i32 %1653 to i64
  %1655 = mul i64 %1654, %region_0_446_constant_92349
  %1656 = trunc i64 %1655 to i32
  %1657 = xor i32 %1647, %1656
  %region_0_446_constant_257384 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %1658 = xor i32 %1657, %region_0_446_constant_257384
  %1659 = zext i32 %1658 to i64
  %1660 = mul i64 %1659, %region_0_446_constant_64353
  %1661 = trunc i64 %1660 to i32
  br label %concatenate.272.merge208

concatenate.pivot.2.385:                          ; preds = %concatenate.272.merge
  %1662 = icmp ult i32 %925, 2
  br i1 %1662, label %concatenate.pivot.1.386, label %concatenate.pivot.3.389

concatenate.pivot.1.386:                          ; preds = %concatenate.pivot.2.385
  %1663 = icmp ult i32 %925, 1
  br i1 %1663, label %concatenate.pivot.0.387, label %concatenate.pivot.1.388

concatenate.pivot.0.387:                          ; preds = %concatenate.pivot.1.386
  br label %concat_index_from_operand_id0209

concatenate.pivot.1.388:                          ; preds = %concatenate.pivot.1.386
  br label %concat_index_from_operand_id1255

concatenate.pivot.3.389:                          ; preds = %concatenate.pivot.2.385
  %1664 = icmp ult i32 %925, 3
  br i1 %1664, label %concatenate.pivot.2.390, label %concatenate.pivot.3.391

concatenate.pivot.2.390:                          ; preds = %concatenate.pivot.3.389
  br label %concat_index_from_operand_id2297

concatenate.pivot.3.391:                          ; preds = %concatenate.pivot.3.389
  br label %concat_index_from_operand_id3343

concatenate.272.merge208:                         ; preds = %concat_index_from_operand_id3343, %concat_index_from_operand_id2297, %concat_index_from_operand_id1255, %concat_index_from_operand_id0209
  %1665 = phi i32 [ %1117, %concat_index_from_operand_id0209 ], [ %1294, %concat_index_from_operand_id1255 ], [ %1484, %concat_index_from_operand_id2297 ], [ %1661, %concat_index_from_operand_id3343 ]
  %region_0_446_constant_273392 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %1666 = lshr i32 %1665, %region_0_446_constant_273392
  %shft.chk393 = icmp ult i32 %region_0_446_constant_273392, 32
  %1667 = select i1 %shft.chk393, i32 %1666, i32 0
  %1668 = uitofp i32 %1667 to float
  %region_0_446_constant_277394 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %1669 = load i32, i32* %5, align 4, !invariant.load !6
  %1670 = sitofp i32 %1669 to float
  %region_0_446_constant_278395 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.279396 = fmul float %1670, %region_0_446_constant_278395
  %region_0_446_constant_280397 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %1671 = fcmp oge float %region_0_446_constant_277394, %multiply.279396
  %1672 = fcmp une float %region_0_446_constant_277394, %region_0_446_constant_277394
  %1673 = or i1 %1671, %1672
  %1674 = select i1 %1673, float %region_0_446_constant_277394, float %multiply.279396
  %1675 = fcmp ole float %region_0_446_constant_280397, %1674
  %1676 = fcmp une float %region_0_446_constant_280397, %region_0_446_constant_280397
  %1677 = or i1 %1675, %1676
  %1678 = select i1 %1677, float %region_0_446_constant_280397, float %1674
  %multiply.282398 = fmul float %1678, %1678
  %region_0_446_constant_283399 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.284400 = fmul float %multiply.282398, %region_0_446_constant_283399
  %region_0_446_constant_285401 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.286402 = fadd float %multiply.284400, %region_0_446_constant_285401
  %multiply.287403 = fmul float %add.286402, %multiply.282398
  %region_0_446_constant_288404 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.289405 = fadd float %multiply.287403, %region_0_446_constant_288404
  %multiply.290406 = fmul float %add.289405, %multiply.282398
  %region_0_446_constant_291407 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.292408 = fadd float %multiply.290406, %region_0_446_constant_291407
  %multiply.293409 = fmul float %add.292408, %multiply.282398
  %region_0_446_constant_294410 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.295411 = fadd float %multiply.293409, %region_0_446_constant_294410
  %multiply.296412 = fmul float %add.295411, %multiply.282398
  %region_0_446_constant_297413 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.298414 = fadd float %multiply.296412, %region_0_446_constant_297413
  %multiply.299415 = fmul float %add.298414, %multiply.282398
  %region_0_446_constant_300416 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.301417 = fadd float %multiply.299415, %region_0_446_constant_300416
  %multiply.302418 = fmul float %add.301417, %multiply.282398
  %region_0_446_constant_303419 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.304420 = fadd float %multiply.302418, %region_0_446_constant_303419
  %multiply.305421 = fmul float %1678, %add.304420
  %region_0_446_constant_306422 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.307423 = fadd float %multiply.284400, %region_0_446_constant_306422
  %multiply.308424 = fmul float %add.307423, %multiply.282398
  %region_0_446_constant_309425 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.310426 = fadd float %multiply.308424, %region_0_446_constant_309425
  %multiply.311427 = fmul float %add.310426, %multiply.282398
  %region_0_446_constant_312428 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.313429 = fadd float %multiply.311427, %region_0_446_constant_312428
  %multiply.314430 = fmul float %add.313429, %multiply.282398
  %region_0_446_constant_315431 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.316432 = fadd float %multiply.314430, %region_0_446_constant_315431
  %multiply.317433 = fmul float %add.316432, %multiply.282398
  %region_0_446_constant_318434 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.319435 = fadd float %multiply.317433, %region_0_446_constant_318434
  %divide.320436 = fdiv float %multiply.305421, %add.319435
  %region_0_446_constant_277437 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %1679 = load i32, i32* %3, align 4, !invariant.load !6
  %1680 = sitofp i32 %1679 to float
  %region_0_446_constant_278438 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.321439 = fmul float %1680, %region_0_446_constant_278438
  %region_0_446_constant_280440 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %1681 = fcmp oge float %region_0_446_constant_277437, %multiply.321439
  %1682 = fcmp une float %region_0_446_constant_277437, %region_0_446_constant_277437
  %1683 = or i1 %1681, %1682
  %1684 = select i1 %1683, float %region_0_446_constant_277437, float %multiply.321439
  %1685 = fcmp ole float %region_0_446_constant_280440, %1684
  %1686 = fcmp une float %region_0_446_constant_280440, %region_0_446_constant_280440
  %1687 = or i1 %1685, %1686
  %1688 = select i1 %1687, float %region_0_446_constant_280440, float %1684
  %multiply.323441 = fmul float %1688, %1688
  %region_0_446_constant_283442 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.324443 = fmul float %multiply.323441, %region_0_446_constant_283442
  %region_0_446_constant_285444 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.325445 = fadd float %multiply.324443, %region_0_446_constant_285444
  %multiply.326446 = fmul float %add.325445, %multiply.323441
  %region_0_446_constant_288447 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.327448 = fadd float %multiply.326446, %region_0_446_constant_288447
  %multiply.328449 = fmul float %add.327448, %multiply.323441
  %region_0_446_constant_291450 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.329451 = fadd float %multiply.328449, %region_0_446_constant_291450
  %multiply.330452 = fmul float %add.329451, %multiply.323441
  %region_0_446_constant_294453 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.331454 = fadd float %multiply.330452, %region_0_446_constant_294453
  %multiply.332455 = fmul float %add.331454, %multiply.323441
  %region_0_446_constant_297456 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.333457 = fadd float %multiply.332455, %region_0_446_constant_297456
  %multiply.334458 = fmul float %add.333457, %multiply.323441
  %region_0_446_constant_300459 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.335460 = fadd float %multiply.334458, %region_0_446_constant_300459
  %multiply.336461 = fmul float %add.335460, %multiply.323441
  %region_0_446_constant_303462 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.337463 = fadd float %multiply.336461, %region_0_446_constant_303462
  %multiply.338464 = fmul float %1688, %add.337463
  %region_0_446_constant_306465 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.339466 = fadd float %multiply.324443, %region_0_446_constant_306465
  %multiply.340467 = fmul float %add.339466, %multiply.323441
  %region_0_446_constant_309468 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.341469 = fadd float %multiply.340467, %region_0_446_constant_309468
  %multiply.342470 = fmul float %add.341469, %multiply.323441
  %region_0_446_constant_312471 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.343472 = fadd float %multiply.342470, %region_0_446_constant_312471
  %multiply.344473 = fmul float %add.343472, %multiply.323441
  %region_0_446_constant_315474 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.345475 = fadd float %multiply.344473, %region_0_446_constant_315474
  %multiply.346476 = fmul float %add.345475, %multiply.323441
  %region_0_446_constant_318477 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.347478 = fadd float %multiply.346476, %region_0_446_constant_318477
  %divide.348479 = fdiv float %multiply.338464, %add.347478
  %subtract.349480 = fsub float %divide.320436, %divide.348479
  %region_0_446_constant_350481 = load float, float* bitcast ([4 x i8]* @17 to float*), align 4
  %multiply.351482 = fmul float %subtract.349480, %region_0_446_constant_350481
  %multiply.353483 = fmul float %1668, %multiply.351482
  %add.355484 = fadd float %multiply.353483, %divide.348479
  %1689 = call float @llvm.fabs.f32(float %add.355484)
  %region_0_446_constant_358485 = load float, float* bitcast ([4 x i8]* @64 to float*), align 4
  %compare.360486 = fcmp oeq float %1689, %region_0_446_constant_358485
  %1690 = zext i1 %compare.360486 to i8
  %region_0_446_constant_34487 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %multiply.362488 = fmul float %add.355484, %region_0_446_constant_34487
  %1691 = fneg float %add.355484
  %multiply.364489 = fmul float %1691, %add.355484
  %1692 = call float @__nv_log1pf(float %multiply.364489)
  %1693 = fneg float %1692
  %region_0_446_constant_367490 = load float, float* bitcast ([4 x i8]* @44 to float*), align 4
  %compare.369491 = fcmp olt float %1693, %region_0_446_constant_367490
  %1694 = zext i1 %compare.369491 to i8
  %region_0_446_constant_370492 = load float, float* bitcast ([4 x i8]* @62 to float*), align 4
  %region_0_446_constant_372493 = load float, float* bitcast ([4 x i8]* @61 to float*), align 4
  %1695 = trunc i8 %1694 to i1
  %1696 = select i1 %1695, float %region_0_446_constant_370492, float %region_0_446_constant_372493
  %region_0_446_constant_375494 = load float, float* bitcast ([4 x i8]* @60 to float*), align 4
  %region_0_446_constant_377495 = load float, float* bitcast ([4 x i8]* @59 to float*), align 4
  %1697 = trunc i8 %1694 to i1
  %1698 = select i1 %1697, float %region_0_446_constant_375494, float %region_0_446_constant_377495
  %region_0_446_constant_380496 = load float, float* bitcast ([4 x i8]* @58 to float*), align 4
  %region_0_446_constant_382497 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %1699 = trunc i8 %1694 to i1
  %1700 = select i1 %1699, float %region_0_446_constant_380496, float %region_0_446_constant_382497
  %region_0_446_constant_385498 = load float, float* bitcast ([4 x i8]* @56 to float*), align 4
  %region_0_446_constant_387499 = load float, float* bitcast ([4 x i8]* @55 to float*), align 4
  %1701 = trunc i8 %1694 to i1
  %1702 = select i1 %1701, float %region_0_446_constant_385498, float %region_0_446_constant_387499
  %region_0_446_constant_390500 = load float, float* bitcast ([4 x i8]* @54 to float*), align 4
  %region_0_446_constant_392501 = load float, float* bitcast ([4 x i8]* @53 to float*), align 4
  %1703 = trunc i8 %1694 to i1
  %1704 = select i1 %1703, float %region_0_446_constant_390500, float %region_0_446_constant_392501
  %region_0_446_constant_395502 = load float, float* bitcast ([4 x i8]* @52 to float*), align 4
  %region_0_446_constant_397503 = load float, float* bitcast ([4 x i8]* @51 to float*), align 4
  %1705 = trunc i8 %1694 to i1
  %1706 = select i1 %1705, float %region_0_446_constant_395502, float %region_0_446_constant_397503
  %region_0_446_constant_400504 = load float, float* bitcast ([4 x i8]* @50 to float*), align 4
  %region_0_446_constant_402505 = load float, float* bitcast ([4 x i8]* @49 to float*), align 4
  %1707 = trunc i8 %1694 to i1
  %1708 = select i1 %1707, float %region_0_446_constant_400504, float %region_0_446_constant_402505
  %region_0_446_constant_405506 = load float, float* bitcast ([4 x i8]* @48 to float*), align 4
  %region_0_446_constant_407507 = load float, float* bitcast ([4 x i8]* @47 to float*), align 4
  %1709 = trunc i8 %1694 to i1
  %1710 = select i1 %1709, float %region_0_446_constant_405506, float %region_0_446_constant_407507
  %region_0_446_constant_410508 = load float, float* bitcast ([4 x i8]* @46 to float*), align 4
  %region_0_446_constant_412509 = load float, float* bitcast ([4 x i8]* @45 to float*), align 4
  %1711 = trunc i8 %1694 to i1
  %1712 = select i1 %1711, float %region_0_446_constant_410508, float %region_0_446_constant_412509
  %region_0_446_constant_415510 = load float, float* bitcast ([4 x i8]* @43 to float*), align 4
  %add.417511 = fadd float %1693, %region_0_446_constant_415510
  %1713 = call float @__nv_sqrtf(float %1693)
  %region_0_446_constant_419512 = load float, float* bitcast ([4 x i8]* @42 to float*), align 4
  %add.421513 = fadd float %1713, %region_0_446_constant_419512
  %1714 = trunc i8 %1694 to i1
  %1715 = select i1 %1714, float %add.417511, float %add.421513
  %multiply.423514 = fmul float %1712, %1715
  %add.424515 = fadd float %1710, %multiply.423514
  %multiply.425516 = fmul float %add.424515, %1715
  %add.426517 = fadd float %1708, %multiply.425516
  %multiply.427518 = fmul float %add.426517, %1715
  %add.428519 = fadd float %1706, %multiply.427518
  %multiply.429520 = fmul float %add.428519, %1715
  %add.430521 = fadd float %1704, %multiply.429520
  %multiply.431522 = fmul float %add.430521, %1715
  %add.432523 = fadd float %1702, %multiply.431522
  %multiply.433524 = fmul float %add.432523, %1715
  %add.434525 = fadd float %1700, %multiply.433524
  %multiply.435526 = fmul float %add.434525, %1715
  %add.436527 = fadd float %1698, %multiply.435526
  %multiply.437528 = fmul float %add.436527, %1715
  %add.438529 = fadd float %1696, %multiply.437528
  %multiply.439530 = fmul float %add.438529, %add.355484
  %1716 = trunc i8 %1690 to i1
  %1717 = select i1 %1716, float %multiply.362488, float %multiply.439530
  %region_0_446_constant_441531 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %multiply.443532 = fmul float %1717, %region_0_446_constant_441531
  %1718 = fcmp oge float %920, %multiply.443532
  %1719 = fcmp une float %920, %920
  %1720 = or i1 %1718, %1719
  %maximum.444533 = select i1 %1720, float %920, float %multiply.443532
  %1721 = fcmp ole float %898, %maximum.444533
  %1722 = fcmp une float %898, %898
  %1723 = or i1 %1721, %1722
  %minimum.445534 = select i1 %1723, float %898, float %maximum.444533
  %1724 = bitcast [16 x [16 x float]]* %7 to float*
  %1725 = getelementptr inbounds float, float* %1724, i32 %linear_index1
  store float %minimum.445534, float* %1725, align 4
  %compare.6535 = fcmp une float %1670, %1670
  %1726 = zext i1 %compare.6535 to i8
  %region_0_446_constant_7536 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_8537 = load float, float* bitcast ([4 x i8]* @74 to float*), align 4
  %compare.9538 = fcmp oeq float %1670, %region_0_446_constant_8537
  %1727 = zext i1 %compare.9538 to i8
  %region_0_446_constant_10539 = load i32, i32* bitcast ([4 x i8]* @73 to i32*), align 4
  %1728 = bitcast float %1670 to i32
  %region_0_446_constant_12540 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %1729 = and i32 %1728, %region_0_446_constant_12540
  %region_0_446_constant_14541 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %1730 = icmp eq i32 %1729, %region_0_446_constant_14541
  %1731 = zext i1 %1730 to i8
  %region_0_446_constant_16542 = load i32, i32* bitcast ([4 x i8]* @72 to i32*), align 4
  %region_0_446_constant_17543 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %1732 = icmp sgt i32 %1729, %region_0_446_constant_17543
  %1733 = zext i1 %1732 to i8
  %region_0_446_constant_19544 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %1734 = and i32 %1728, %region_0_446_constant_19544
  %region_0_446_constant_19545 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %1735 = icmp ne i32 %1734, %region_0_446_constant_19545
  %1736 = zext i1 %1735 to i8
  %1737 = or i8 %1733, %1736
  %region_0_446_constant_23546 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24547 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %1738 = trunc i8 %1737 to i1
  %1739 = select i1 %1738, i32 %region_0_446_constant_23546, i32 %region_0_446_constant_24547
  %1740 = add i32 %1728, %1739
  %1741 = trunc i8 %1731 to i1
  %1742 = select i1 %1741, i32 %region_0_446_constant_16542, i32 %1740
  %1743 = trunc i8 %1727 to i1
  %1744 = select i1 %1743, i32 %region_0_446_constant_10539, i32 %1742
  %1745 = trunc i8 %1726 to i1
  %1746 = select i1 %1745, i32 %region_0_446_constant_7536, i32 %1744
  %1747 = bitcast i32 %1746 to float
  %compare.33548 = fcmp une float %1680, %1680
  %1748 = zext i1 %compare.33548 to i8
  %region_0_446_constant_7549 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_34550 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %compare.35551 = fcmp oeq float %1680, %region_0_446_constant_34550
  %1749 = zext i1 %compare.35551 to i8
  %region_0_446_constant_17552 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %1750 = bitcast float %1680 to i32
  %region_0_446_constant_12553 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %1751 = and i32 %1750, %region_0_446_constant_12553
  %region_0_446_constant_14554 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %1752 = icmp eq i32 %1751, %region_0_446_constant_14554
  %1753 = zext i1 %1752 to i8
  %region_0_446_constant_24555 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %region_0_446_constant_17556 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %1754 = icmp sgt i32 %1751, %region_0_446_constant_17556
  %1755 = zext i1 %1754 to i8
  %region_0_446_constant_19557 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %1756 = and i32 %1750, %region_0_446_constant_19557
  %region_0_446_constant_14558 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %1757 = icmp ne i32 %1756, %region_0_446_constant_14558
  %1758 = zext i1 %1757 to i8
  %1759 = or i8 %1755, %1758
  %region_0_446_constant_23559 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24560 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %1760 = trunc i8 %1759 to i1
  %1761 = select i1 %1760, i32 %region_0_446_constant_23559, i32 %region_0_446_constant_24560
  %1762 = add i32 %1750, %1761
  %1763 = trunc i8 %1753 to i1
  %1764 = select i1 %1763, i32 %region_0_446_constant_24555, i32 %1762
  %1765 = trunc i8 %1749 to i1
  %1766 = select i1 %1765, i32 %region_0_446_constant_17552, i32 %1764
  %1767 = trunc i8 %1748 to i1
  %1768 = select i1 %1767, i32 %region_0_446_constant_7549, i32 %1766
  %1769 = bitcast i32 %1768 to float
  %1770 = mul nuw nsw i32 %18, 1
  %1771 = add nuw nsw i32 0, %1770
  %1772 = mul nuw nsw i32 %19, 16
  %1773 = add nuw nsw i32 %1771, %1772
  %1774 = urem i32 %1773, 4
  %1775 = udiv i32 %1773, 4
  %1776 = udiv i32 %1775, 64
  br label %concatenate.pivot.2.738

concat_index_from_operand_id0562:                 ; preds = %concatenate.pivot.0.740
  %1777 = phi i32 [ 0, %concatenate.pivot.0.740 ]
  %1778 = sub nsw i32 %1774, %1777
  %1779 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %1780 = load i64, i64* %1779, align 8, !invariant.load !6
  %1781 = trunc i64 %1780 to i32
  %1782 = zext i32 %1781 to i64
  %1783 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1784 = lshr i64 %1780, %1783
  %shft.chk563 = icmp ult i64 %1783, 64
  %1785 = select i1 %shft.chk563, i64 %1784, i64 0
  %1786 = trunc i64 %1785 to i32
  %1787 = zext i32 %1786 to i64
  %1788 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1789 = shl i64 %1787, %1788
  %shft.chk564 = icmp ult i64 %1788, 64
  %1790 = select i1 %shft.chk564, i64 %1789, i64 0
  %1791 = or i64 %1782, %1790
  %1792 = mul nuw nsw i32 %1775, 1
  %1793 = add nuw nsw i32 0, %1792
  %1794 = zext i32 %1793 to i64
  %1795 = add i64 %1791, %1794
  %1796 = trunc i64 %1795 to i32
  %1797 = zext i32 %1796 to i64
  %region_0_446_constant_64565 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %1798 = mul i64 %1797, %region_0_446_constant_64565
  %1799 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1800 = lshr i64 %1798, %1799
  %shft.chk566 = icmp ult i64 %1799, 64
  %1801 = select i1 %shft.chk566, i64 %1800, i64 0
  %1802 = trunc i64 %1801 to i32
  %1803 = icmp ult i64 %1795, %1791
  %1804 = zext i1 %1803 to i8
  %1805 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %1806 = load i64, i64* %1805, align 8, !invariant.load !6
  %1807 = trunc i64 %1806 to i32
  %1808 = zext i32 %1807 to i64
  %1809 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1810 = lshr i64 %1806, %1809
  %shft.chk567 = icmp ult i64 %1809, 64
  %1811 = select i1 %shft.chk567, i64 %1810, i64 0
  %1812 = trunc i64 %1811 to i32
  %1813 = zext i32 %1812 to i64
  %1814 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1815 = shl i64 %1813, %1814
  %shft.chk568 = icmp ult i64 %1814, 64
  %1816 = select i1 %shft.chk568, i64 %1815, i64 0
  %1817 = or i64 %1808, %1816
  %region_0_446_constant_80569 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %1818 = add i64 %1817, %region_0_446_constant_80569
  %1819 = trunc i8 %1804 to i1
  %1820 = select i1 %1819, i64 %1818, i64 %1817
  %1821 = lshr i64 %1820, %1799
  %shft.chk570 = icmp ult i64 %1799, 64
  %1822 = select i1 %shft.chk570, i64 %1821, i64 0
  %1823 = trunc i64 %1822 to i32
  %1824 = xor i32 %1802, %1823
  %region_0_446_constant_88571 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %1825 = xor i32 %1824, %region_0_446_constant_88571
  %1826 = zext i32 %1825 to i64
  %region_0_446_constant_92572 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %1827 = mul i64 %1826, %region_0_446_constant_92572
  %1828 = lshr i64 %1827, %1799
  %shft.chk573 = icmp ult i64 %1799, 64
  %1829 = select i1 %shft.chk573, i64 %1828, i64 0
  %1830 = trunc i64 %1829 to i32
  %1831 = trunc i64 %1820 to i32
  %1832 = zext i32 %1831 to i64
  %1833 = mul i64 %1832, %region_0_446_constant_92572
  %1834 = trunc i64 %1833 to i32
  %1835 = xor i32 %1830, %1834
  %region_0_446_constant_102574 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %1836 = xor i32 %1835, %region_0_446_constant_102574
  %1837 = zext i32 %1836 to i64
  %1838 = mul i64 %1837, %region_0_446_constant_64565
  %1839 = lshr i64 %1838, %1799
  %shft.chk575 = icmp ult i64 %1799, 64
  %1840 = select i1 %shft.chk575, i64 %1839, i64 0
  %1841 = trunc i64 %1840 to i32
  %1842 = lshr i64 %1833, %1799
  %shft.chk576 = icmp ult i64 %1799, 64
  %1843 = select i1 %shft.chk576, i64 %1842, i64 0
  %1844 = trunc i64 %1843 to i32
  %1845 = lshr i64 %1795, %1799
  %shft.chk577 = icmp ult i64 %1799, 64
  %1846 = select i1 %shft.chk577, i64 %1845, i64 0
  %1847 = trunc i64 %1846 to i32
  %1848 = xor i32 %1844, %1847
  %region_0_446_constant_114578 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %1849 = xor i32 %1848, %region_0_446_constant_114578
  %1850 = zext i32 %1849 to i64
  %1851 = mul i64 %1850, %region_0_446_constant_64565
  %1852 = trunc i64 %1851 to i32
  %1853 = xor i32 %1841, %1852
  %region_0_446_constant_121579 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %1854 = xor i32 %1853, %region_0_446_constant_121579
  %1855 = zext i32 %1854 to i64
  %1856 = mul i64 %1855, %region_0_446_constant_92572
  %1857 = lshr i64 %1856, %1799
  %shft.chk580 = icmp ult i64 %1799, 64
  %1858 = select i1 %shft.chk580, i64 %1857, i64 0
  %1859 = trunc i64 %1858 to i32
  %1860 = lshr i64 %1851, %1799
  %shft.chk581 = icmp ult i64 %1799, 64
  %1861 = select i1 %shft.chk581, i64 %1860, i64 0
  %1862 = trunc i64 %1861 to i32
  %1863 = trunc i64 %1798 to i32
  %1864 = xor i32 %1862, %1863
  %region_0_446_constant_132582 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %1865 = xor i32 %1864, %region_0_446_constant_132582
  %1866 = zext i32 %1865 to i64
  %1867 = mul i64 %1866, %region_0_446_constant_92572
  %1868 = trunc i64 %1867 to i32
  %1869 = xor i32 %1859, %1868
  %region_0_446_constant_139583 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %1870 = xor i32 %1869, %region_0_446_constant_139583
  %1871 = zext i32 %1870 to i64
  %1872 = mul i64 %1871, %region_0_446_constant_64565
  %1873 = lshr i64 %1872, %1799
  %shft.chk584 = icmp ult i64 %1799, 64
  %1874 = select i1 %shft.chk584, i64 %1873, i64 0
  %1875 = trunc i64 %1874 to i32
  %1876 = lshr i64 %1867, %1799
  %shft.chk585 = icmp ult i64 %1799, 64
  %1877 = select i1 %shft.chk585, i64 %1876, i64 0
  %1878 = trunc i64 %1877 to i32
  %1879 = trunc i64 %1827 to i32
  %1880 = xor i32 %1878, %1879
  %region_0_446_constant_150586 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %1881 = xor i32 %1880, %region_0_446_constant_150586
  %1882 = zext i32 %1881 to i64
  %1883 = mul i64 %1882, %region_0_446_constant_64565
  %1884 = trunc i64 %1883 to i32
  %1885 = xor i32 %1875, %1884
  %region_0_446_constant_157587 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %1886 = xor i32 %1885, %region_0_446_constant_157587
  %1887 = zext i32 %1886 to i64
  %1888 = mul i64 %1887, %region_0_446_constant_92572
  %1889 = lshr i64 %1888, %1799
  %shft.chk588 = icmp ult i64 %1799, 64
  %1890 = select i1 %shft.chk588, i64 %1889, i64 0
  %1891 = trunc i64 %1890 to i32
  %1892 = lshr i64 %1883, %1799
  %shft.chk589 = icmp ult i64 %1799, 64
  %1893 = select i1 %shft.chk589, i64 %1892, i64 0
  %1894 = trunc i64 %1893 to i32
  %1895 = trunc i64 %1838 to i32
  %1896 = xor i32 %1894, %1895
  %region_0_446_constant_168590 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %1897 = xor i32 %1896, %region_0_446_constant_168590
  %1898 = zext i32 %1897 to i64
  %1899 = mul i64 %1898, %region_0_446_constant_92572
  %1900 = trunc i64 %1899 to i32
  %1901 = xor i32 %1891, %1900
  %region_0_446_constant_175591 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %1902 = xor i32 %1901, %region_0_446_constant_175591
  %1903 = zext i32 %1902 to i64
  %1904 = mul i64 %1903, %region_0_446_constant_64565
  %1905 = lshr i64 %1904, %1799
  %shft.chk592 = icmp ult i64 %1799, 64
  %1906 = select i1 %shft.chk592, i64 %1905, i64 0
  %1907 = trunc i64 %1906 to i32
  %1908 = lshr i64 %1899, %1799
  %shft.chk593 = icmp ult i64 %1799, 64
  %1909 = select i1 %shft.chk593, i64 %1908, i64 0
  %1910 = trunc i64 %1909 to i32
  %1911 = trunc i64 %1856 to i32
  %1912 = xor i32 %1910, %1911
  %region_0_446_constant_186594 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %1913 = xor i32 %1912, %region_0_446_constant_186594
  %1914 = zext i32 %1913 to i64
  %1915 = mul i64 %1914, %region_0_446_constant_64565
  %1916 = trunc i64 %1915 to i32
  %1917 = xor i32 %1907, %1916
  %region_0_446_constant_193595 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %1918 = xor i32 %1917, %region_0_446_constant_193595
  %1919 = zext i32 %1918 to i64
  %1920 = mul i64 %1919, %region_0_446_constant_92572
  %1921 = lshr i64 %1920, %1799
  %shft.chk596 = icmp ult i64 %1799, 64
  %1922 = select i1 %shft.chk596, i64 %1921, i64 0
  %1923 = trunc i64 %1922 to i32
  %1924 = lshr i64 %1915, %1799
  %shft.chk597 = icmp ult i64 %1799, 64
  %1925 = select i1 %shft.chk597, i64 %1924, i64 0
  %1926 = trunc i64 %1925 to i32
  %1927 = trunc i64 %1872 to i32
  %1928 = xor i32 %1926, %1927
  %region_0_446_constant_204598 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %1929 = xor i32 %1928, %region_0_446_constant_204598
  %1930 = zext i32 %1929 to i64
  %1931 = mul i64 %1930, %region_0_446_constant_92572
  %1932 = trunc i64 %1931 to i32
  %1933 = xor i32 %1923, %1932
  %region_0_446_constant_211599 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %1934 = xor i32 %1933, %region_0_446_constant_211599
  %1935 = zext i32 %1934 to i64
  %1936 = mul i64 %1935, %region_0_446_constant_64565
  %1937 = lshr i64 %1936, %1799
  %shft.chk600 = icmp ult i64 %1799, 64
  %1938 = select i1 %shft.chk600, i64 %1937, i64 0
  %1939 = trunc i64 %1938 to i32
  %1940 = lshr i64 %1931, %1799
  %shft.chk601 = icmp ult i64 %1799, 64
  %1941 = select i1 %shft.chk601, i64 %1940, i64 0
  %1942 = trunc i64 %1941 to i32
  %1943 = trunc i64 %1888 to i32
  %1944 = xor i32 %1942, %1943
  %region_0_446_constant_222602 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %1945 = xor i32 %1944, %region_0_446_constant_222602
  %1946 = zext i32 %1945 to i64
  %1947 = mul i64 %1946, %region_0_446_constant_64565
  %1948 = trunc i64 %1947 to i32
  %1949 = xor i32 %1939, %1948
  %region_0_446_constant_229603 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %1950 = xor i32 %1949, %region_0_446_constant_229603
  %1951 = zext i32 %1950 to i64
  %1952 = mul i64 %1951, %region_0_446_constant_92572
  %1953 = lshr i64 %1952, %1799
  %shft.chk604 = icmp ult i64 %1799, 64
  %1954 = select i1 %shft.chk604, i64 %1953, i64 0
  %1955 = trunc i64 %1954 to i32
  %1956 = lshr i64 %1947, %1799
  %shft.chk605 = icmp ult i64 %1799, 64
  %1957 = select i1 %shft.chk605, i64 %1956, i64 0
  %1958 = trunc i64 %1957 to i32
  %1959 = trunc i64 %1904 to i32
  %1960 = xor i32 %1958, %1959
  %region_0_446_constant_240606 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %1961 = xor i32 %1960, %region_0_446_constant_240606
  %1962 = zext i32 %1961 to i64
  %1963 = mul i64 %1962, %region_0_446_constant_92572
  %1964 = trunc i64 %1963 to i32
  %1965 = xor i32 %1955, %1964
  %region_0_446_constant_247607 = load i32, i32* bitcast ([4 x i8]* @41 to i32*), align 4
  %1966 = xor i32 %1965, %region_0_446_constant_247607
  br label %concatenate.272.merge561

concat_index_from_operand_id1608:                 ; preds = %concatenate.pivot.1.741
  %1967 = phi i32 [ 1, %concatenate.pivot.1.741 ]
  %1968 = sub nsw i32 %1774, %1967
  %1969 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %1970 = load i64, i64* %1969, align 8, !invariant.load !6
  %1971 = trunc i64 %1970 to i32
  %1972 = zext i32 %1971 to i64
  %1973 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1974 = lshr i64 %1970, %1973
  %shft.chk609 = icmp ult i64 %1973, 64
  %1975 = select i1 %shft.chk609, i64 %1974, i64 0
  %1976 = trunc i64 %1975 to i32
  %1977 = zext i32 %1976 to i64
  %1978 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1979 = shl i64 %1977, %1978
  %shft.chk610 = icmp ult i64 %1978, 64
  %1980 = select i1 %shft.chk610, i64 %1979, i64 0
  %1981 = or i64 %1972, %1980
  %1982 = mul nuw nsw i32 %1775, 1
  %1983 = add nuw nsw i32 0, %1982
  %1984 = zext i32 %1983 to i64
  %1985 = add i64 %1981, %1984
  %1986 = trunc i64 %1985 to i32
  %1987 = zext i32 %1986 to i64
  %region_0_446_constant_64611 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %1988 = mul i64 %1987, %region_0_446_constant_64611
  %1989 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %1990 = lshr i64 %1988, %1989
  %shft.chk612 = icmp ult i64 %1989, 64
  %1991 = select i1 %shft.chk612, i64 %1990, i64 0
  %1992 = trunc i64 %1991 to i32
  %1993 = icmp ult i64 %1985, %1981
  %1994 = zext i1 %1993 to i8
  %1995 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %1996 = load i64, i64* %1995, align 8, !invariant.load !6
  %1997 = trunc i64 %1996 to i32
  %1998 = zext i32 %1997 to i64
  %1999 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2000 = lshr i64 %1996, %1999
  %shft.chk613 = icmp ult i64 %1999, 64
  %2001 = select i1 %shft.chk613, i64 %2000, i64 0
  %2002 = trunc i64 %2001 to i32
  %2003 = zext i32 %2002 to i64
  %2004 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2005 = shl i64 %2003, %2004
  %shft.chk614 = icmp ult i64 %2004, 64
  %2006 = select i1 %shft.chk614, i64 %2005, i64 0
  %2007 = or i64 %1998, %2006
  %region_0_446_constant_80615 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %2008 = add i64 %2007, %region_0_446_constant_80615
  %2009 = trunc i8 %1994 to i1
  %2010 = select i1 %2009, i64 %2008, i64 %2007
  %2011 = lshr i64 %2010, %1989
  %shft.chk616 = icmp ult i64 %1989, 64
  %2012 = select i1 %shft.chk616, i64 %2011, i64 0
  %2013 = trunc i64 %2012 to i32
  %2014 = xor i32 %1992, %2013
  %region_0_446_constant_88617 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %2015 = xor i32 %2014, %region_0_446_constant_88617
  %2016 = zext i32 %2015 to i64
  %region_0_446_constant_92618 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %2017 = mul i64 %2016, %region_0_446_constant_92618
  %2018 = lshr i64 %2017, %1989
  %shft.chk619 = icmp ult i64 %1989, 64
  %2019 = select i1 %shft.chk619, i64 %2018, i64 0
  %2020 = trunc i64 %2019 to i32
  %2021 = trunc i64 %2010 to i32
  %2022 = zext i32 %2021 to i64
  %2023 = mul i64 %2022, %region_0_446_constant_92618
  %2024 = trunc i64 %2023 to i32
  %2025 = xor i32 %2020, %2024
  %region_0_446_constant_102620 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %2026 = xor i32 %2025, %region_0_446_constant_102620
  %2027 = zext i32 %2026 to i64
  %2028 = mul i64 %2027, %region_0_446_constant_64611
  %2029 = lshr i64 %2028, %1989
  %shft.chk621 = icmp ult i64 %1989, 64
  %2030 = select i1 %shft.chk621, i64 %2029, i64 0
  %2031 = trunc i64 %2030 to i32
  %2032 = lshr i64 %2023, %1989
  %shft.chk622 = icmp ult i64 %1989, 64
  %2033 = select i1 %shft.chk622, i64 %2032, i64 0
  %2034 = trunc i64 %2033 to i32
  %2035 = lshr i64 %1985, %1989
  %shft.chk623 = icmp ult i64 %1989, 64
  %2036 = select i1 %shft.chk623, i64 %2035, i64 0
  %2037 = trunc i64 %2036 to i32
  %2038 = xor i32 %2034, %2037
  %region_0_446_constant_114624 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2039 = xor i32 %2038, %region_0_446_constant_114624
  %2040 = zext i32 %2039 to i64
  %2041 = mul i64 %2040, %region_0_446_constant_64611
  %2042 = trunc i64 %2041 to i32
  %2043 = xor i32 %2031, %2042
  %region_0_446_constant_121625 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2044 = xor i32 %2043, %region_0_446_constant_121625
  %2045 = zext i32 %2044 to i64
  %2046 = mul i64 %2045, %region_0_446_constant_92618
  %2047 = lshr i64 %2046, %1989
  %shft.chk626 = icmp ult i64 %1989, 64
  %2048 = select i1 %shft.chk626, i64 %2047, i64 0
  %2049 = trunc i64 %2048 to i32
  %2050 = lshr i64 %2041, %1989
  %shft.chk627 = icmp ult i64 %1989, 64
  %2051 = select i1 %shft.chk627, i64 %2050, i64 0
  %2052 = trunc i64 %2051 to i32
  %2053 = trunc i64 %1988 to i32
  %2054 = xor i32 %2052, %2053
  %region_0_446_constant_132628 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %2055 = xor i32 %2054, %region_0_446_constant_132628
  %2056 = zext i32 %2055 to i64
  %2057 = mul i64 %2056, %region_0_446_constant_92618
  %2058 = trunc i64 %2057 to i32
  %2059 = xor i32 %2049, %2058
  %region_0_446_constant_139629 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %2060 = xor i32 %2059, %region_0_446_constant_139629
  %2061 = zext i32 %2060 to i64
  %2062 = mul i64 %2061, %region_0_446_constant_64611
  %2063 = lshr i64 %2062, %1989
  %shft.chk630 = icmp ult i64 %1989, 64
  %2064 = select i1 %shft.chk630, i64 %2063, i64 0
  %2065 = trunc i64 %2064 to i32
  %2066 = lshr i64 %2057, %1989
  %shft.chk631 = icmp ult i64 %1989, 64
  %2067 = select i1 %shft.chk631, i64 %2066, i64 0
  %2068 = trunc i64 %2067 to i32
  %2069 = trunc i64 %2017 to i32
  %2070 = xor i32 %2068, %2069
  %region_0_446_constant_150632 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %2071 = xor i32 %2070, %region_0_446_constant_150632
  %2072 = zext i32 %2071 to i64
  %2073 = mul i64 %2072, %region_0_446_constant_64611
  %2074 = trunc i64 %2073 to i32
  %2075 = xor i32 %2065, %2074
  %region_0_446_constant_157633 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2076 = xor i32 %2075, %region_0_446_constant_157633
  %2077 = zext i32 %2076 to i64
  %2078 = mul i64 %2077, %region_0_446_constant_92618
  %2079 = lshr i64 %2078, %1989
  %shft.chk634 = icmp ult i64 %1989, 64
  %2080 = select i1 %shft.chk634, i64 %2079, i64 0
  %2081 = trunc i64 %2080 to i32
  %2082 = lshr i64 %2073, %1989
  %shft.chk635 = icmp ult i64 %1989, 64
  %2083 = select i1 %shft.chk635, i64 %2082, i64 0
  %2084 = trunc i64 %2083 to i32
  %2085 = trunc i64 %2028 to i32
  %2086 = xor i32 %2084, %2085
  %region_0_446_constant_168636 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %2087 = xor i32 %2086, %region_0_446_constant_168636
  %2088 = zext i32 %2087 to i64
  %2089 = mul i64 %2088, %region_0_446_constant_92618
  %2090 = trunc i64 %2089 to i32
  %2091 = xor i32 %2081, %2090
  %region_0_446_constant_175637 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %2092 = xor i32 %2091, %region_0_446_constant_175637
  %2093 = zext i32 %2092 to i64
  %2094 = mul i64 %2093, %region_0_446_constant_64611
  %2095 = lshr i64 %2094, %1989
  %shft.chk638 = icmp ult i64 %1989, 64
  %2096 = select i1 %shft.chk638, i64 %2095, i64 0
  %2097 = trunc i64 %2096 to i32
  %2098 = lshr i64 %2089, %1989
  %shft.chk639 = icmp ult i64 %1989, 64
  %2099 = select i1 %shft.chk639, i64 %2098, i64 0
  %2100 = trunc i64 %2099 to i32
  %2101 = trunc i64 %2046 to i32
  %2102 = xor i32 %2100, %2101
  %region_0_446_constant_186640 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2103 = xor i32 %2102, %region_0_446_constant_186640
  %2104 = zext i32 %2103 to i64
  %2105 = mul i64 %2104, %region_0_446_constant_64611
  %2106 = trunc i64 %2105 to i32
  %2107 = xor i32 %2097, %2106
  %region_0_446_constant_193641 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2108 = xor i32 %2107, %region_0_446_constant_193641
  %2109 = zext i32 %2108 to i64
  %2110 = mul i64 %2109, %region_0_446_constant_92618
  %2111 = lshr i64 %2110, %1989
  %shft.chk642 = icmp ult i64 %1989, 64
  %2112 = select i1 %shft.chk642, i64 %2111, i64 0
  %2113 = trunc i64 %2112 to i32
  %2114 = lshr i64 %2105, %1989
  %shft.chk643 = icmp ult i64 %1989, 64
  %2115 = select i1 %shft.chk643, i64 %2114, i64 0
  %2116 = trunc i64 %2115 to i32
  %2117 = trunc i64 %2062 to i32
  %2118 = xor i32 %2116, %2117
  %region_0_446_constant_204644 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2119 = xor i32 %2118, %region_0_446_constant_204644
  %2120 = zext i32 %2119 to i64
  %2121 = mul i64 %2120, %region_0_446_constant_92618
  %2122 = trunc i64 %2121 to i32
  %2123 = xor i32 %2113, %2122
  %region_0_446_constant_211645 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2124 = xor i32 %2123, %region_0_446_constant_211645
  %2125 = zext i32 %2124 to i64
  %2126 = mul i64 %2125, %region_0_446_constant_64611
  %2127 = lshr i64 %2126, %1989
  %shft.chk646 = icmp ult i64 %1989, 64
  %2128 = select i1 %shft.chk646, i64 %2127, i64 0
  %2129 = trunc i64 %2128 to i32
  %2130 = lshr i64 %2121, %1989
  %shft.chk647 = icmp ult i64 %1989, 64
  %2131 = select i1 %shft.chk647, i64 %2130, i64 0
  %2132 = trunc i64 %2131 to i32
  %2133 = trunc i64 %2078 to i32
  %2134 = xor i32 %2132, %2133
  %region_0_446_constant_222648 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %2135 = xor i32 %2134, %region_0_446_constant_222648
  %2136 = zext i32 %2135 to i64
  %2137 = mul i64 %2136, %region_0_446_constant_64611
  %2138 = trunc i64 %2137 to i32
  %2139 = xor i32 %2129, %2138
  %region_0_446_constant_229649 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2140 = xor i32 %2139, %region_0_446_constant_229649
  %2141 = zext i32 %2140 to i64
  %2142 = mul i64 %2141, %region_0_446_constant_92618
  %2143 = trunc i64 %2142 to i32
  br label %concatenate.272.merge561

concat_index_from_operand_id2650:                 ; preds = %concatenate.pivot.2.743
  %2144 = phi i32 [ 2, %concatenate.pivot.2.743 ]
  %2145 = sub nsw i32 %1774, %2144
  %2146 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %2147 = load i64, i64* %2146, align 8, !invariant.load !6
  %2148 = trunc i64 %2147 to i32
  %2149 = zext i32 %2148 to i64
  %2150 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2151 = lshr i64 %2147, %2150
  %shft.chk651 = icmp ult i64 %2150, 64
  %2152 = select i1 %shft.chk651, i64 %2151, i64 0
  %2153 = trunc i64 %2152 to i32
  %2154 = zext i32 %2153 to i64
  %2155 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2156 = shl i64 %2154, %2155
  %shft.chk652 = icmp ult i64 %2155, 64
  %2157 = select i1 %shft.chk652, i64 %2156, i64 0
  %2158 = or i64 %2149, %2157
  %2159 = mul nuw nsw i32 %1775, 1
  %2160 = add nuw nsw i32 0, %2159
  %2161 = zext i32 %2160 to i64
  %2162 = add i64 %2158, %2161
  %2163 = icmp ult i64 %2162, %2158
  %2164 = zext i1 %2163 to i8
  %2165 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %2166 = load i64, i64* %2165, align 8, !invariant.load !6
  %2167 = trunc i64 %2166 to i32
  %2168 = zext i32 %2167 to i64
  %2169 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2170 = lshr i64 %2166, %2169
  %shft.chk653 = icmp ult i64 %2169, 64
  %2171 = select i1 %shft.chk653, i64 %2170, i64 0
  %2172 = trunc i64 %2171 to i32
  %2173 = zext i32 %2172 to i64
  %2174 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2175 = shl i64 %2173, %2174
  %shft.chk654 = icmp ult i64 %2174, 64
  %2176 = select i1 %shft.chk654, i64 %2175, i64 0
  %2177 = or i64 %2168, %2176
  %region_0_446_constant_80655 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %2178 = add i64 %2177, %region_0_446_constant_80655
  %2179 = trunc i8 %2164 to i1
  %2180 = select i1 %2179, i64 %2178, i64 %2177
  %2181 = trunc i64 %2180 to i32
  %2182 = zext i32 %2181 to i64
  %region_0_446_constant_92656 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %2183 = mul i64 %2182, %region_0_446_constant_92656
  %2184 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2185 = lshr i64 %2183, %2184
  %shft.chk657 = icmp ult i64 %2184, 64
  %2186 = select i1 %shft.chk657, i64 %2185, i64 0
  %2187 = trunc i64 %2186 to i32
  %2188 = lshr i64 %2162, %2184
  %shft.chk658 = icmp ult i64 %2184, 64
  %2189 = select i1 %shft.chk658, i64 %2188, i64 0
  %2190 = trunc i64 %2189 to i32
  %2191 = xor i32 %2187, %2190
  %region_0_446_constant_114659 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2192 = xor i32 %2191, %region_0_446_constant_114659
  %2193 = zext i32 %2192 to i64
  %region_0_446_constant_64660 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %2194 = mul i64 %2193, %region_0_446_constant_64660
  %2195 = lshr i64 %2194, %2184
  %shft.chk661 = icmp ult i64 %2184, 64
  %2196 = select i1 %shft.chk661, i64 %2195, i64 0
  %2197 = trunc i64 %2196 to i32
  %2198 = trunc i64 %2162 to i32
  %2199 = zext i32 %2198 to i64
  %2200 = mul i64 %2199, %region_0_446_constant_64660
  %2201 = trunc i64 %2200 to i32
  %2202 = xor i32 %2197, %2201
  %region_0_446_constant_132662 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %2203 = xor i32 %2202, %region_0_446_constant_132662
  %2204 = zext i32 %2203 to i64
  %2205 = mul i64 %2204, %region_0_446_constant_92656
  %2206 = lshr i64 %2205, %2184
  %shft.chk663 = icmp ult i64 %2184, 64
  %2207 = select i1 %shft.chk663, i64 %2206, i64 0
  %2208 = trunc i64 %2207 to i32
  %2209 = lshr i64 %2200, %2184
  %shft.chk664 = icmp ult i64 %2184, 64
  %2210 = select i1 %shft.chk664, i64 %2209, i64 0
  %2211 = trunc i64 %2210 to i32
  %2212 = lshr i64 %2180, %2184
  %shft.chk665 = icmp ult i64 %2184, 64
  %2213 = select i1 %shft.chk665, i64 %2212, i64 0
  %2214 = trunc i64 %2213 to i32
  %2215 = xor i32 %2211, %2214
  %region_0_446_constant_88666 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %2216 = xor i32 %2215, %region_0_446_constant_88666
  %2217 = zext i32 %2216 to i64
  %2218 = mul i64 %2217, %region_0_446_constant_92656
  %2219 = trunc i64 %2218 to i32
  %2220 = xor i32 %2208, %2219
  %region_0_446_constant_150667 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %2221 = xor i32 %2220, %region_0_446_constant_150667
  %2222 = zext i32 %2221 to i64
  %2223 = mul i64 %2222, %region_0_446_constant_64660
  %2224 = lshr i64 %2223, %2184
  %shft.chk668 = icmp ult i64 %2184, 64
  %2225 = select i1 %shft.chk668, i64 %2224, i64 0
  %2226 = trunc i64 %2225 to i32
  %2227 = lshr i64 %2218, %2184
  %shft.chk669 = icmp ult i64 %2184, 64
  %2228 = select i1 %shft.chk669, i64 %2227, i64 0
  %2229 = trunc i64 %2228 to i32
  %2230 = trunc i64 %2183 to i32
  %2231 = xor i32 %2229, %2230
  %region_0_446_constant_102670 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %2232 = xor i32 %2231, %region_0_446_constant_102670
  %2233 = zext i32 %2232 to i64
  %2234 = mul i64 %2233, %region_0_446_constant_64660
  %2235 = trunc i64 %2234 to i32
  %2236 = xor i32 %2226, %2235
  %region_0_446_constant_168671 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %2237 = xor i32 %2236, %region_0_446_constant_168671
  %2238 = zext i32 %2237 to i64
  %2239 = mul i64 %2238, %region_0_446_constant_92656
  %2240 = lshr i64 %2239, %2184
  %shft.chk672 = icmp ult i64 %2184, 64
  %2241 = select i1 %shft.chk672, i64 %2240, i64 0
  %2242 = trunc i64 %2241 to i32
  %2243 = lshr i64 %2234, %2184
  %shft.chk673 = icmp ult i64 %2184, 64
  %2244 = select i1 %shft.chk673, i64 %2243, i64 0
  %2245 = trunc i64 %2244 to i32
  %2246 = trunc i64 %2194 to i32
  %2247 = xor i32 %2245, %2246
  %region_0_446_constant_121674 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2248 = xor i32 %2247, %region_0_446_constant_121674
  %2249 = zext i32 %2248 to i64
  %2250 = mul i64 %2249, %region_0_446_constant_92656
  %2251 = trunc i64 %2250 to i32
  %2252 = xor i32 %2242, %2251
  %region_0_446_constant_186675 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2253 = xor i32 %2252, %region_0_446_constant_186675
  %2254 = zext i32 %2253 to i64
  %2255 = mul i64 %2254, %region_0_446_constant_64660
  %2256 = lshr i64 %2255, %2184
  %shft.chk676 = icmp ult i64 %2184, 64
  %2257 = select i1 %shft.chk676, i64 %2256, i64 0
  %2258 = trunc i64 %2257 to i32
  %2259 = lshr i64 %2250, %2184
  %shft.chk677 = icmp ult i64 %2184, 64
  %2260 = select i1 %shft.chk677, i64 %2259, i64 0
  %2261 = trunc i64 %2260 to i32
  %2262 = trunc i64 %2205 to i32
  %2263 = xor i32 %2261, %2262
  %region_0_446_constant_139678 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %2264 = xor i32 %2263, %region_0_446_constant_139678
  %2265 = zext i32 %2264 to i64
  %2266 = mul i64 %2265, %region_0_446_constant_64660
  %2267 = trunc i64 %2266 to i32
  %2268 = xor i32 %2258, %2267
  %region_0_446_constant_204679 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2269 = xor i32 %2268, %region_0_446_constant_204679
  %2270 = zext i32 %2269 to i64
  %2271 = mul i64 %2270, %region_0_446_constant_92656
  %2272 = lshr i64 %2271, %2184
  %shft.chk680 = icmp ult i64 %2184, 64
  %2273 = select i1 %shft.chk680, i64 %2272, i64 0
  %2274 = trunc i64 %2273 to i32
  %2275 = lshr i64 %2266, %2184
  %shft.chk681 = icmp ult i64 %2184, 64
  %2276 = select i1 %shft.chk681, i64 %2275, i64 0
  %2277 = trunc i64 %2276 to i32
  %2278 = trunc i64 %2223 to i32
  %2279 = xor i32 %2277, %2278
  %region_0_446_constant_157682 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2280 = xor i32 %2279, %region_0_446_constant_157682
  %2281 = zext i32 %2280 to i64
  %2282 = mul i64 %2281, %region_0_446_constant_92656
  %2283 = trunc i64 %2282 to i32
  %2284 = xor i32 %2274, %2283
  %region_0_446_constant_222683 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %2285 = xor i32 %2284, %region_0_446_constant_222683
  %2286 = zext i32 %2285 to i64
  %2287 = mul i64 %2286, %region_0_446_constant_64660
  %2288 = lshr i64 %2287, %2184
  %shft.chk684 = icmp ult i64 %2184, 64
  %2289 = select i1 %shft.chk684, i64 %2288, i64 0
  %2290 = trunc i64 %2289 to i32
  %2291 = lshr i64 %2282, %2184
  %shft.chk685 = icmp ult i64 %2184, 64
  %2292 = select i1 %shft.chk685, i64 %2291, i64 0
  %2293 = trunc i64 %2292 to i32
  %2294 = trunc i64 %2239 to i32
  %2295 = xor i32 %2293, %2294
  %region_0_446_constant_175686 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %2296 = xor i32 %2295, %region_0_446_constant_175686
  %2297 = zext i32 %2296 to i64
  %2298 = mul i64 %2297, %region_0_446_constant_64660
  %2299 = trunc i64 %2298 to i32
  %2300 = xor i32 %2290, %2299
  %region_0_446_constant_240687 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2301 = xor i32 %2300, %region_0_446_constant_240687
  %2302 = zext i32 %2301 to i64
  %2303 = mul i64 %2302, %region_0_446_constant_92656
  %2304 = lshr i64 %2303, %2184
  %shft.chk688 = icmp ult i64 %2184, 64
  %2305 = select i1 %shft.chk688, i64 %2304, i64 0
  %2306 = trunc i64 %2305 to i32
  %2307 = lshr i64 %2298, %2184
  %shft.chk689 = icmp ult i64 %2184, 64
  %2308 = select i1 %shft.chk689, i64 %2307, i64 0
  %2309 = trunc i64 %2308 to i32
  %2310 = trunc i64 %2255 to i32
  %2311 = xor i32 %2309, %2310
  %region_0_446_constant_193690 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2312 = xor i32 %2311, %region_0_446_constant_193690
  %2313 = zext i32 %2312 to i64
  %2314 = mul i64 %2313, %region_0_446_constant_92656
  %2315 = trunc i64 %2314 to i32
  %2316 = xor i32 %2306, %2315
  %region_0_446_constant_257691 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2317 = xor i32 %2316, %region_0_446_constant_257691
  %2318 = zext i32 %2317 to i64
  %2319 = mul i64 %2318, %region_0_446_constant_64660
  %2320 = lshr i64 %2319, %2184
  %shft.chk692 = icmp ult i64 %2184, 64
  %2321 = select i1 %shft.chk692, i64 %2320, i64 0
  %2322 = trunc i64 %2321 to i32
  %2323 = lshr i64 %2314, %2184
  %shft.chk693 = icmp ult i64 %2184, 64
  %2324 = select i1 %shft.chk693, i64 %2323, i64 0
  %2325 = trunc i64 %2324 to i32
  %2326 = trunc i64 %2271 to i32
  %2327 = xor i32 %2325, %2326
  %region_0_446_constant_211694 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2328 = xor i32 %2327, %region_0_446_constant_211694
  %2329 = zext i32 %2328 to i64
  %2330 = mul i64 %2329, %region_0_446_constant_64660
  %2331 = trunc i64 %2330 to i32
  %2332 = xor i32 %2322, %2331
  %region_0_446_constant_266695 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %2333 = xor i32 %2332, %region_0_446_constant_266695
  br label %concatenate.272.merge561

concat_index_from_operand_id3696:                 ; preds = %concatenate.pivot.3.744
  %2334 = phi i32 [ 3, %concatenate.pivot.3.744 ]
  %2335 = sub nsw i32 %1774, %2334
  %2336 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %2337 = load i64, i64* %2336, align 8, !invariant.load !6
  %2338 = trunc i64 %2337 to i32
  %2339 = zext i32 %2338 to i64
  %2340 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2341 = lshr i64 %2337, %2340
  %shft.chk697 = icmp ult i64 %2340, 64
  %2342 = select i1 %shft.chk697, i64 %2341, i64 0
  %2343 = trunc i64 %2342 to i32
  %2344 = zext i32 %2343 to i64
  %2345 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2346 = shl i64 %2344, %2345
  %shft.chk698 = icmp ult i64 %2345, 64
  %2347 = select i1 %shft.chk698, i64 %2346, i64 0
  %2348 = or i64 %2339, %2347
  %2349 = mul nuw nsw i32 %1775, 1
  %2350 = add nuw nsw i32 0, %2349
  %2351 = zext i32 %2350 to i64
  %2352 = add i64 %2348, %2351
  %2353 = icmp ult i64 %2352, %2348
  %2354 = zext i1 %2353 to i8
  %2355 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %2356 = load i64, i64* %2355, align 8, !invariant.load !6
  %2357 = trunc i64 %2356 to i32
  %2358 = zext i32 %2357 to i64
  %2359 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2360 = lshr i64 %2356, %2359
  %shft.chk699 = icmp ult i64 %2359, 64
  %2361 = select i1 %shft.chk699, i64 %2360, i64 0
  %2362 = trunc i64 %2361 to i32
  %2363 = zext i32 %2362 to i64
  %2364 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2365 = shl i64 %2363, %2364
  %shft.chk700 = icmp ult i64 %2364, 64
  %2366 = select i1 %shft.chk700, i64 %2365, i64 0
  %2367 = or i64 %2358, %2366
  %region_0_446_constant_80701 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %2368 = add i64 %2367, %region_0_446_constant_80701
  %2369 = trunc i8 %2354 to i1
  %2370 = select i1 %2369, i64 %2368, i64 %2367
  %2371 = trunc i64 %2370 to i32
  %2372 = zext i32 %2371 to i64
  %region_0_446_constant_92702 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %2373 = mul i64 %2372, %region_0_446_constant_92702
  %2374 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2375 = lshr i64 %2373, %2374
  %shft.chk703 = icmp ult i64 %2374, 64
  %2376 = select i1 %shft.chk703, i64 %2375, i64 0
  %2377 = trunc i64 %2376 to i32
  %2378 = lshr i64 %2352, %2374
  %shft.chk704 = icmp ult i64 %2374, 64
  %2379 = select i1 %shft.chk704, i64 %2378, i64 0
  %2380 = trunc i64 %2379 to i32
  %2381 = xor i32 %2377, %2380
  %region_0_446_constant_114705 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2382 = xor i32 %2381, %region_0_446_constant_114705
  %2383 = zext i32 %2382 to i64
  %region_0_446_constant_64706 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %2384 = mul i64 %2383, %region_0_446_constant_64706
  %2385 = lshr i64 %2384, %2374
  %shft.chk707 = icmp ult i64 %2374, 64
  %2386 = select i1 %shft.chk707, i64 %2385, i64 0
  %2387 = trunc i64 %2386 to i32
  %2388 = trunc i64 %2352 to i32
  %2389 = zext i32 %2388 to i64
  %2390 = mul i64 %2389, %region_0_446_constant_64706
  %2391 = trunc i64 %2390 to i32
  %2392 = xor i32 %2387, %2391
  %region_0_446_constant_132708 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %2393 = xor i32 %2392, %region_0_446_constant_132708
  %2394 = zext i32 %2393 to i64
  %2395 = mul i64 %2394, %region_0_446_constant_92702
  %2396 = lshr i64 %2395, %2374
  %shft.chk709 = icmp ult i64 %2374, 64
  %2397 = select i1 %shft.chk709, i64 %2396, i64 0
  %2398 = trunc i64 %2397 to i32
  %2399 = lshr i64 %2390, %2374
  %shft.chk710 = icmp ult i64 %2374, 64
  %2400 = select i1 %shft.chk710, i64 %2399, i64 0
  %2401 = trunc i64 %2400 to i32
  %2402 = lshr i64 %2370, %2374
  %shft.chk711 = icmp ult i64 %2374, 64
  %2403 = select i1 %shft.chk711, i64 %2402, i64 0
  %2404 = trunc i64 %2403 to i32
  %2405 = xor i32 %2401, %2404
  %region_0_446_constant_88712 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %2406 = xor i32 %2405, %region_0_446_constant_88712
  %2407 = zext i32 %2406 to i64
  %2408 = mul i64 %2407, %region_0_446_constant_92702
  %2409 = trunc i64 %2408 to i32
  %2410 = xor i32 %2398, %2409
  %region_0_446_constant_150713 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %2411 = xor i32 %2410, %region_0_446_constant_150713
  %2412 = zext i32 %2411 to i64
  %2413 = mul i64 %2412, %region_0_446_constant_64706
  %2414 = lshr i64 %2413, %2374
  %shft.chk714 = icmp ult i64 %2374, 64
  %2415 = select i1 %shft.chk714, i64 %2414, i64 0
  %2416 = trunc i64 %2415 to i32
  %2417 = lshr i64 %2408, %2374
  %shft.chk715 = icmp ult i64 %2374, 64
  %2418 = select i1 %shft.chk715, i64 %2417, i64 0
  %2419 = trunc i64 %2418 to i32
  %2420 = trunc i64 %2373 to i32
  %2421 = xor i32 %2419, %2420
  %region_0_446_constant_102716 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %2422 = xor i32 %2421, %region_0_446_constant_102716
  %2423 = zext i32 %2422 to i64
  %2424 = mul i64 %2423, %region_0_446_constant_64706
  %2425 = trunc i64 %2424 to i32
  %2426 = xor i32 %2416, %2425
  %region_0_446_constant_168717 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %2427 = xor i32 %2426, %region_0_446_constant_168717
  %2428 = zext i32 %2427 to i64
  %2429 = mul i64 %2428, %region_0_446_constant_92702
  %2430 = lshr i64 %2429, %2374
  %shft.chk718 = icmp ult i64 %2374, 64
  %2431 = select i1 %shft.chk718, i64 %2430, i64 0
  %2432 = trunc i64 %2431 to i32
  %2433 = lshr i64 %2424, %2374
  %shft.chk719 = icmp ult i64 %2374, 64
  %2434 = select i1 %shft.chk719, i64 %2433, i64 0
  %2435 = trunc i64 %2434 to i32
  %2436 = trunc i64 %2384 to i32
  %2437 = xor i32 %2435, %2436
  %region_0_446_constant_121720 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2438 = xor i32 %2437, %region_0_446_constant_121720
  %2439 = zext i32 %2438 to i64
  %2440 = mul i64 %2439, %region_0_446_constant_92702
  %2441 = trunc i64 %2440 to i32
  %2442 = xor i32 %2432, %2441
  %region_0_446_constant_186721 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2443 = xor i32 %2442, %region_0_446_constant_186721
  %2444 = zext i32 %2443 to i64
  %2445 = mul i64 %2444, %region_0_446_constant_64706
  %2446 = lshr i64 %2445, %2374
  %shft.chk722 = icmp ult i64 %2374, 64
  %2447 = select i1 %shft.chk722, i64 %2446, i64 0
  %2448 = trunc i64 %2447 to i32
  %2449 = lshr i64 %2440, %2374
  %shft.chk723 = icmp ult i64 %2374, 64
  %2450 = select i1 %shft.chk723, i64 %2449, i64 0
  %2451 = trunc i64 %2450 to i32
  %2452 = trunc i64 %2395 to i32
  %2453 = xor i32 %2451, %2452
  %region_0_446_constant_139724 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %2454 = xor i32 %2453, %region_0_446_constant_139724
  %2455 = zext i32 %2454 to i64
  %2456 = mul i64 %2455, %region_0_446_constant_64706
  %2457 = trunc i64 %2456 to i32
  %2458 = xor i32 %2448, %2457
  %region_0_446_constant_204725 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2459 = xor i32 %2458, %region_0_446_constant_204725
  %2460 = zext i32 %2459 to i64
  %2461 = mul i64 %2460, %region_0_446_constant_92702
  %2462 = lshr i64 %2461, %2374
  %shft.chk726 = icmp ult i64 %2374, 64
  %2463 = select i1 %shft.chk726, i64 %2462, i64 0
  %2464 = trunc i64 %2463 to i32
  %2465 = lshr i64 %2456, %2374
  %shft.chk727 = icmp ult i64 %2374, 64
  %2466 = select i1 %shft.chk727, i64 %2465, i64 0
  %2467 = trunc i64 %2466 to i32
  %2468 = trunc i64 %2413 to i32
  %2469 = xor i32 %2467, %2468
  %region_0_446_constant_157728 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2470 = xor i32 %2469, %region_0_446_constant_157728
  %2471 = zext i32 %2470 to i64
  %2472 = mul i64 %2471, %region_0_446_constant_92702
  %2473 = trunc i64 %2472 to i32
  %2474 = xor i32 %2464, %2473
  %region_0_446_constant_222729 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %2475 = xor i32 %2474, %region_0_446_constant_222729
  %2476 = zext i32 %2475 to i64
  %2477 = mul i64 %2476, %region_0_446_constant_64706
  %2478 = lshr i64 %2477, %2374
  %shft.chk730 = icmp ult i64 %2374, 64
  %2479 = select i1 %shft.chk730, i64 %2478, i64 0
  %2480 = trunc i64 %2479 to i32
  %2481 = lshr i64 %2472, %2374
  %shft.chk731 = icmp ult i64 %2374, 64
  %2482 = select i1 %shft.chk731, i64 %2481, i64 0
  %2483 = trunc i64 %2482 to i32
  %2484 = trunc i64 %2429 to i32
  %2485 = xor i32 %2483, %2484
  %region_0_446_constant_175732 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %2486 = xor i32 %2485, %region_0_446_constant_175732
  %2487 = zext i32 %2486 to i64
  %2488 = mul i64 %2487, %region_0_446_constant_64706
  %2489 = trunc i64 %2488 to i32
  %2490 = xor i32 %2480, %2489
  %region_0_446_constant_240733 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2491 = xor i32 %2490, %region_0_446_constant_240733
  %2492 = zext i32 %2491 to i64
  %2493 = mul i64 %2492, %region_0_446_constant_92702
  %2494 = lshr i64 %2493, %2374
  %shft.chk734 = icmp ult i64 %2374, 64
  %2495 = select i1 %shft.chk734, i64 %2494, i64 0
  %2496 = trunc i64 %2495 to i32
  %2497 = lshr i64 %2488, %2374
  %shft.chk735 = icmp ult i64 %2374, 64
  %2498 = select i1 %shft.chk735, i64 %2497, i64 0
  %2499 = trunc i64 %2498 to i32
  %2500 = trunc i64 %2445 to i32
  %2501 = xor i32 %2499, %2500
  %region_0_446_constant_193736 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2502 = xor i32 %2501, %region_0_446_constant_193736
  %2503 = zext i32 %2502 to i64
  %2504 = mul i64 %2503, %region_0_446_constant_92702
  %2505 = trunc i64 %2504 to i32
  %2506 = xor i32 %2496, %2505
  %region_0_446_constant_257737 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %2507 = xor i32 %2506, %region_0_446_constant_257737
  %2508 = zext i32 %2507 to i64
  %2509 = mul i64 %2508, %region_0_446_constant_64706
  %2510 = trunc i64 %2509 to i32
  br label %concatenate.272.merge561

concatenate.pivot.2.738:                          ; preds = %concatenate.272.merge208
  %2511 = icmp ult i32 %1774, 2
  br i1 %2511, label %concatenate.pivot.1.739, label %concatenate.pivot.3.742

concatenate.pivot.1.739:                          ; preds = %concatenate.pivot.2.738
  %2512 = icmp ult i32 %1774, 1
  br i1 %2512, label %concatenate.pivot.0.740, label %concatenate.pivot.1.741

concatenate.pivot.0.740:                          ; preds = %concatenate.pivot.1.739
  br label %concat_index_from_operand_id0562

concatenate.pivot.1.741:                          ; preds = %concatenate.pivot.1.739
  br label %concat_index_from_operand_id1608

concatenate.pivot.3.742:                          ; preds = %concatenate.pivot.2.738
  %2513 = icmp ult i32 %1774, 3
  br i1 %2513, label %concatenate.pivot.2.743, label %concatenate.pivot.3.744

concatenate.pivot.2.743:                          ; preds = %concatenate.pivot.3.742
  br label %concat_index_from_operand_id2650

concatenate.pivot.3.744:                          ; preds = %concatenate.pivot.3.742
  br label %concat_index_from_operand_id3696

concatenate.272.merge561:                         ; preds = %concat_index_from_operand_id3696, %concat_index_from_operand_id2650, %concat_index_from_operand_id1608, %concat_index_from_operand_id0562
  %2514 = phi i32 [ %1966, %concat_index_from_operand_id0562 ], [ %2143, %concat_index_from_operand_id1608 ], [ %2333, %concat_index_from_operand_id2650 ], [ %2510, %concat_index_from_operand_id3696 ]
  %region_0_446_constant_273745 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %2515 = lshr i32 %2514, %region_0_446_constant_273745
  %shft.chk746 = icmp ult i32 %region_0_446_constant_273745, 32
  %2516 = select i1 %shft.chk746, i32 %2515, i32 0
  %2517 = uitofp i32 %2516 to float
  %region_0_446_constant_277747 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %2518 = load i32, i32* %5, align 4, !invariant.load !6
  %2519 = sitofp i32 %2518 to float
  %region_0_446_constant_278748 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.279749 = fmul float %2519, %region_0_446_constant_278748
  %region_0_446_constant_280750 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %2520 = fcmp oge float %region_0_446_constant_277747, %multiply.279749
  %2521 = fcmp une float %region_0_446_constant_277747, %region_0_446_constant_277747
  %2522 = or i1 %2520, %2521
  %2523 = select i1 %2522, float %region_0_446_constant_277747, float %multiply.279749
  %2524 = fcmp ole float %region_0_446_constant_280750, %2523
  %2525 = fcmp une float %region_0_446_constant_280750, %region_0_446_constant_280750
  %2526 = or i1 %2524, %2525
  %2527 = select i1 %2526, float %region_0_446_constant_280750, float %2523
  %multiply.282751 = fmul float %2527, %2527
  %region_0_446_constant_283752 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.284753 = fmul float %multiply.282751, %region_0_446_constant_283752
  %region_0_446_constant_285754 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.286755 = fadd float %multiply.284753, %region_0_446_constant_285754
  %multiply.287756 = fmul float %add.286755, %multiply.282751
  %region_0_446_constant_288757 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.289758 = fadd float %multiply.287756, %region_0_446_constant_288757
  %multiply.290759 = fmul float %add.289758, %multiply.282751
  %region_0_446_constant_291760 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.292761 = fadd float %multiply.290759, %region_0_446_constant_291760
  %multiply.293762 = fmul float %add.292761, %multiply.282751
  %region_0_446_constant_294763 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.295764 = fadd float %multiply.293762, %region_0_446_constant_294763
  %multiply.296765 = fmul float %add.295764, %multiply.282751
  %region_0_446_constant_297766 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.298767 = fadd float %multiply.296765, %region_0_446_constant_297766
  %multiply.299768 = fmul float %add.298767, %multiply.282751
  %region_0_446_constant_300769 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.301770 = fadd float %multiply.299768, %region_0_446_constant_300769
  %multiply.302771 = fmul float %add.301770, %multiply.282751
  %region_0_446_constant_303772 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.304773 = fadd float %multiply.302771, %region_0_446_constant_303772
  %multiply.305774 = fmul float %2527, %add.304773
  %region_0_446_constant_306775 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.307776 = fadd float %multiply.284753, %region_0_446_constant_306775
  %multiply.308777 = fmul float %add.307776, %multiply.282751
  %region_0_446_constant_309778 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.310779 = fadd float %multiply.308777, %region_0_446_constant_309778
  %multiply.311780 = fmul float %add.310779, %multiply.282751
  %region_0_446_constant_312781 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.313782 = fadd float %multiply.311780, %region_0_446_constant_312781
  %multiply.314783 = fmul float %add.313782, %multiply.282751
  %region_0_446_constant_315784 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.316785 = fadd float %multiply.314783, %region_0_446_constant_315784
  %multiply.317786 = fmul float %add.316785, %multiply.282751
  %region_0_446_constant_318787 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.319788 = fadd float %multiply.317786, %region_0_446_constant_318787
  %divide.320789 = fdiv float %multiply.305774, %add.319788
  %region_0_446_constant_277790 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %2528 = load i32, i32* %3, align 4, !invariant.load !6
  %2529 = sitofp i32 %2528 to float
  %region_0_446_constant_278791 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.321792 = fmul float %2529, %region_0_446_constant_278791
  %region_0_446_constant_280793 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %2530 = fcmp oge float %region_0_446_constant_277790, %multiply.321792
  %2531 = fcmp une float %region_0_446_constant_277790, %region_0_446_constant_277790
  %2532 = or i1 %2530, %2531
  %2533 = select i1 %2532, float %region_0_446_constant_277790, float %multiply.321792
  %2534 = fcmp ole float %region_0_446_constant_280793, %2533
  %2535 = fcmp une float %region_0_446_constant_280793, %region_0_446_constant_280793
  %2536 = or i1 %2534, %2535
  %2537 = select i1 %2536, float %region_0_446_constant_280793, float %2533
  %multiply.323794 = fmul float %2537, %2537
  %region_0_446_constant_283795 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.324796 = fmul float %multiply.323794, %region_0_446_constant_283795
  %region_0_446_constant_285797 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.325798 = fadd float %multiply.324796, %region_0_446_constant_285797
  %multiply.326799 = fmul float %add.325798, %multiply.323794
  %region_0_446_constant_288800 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.327801 = fadd float %multiply.326799, %region_0_446_constant_288800
  %multiply.328802 = fmul float %add.327801, %multiply.323794
  %region_0_446_constant_291803 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.329804 = fadd float %multiply.328802, %region_0_446_constant_291803
  %multiply.330805 = fmul float %add.329804, %multiply.323794
  %region_0_446_constant_294806 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.331807 = fadd float %multiply.330805, %region_0_446_constant_294806
  %multiply.332808 = fmul float %add.331807, %multiply.323794
  %region_0_446_constant_297809 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.333810 = fadd float %multiply.332808, %region_0_446_constant_297809
  %multiply.334811 = fmul float %add.333810, %multiply.323794
  %region_0_446_constant_300812 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.335813 = fadd float %multiply.334811, %region_0_446_constant_300812
  %multiply.336814 = fmul float %add.335813, %multiply.323794
  %region_0_446_constant_303815 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.337816 = fadd float %multiply.336814, %region_0_446_constant_303815
  %multiply.338817 = fmul float %2537, %add.337816
  %region_0_446_constant_306818 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.339819 = fadd float %multiply.324796, %region_0_446_constant_306818
  %multiply.340820 = fmul float %add.339819, %multiply.323794
  %region_0_446_constant_309821 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.341822 = fadd float %multiply.340820, %region_0_446_constant_309821
  %multiply.342823 = fmul float %add.341822, %multiply.323794
  %region_0_446_constant_312824 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.343825 = fadd float %multiply.342823, %region_0_446_constant_312824
  %multiply.344826 = fmul float %add.343825, %multiply.323794
  %region_0_446_constant_315827 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.345828 = fadd float %multiply.344826, %region_0_446_constant_315827
  %multiply.346829 = fmul float %add.345828, %multiply.323794
  %region_0_446_constant_318830 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.347831 = fadd float %multiply.346829, %region_0_446_constant_318830
  %divide.348832 = fdiv float %multiply.338817, %add.347831
  %subtract.349833 = fsub float %divide.320789, %divide.348832
  %region_0_446_constant_350834 = load float, float* bitcast ([4 x i8]* @17 to float*), align 4
  %multiply.351835 = fmul float %subtract.349833, %region_0_446_constant_350834
  %multiply.353836 = fmul float %2517, %multiply.351835
  %add.355837 = fadd float %multiply.353836, %divide.348832
  %2538 = call float @llvm.fabs.f32(float %add.355837)
  %region_0_446_constant_358838 = load float, float* bitcast ([4 x i8]* @64 to float*), align 4
  %compare.360839 = fcmp oeq float %2538, %region_0_446_constant_358838
  %2539 = zext i1 %compare.360839 to i8
  %region_0_446_constant_34840 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %multiply.362841 = fmul float %add.355837, %region_0_446_constant_34840
  %2540 = fneg float %add.355837
  %multiply.364842 = fmul float %2540, %add.355837
  %2541 = call float @__nv_log1pf(float %multiply.364842)
  %2542 = fneg float %2541
  %region_0_446_constant_367843 = load float, float* bitcast ([4 x i8]* @44 to float*), align 4
  %compare.369844 = fcmp olt float %2542, %region_0_446_constant_367843
  %2543 = zext i1 %compare.369844 to i8
  %region_0_446_constant_370845 = load float, float* bitcast ([4 x i8]* @62 to float*), align 4
  %region_0_446_constant_372846 = load float, float* bitcast ([4 x i8]* @61 to float*), align 4
  %2544 = trunc i8 %2543 to i1
  %2545 = select i1 %2544, float %region_0_446_constant_370845, float %region_0_446_constant_372846
  %region_0_446_constant_375847 = load float, float* bitcast ([4 x i8]* @60 to float*), align 4
  %region_0_446_constant_377848 = load float, float* bitcast ([4 x i8]* @59 to float*), align 4
  %2546 = trunc i8 %2543 to i1
  %2547 = select i1 %2546, float %region_0_446_constant_375847, float %region_0_446_constant_377848
  %region_0_446_constant_380849 = load float, float* bitcast ([4 x i8]* @58 to float*), align 4
  %region_0_446_constant_382850 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %2548 = trunc i8 %2543 to i1
  %2549 = select i1 %2548, float %region_0_446_constant_380849, float %region_0_446_constant_382850
  %region_0_446_constant_385851 = load float, float* bitcast ([4 x i8]* @56 to float*), align 4
  %region_0_446_constant_387852 = load float, float* bitcast ([4 x i8]* @55 to float*), align 4
  %2550 = trunc i8 %2543 to i1
  %2551 = select i1 %2550, float %region_0_446_constant_385851, float %region_0_446_constant_387852
  %region_0_446_constant_390853 = load float, float* bitcast ([4 x i8]* @54 to float*), align 4
  %region_0_446_constant_392854 = load float, float* bitcast ([4 x i8]* @53 to float*), align 4
  %2552 = trunc i8 %2543 to i1
  %2553 = select i1 %2552, float %region_0_446_constant_390853, float %region_0_446_constant_392854
  %region_0_446_constant_395855 = load float, float* bitcast ([4 x i8]* @52 to float*), align 4
  %region_0_446_constant_397856 = load float, float* bitcast ([4 x i8]* @51 to float*), align 4
  %2554 = trunc i8 %2543 to i1
  %2555 = select i1 %2554, float %region_0_446_constant_395855, float %region_0_446_constant_397856
  %region_0_446_constant_400857 = load float, float* bitcast ([4 x i8]* @50 to float*), align 4
  %region_0_446_constant_402858 = load float, float* bitcast ([4 x i8]* @49 to float*), align 4
  %2556 = trunc i8 %2543 to i1
  %2557 = select i1 %2556, float %region_0_446_constant_400857, float %region_0_446_constant_402858
  %region_0_446_constant_405859 = load float, float* bitcast ([4 x i8]* @48 to float*), align 4
  %region_0_446_constant_407860 = load float, float* bitcast ([4 x i8]* @47 to float*), align 4
  %2558 = trunc i8 %2543 to i1
  %2559 = select i1 %2558, float %region_0_446_constant_405859, float %region_0_446_constant_407860
  %region_0_446_constant_410861 = load float, float* bitcast ([4 x i8]* @46 to float*), align 4
  %region_0_446_constant_412862 = load float, float* bitcast ([4 x i8]* @45 to float*), align 4
  %2560 = trunc i8 %2543 to i1
  %2561 = select i1 %2560, float %region_0_446_constant_410861, float %region_0_446_constant_412862
  %region_0_446_constant_415863 = load float, float* bitcast ([4 x i8]* @43 to float*), align 4
  %add.417864 = fadd float %2542, %region_0_446_constant_415863
  %2562 = call float @__nv_sqrtf(float %2542)
  %region_0_446_constant_419865 = load float, float* bitcast ([4 x i8]* @42 to float*), align 4
  %add.421866 = fadd float %2562, %region_0_446_constant_419865
  %2563 = trunc i8 %2543 to i1
  %2564 = select i1 %2563, float %add.417864, float %add.421866
  %multiply.423867 = fmul float %2561, %2564
  %add.424868 = fadd float %2559, %multiply.423867
  %multiply.425869 = fmul float %add.424868, %2564
  %add.426870 = fadd float %2557, %multiply.425869
  %multiply.427871 = fmul float %add.426870, %2564
  %add.428872 = fadd float %2555, %multiply.427871
  %multiply.429873 = fmul float %add.428872, %2564
  %add.430874 = fadd float %2553, %multiply.429873
  %multiply.431875 = fmul float %add.430874, %2564
  %add.432876 = fadd float %2551, %multiply.431875
  %multiply.433877 = fmul float %add.432876, %2564
  %add.434878 = fadd float %2549, %multiply.433877
  %multiply.435879 = fmul float %add.434878, %2564
  %add.436880 = fadd float %2547, %multiply.435879
  %multiply.437881 = fmul float %add.436880, %2564
  %add.438882 = fadd float %2545, %multiply.437881
  %multiply.439883 = fmul float %add.438882, %add.355837
  %2565 = trunc i8 %2539 to i1
  %2566 = select i1 %2565, float %multiply.362841, float %multiply.439883
  %region_0_446_constant_441884 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %multiply.443885 = fmul float %2566, %region_0_446_constant_441884
  %2567 = fcmp oge float %1769, %multiply.443885
  %2568 = fcmp une float %1769, %1769
  %2569 = or i1 %2567, %2568
  %maximum.444886 = select i1 %2569, float %1769, float %multiply.443885
  %2570 = fcmp ole float %1747, %maximum.444886
  %2571 = fcmp une float %1747, %1747
  %2572 = or i1 %2570, %2571
  %minimum.445887 = select i1 %2572, float %1747, float %maximum.444886
  %2573 = bitcast [16 x [16 x float]]* %7 to float*
  %2574 = getelementptr inbounds float, float* %2573, i32 %linear_index2
  store float %minimum.445887, float* %2574, align 4
  %compare.6888 = fcmp une float %2519, %2519
  %2575 = zext i1 %compare.6888 to i8
  %region_0_446_constant_7889 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_8890 = load float, float* bitcast ([4 x i8]* @74 to float*), align 4
  %compare.9891 = fcmp oeq float %2519, %region_0_446_constant_8890
  %2576 = zext i1 %compare.9891 to i8
  %region_0_446_constant_10892 = load i32, i32* bitcast ([4 x i8]* @73 to i32*), align 4
  %2577 = bitcast float %2519 to i32
  %region_0_446_constant_12893 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %2578 = and i32 %2577, %region_0_446_constant_12893
  %region_0_446_constant_14894 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %2579 = icmp eq i32 %2578, %region_0_446_constant_14894
  %2580 = zext i1 %2579 to i8
  %region_0_446_constant_16895 = load i32, i32* bitcast ([4 x i8]* @72 to i32*), align 4
  %region_0_446_constant_17896 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %2581 = icmp sgt i32 %2578, %region_0_446_constant_17896
  %2582 = zext i1 %2581 to i8
  %region_0_446_constant_19897 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %2583 = and i32 %2577, %region_0_446_constant_19897
  %region_0_446_constant_19898 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %2584 = icmp ne i32 %2583, %region_0_446_constant_19898
  %2585 = zext i1 %2584 to i8
  %2586 = or i8 %2582, %2585
  %region_0_446_constant_23899 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24900 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %2587 = trunc i8 %2586 to i1
  %2588 = select i1 %2587, i32 %region_0_446_constant_23899, i32 %region_0_446_constant_24900
  %2589 = add i32 %2577, %2588
  %2590 = trunc i8 %2580 to i1
  %2591 = select i1 %2590, i32 %region_0_446_constant_16895, i32 %2589
  %2592 = trunc i8 %2576 to i1
  %2593 = select i1 %2592, i32 %region_0_446_constant_10892, i32 %2591
  %2594 = trunc i8 %2575 to i1
  %2595 = select i1 %2594, i32 %region_0_446_constant_7889, i32 %2593
  %2596 = bitcast i32 %2595 to float
  %compare.33901 = fcmp une float %2529, %2529
  %2597 = zext i1 %compare.33901 to i8
  %region_0_446_constant_7902 = load i32, i32* bitcast ([4 x i8]* @71 to i32*), align 4
  %region_0_446_constant_34903 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %compare.35904 = fcmp oeq float %2529, %region_0_446_constant_34903
  %2598 = zext i1 %compare.35904 to i8
  %region_0_446_constant_17905 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %2599 = bitcast float %2529 to i32
  %region_0_446_constant_12906 = load i32, i32* bitcast ([4 x i8]* @70 to i32*), align 4
  %2600 = and i32 %2599, %region_0_446_constant_12906
  %region_0_446_constant_14907 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %2601 = icmp eq i32 %2600, %region_0_446_constant_14907
  %2602 = zext i1 %2601 to i8
  %region_0_446_constant_24908 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %region_0_446_constant_17909 = load i32, i32* bitcast ([4 x i8]* @69 to i32*), align 4
  %2603 = icmp sgt i32 %2600, %region_0_446_constant_17909
  %2604 = zext i1 %2603 to i8
  %region_0_446_constant_19910 = load i32, i32* bitcast ([4 x i8]* @68 to i32*), align 4
  %2605 = and i32 %2599, %region_0_446_constant_19910
  %region_0_446_constant_14911 = load i32, i32* bitcast ([4 x i8]* @67 to i32*), align 4
  %2606 = icmp ne i32 %2605, %region_0_446_constant_14911
  %2607 = zext i1 %2606 to i8
  %2608 = or i8 %2604, %2607
  %region_0_446_constant_23912 = load i32, i32* bitcast ([4 x i8]* @66 to i32*), align 4
  %region_0_446_constant_24913 = load i32, i32* bitcast ([4 x i8]* @65 to i32*), align 4
  %2609 = trunc i8 %2608 to i1
  %2610 = select i1 %2609, i32 %region_0_446_constant_23912, i32 %region_0_446_constant_24913
  %2611 = add i32 %2599, %2610
  %2612 = trunc i8 %2602 to i1
  %2613 = select i1 %2612, i32 %region_0_446_constant_24908, i32 %2611
  %2614 = trunc i8 %2598 to i1
  %2615 = select i1 %2614, i32 %region_0_446_constant_17905, i32 %2613
  %2616 = trunc i8 %2597 to i1
  %2617 = select i1 %2616, i32 %region_0_446_constant_7902, i32 %2615
  %2618 = bitcast i32 %2617 to float
  %2619 = mul nuw nsw i32 %21, 1
  %2620 = add nuw nsw i32 0, %2619
  %2621 = mul nuw nsw i32 %22, 16
  %2622 = add nuw nsw i32 %2620, %2621
  %2623 = urem i32 %2622, 4
  %2624 = udiv i32 %2622, 4
  %2625 = udiv i32 %2624, 64
  br label %concatenate.pivot.2.1091

concat_index_from_operand_id0915:                 ; preds = %concatenate.pivot.0.1093
  %2626 = phi i32 [ 0, %concatenate.pivot.0.1093 ]
  %2627 = sub nsw i32 %2623, %2626
  %2628 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %2629 = load i64, i64* %2628, align 8, !invariant.load !6
  %2630 = trunc i64 %2629 to i32
  %2631 = zext i32 %2630 to i64
  %2632 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2633 = lshr i64 %2629, %2632
  %shft.chk916 = icmp ult i64 %2632, 64
  %2634 = select i1 %shft.chk916, i64 %2633, i64 0
  %2635 = trunc i64 %2634 to i32
  %2636 = zext i32 %2635 to i64
  %2637 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2638 = shl i64 %2636, %2637
  %shft.chk917 = icmp ult i64 %2637, 64
  %2639 = select i1 %shft.chk917, i64 %2638, i64 0
  %2640 = or i64 %2631, %2639
  %2641 = mul nuw nsw i32 %2624, 1
  %2642 = add nuw nsw i32 0, %2641
  %2643 = zext i32 %2642 to i64
  %2644 = add i64 %2640, %2643
  %2645 = trunc i64 %2644 to i32
  %2646 = zext i32 %2645 to i64
  %region_0_446_constant_64918 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %2647 = mul i64 %2646, %region_0_446_constant_64918
  %2648 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2649 = lshr i64 %2647, %2648
  %shft.chk919 = icmp ult i64 %2648, 64
  %2650 = select i1 %shft.chk919, i64 %2649, i64 0
  %2651 = trunc i64 %2650 to i32
  %2652 = icmp ult i64 %2644, %2640
  %2653 = zext i1 %2652 to i8
  %2654 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %2655 = load i64, i64* %2654, align 8, !invariant.load !6
  %2656 = trunc i64 %2655 to i32
  %2657 = zext i32 %2656 to i64
  %2658 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2659 = lshr i64 %2655, %2658
  %shft.chk920 = icmp ult i64 %2658, 64
  %2660 = select i1 %shft.chk920, i64 %2659, i64 0
  %2661 = trunc i64 %2660 to i32
  %2662 = zext i32 %2661 to i64
  %2663 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2664 = shl i64 %2662, %2663
  %shft.chk921 = icmp ult i64 %2663, 64
  %2665 = select i1 %shft.chk921, i64 %2664, i64 0
  %2666 = or i64 %2657, %2665
  %region_0_446_constant_80922 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %2667 = add i64 %2666, %region_0_446_constant_80922
  %2668 = trunc i8 %2653 to i1
  %2669 = select i1 %2668, i64 %2667, i64 %2666
  %2670 = lshr i64 %2669, %2648
  %shft.chk923 = icmp ult i64 %2648, 64
  %2671 = select i1 %shft.chk923, i64 %2670, i64 0
  %2672 = trunc i64 %2671 to i32
  %2673 = xor i32 %2651, %2672
  %region_0_446_constant_88924 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %2674 = xor i32 %2673, %region_0_446_constant_88924
  %2675 = zext i32 %2674 to i64
  %region_0_446_constant_92925 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %2676 = mul i64 %2675, %region_0_446_constant_92925
  %2677 = lshr i64 %2676, %2648
  %shft.chk926 = icmp ult i64 %2648, 64
  %2678 = select i1 %shft.chk926, i64 %2677, i64 0
  %2679 = trunc i64 %2678 to i32
  %2680 = trunc i64 %2669 to i32
  %2681 = zext i32 %2680 to i64
  %2682 = mul i64 %2681, %region_0_446_constant_92925
  %2683 = trunc i64 %2682 to i32
  %2684 = xor i32 %2679, %2683
  %region_0_446_constant_102927 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %2685 = xor i32 %2684, %region_0_446_constant_102927
  %2686 = zext i32 %2685 to i64
  %2687 = mul i64 %2686, %region_0_446_constant_64918
  %2688 = lshr i64 %2687, %2648
  %shft.chk928 = icmp ult i64 %2648, 64
  %2689 = select i1 %shft.chk928, i64 %2688, i64 0
  %2690 = trunc i64 %2689 to i32
  %2691 = lshr i64 %2682, %2648
  %shft.chk929 = icmp ult i64 %2648, 64
  %2692 = select i1 %shft.chk929, i64 %2691, i64 0
  %2693 = trunc i64 %2692 to i32
  %2694 = lshr i64 %2644, %2648
  %shft.chk930 = icmp ult i64 %2648, 64
  %2695 = select i1 %shft.chk930, i64 %2694, i64 0
  %2696 = trunc i64 %2695 to i32
  %2697 = xor i32 %2693, %2696
  %region_0_446_constant_114931 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2698 = xor i32 %2697, %region_0_446_constant_114931
  %2699 = zext i32 %2698 to i64
  %2700 = mul i64 %2699, %region_0_446_constant_64918
  %2701 = trunc i64 %2700 to i32
  %2702 = xor i32 %2690, %2701
  %region_0_446_constant_121932 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2703 = xor i32 %2702, %region_0_446_constant_121932
  %2704 = zext i32 %2703 to i64
  %2705 = mul i64 %2704, %region_0_446_constant_92925
  %2706 = lshr i64 %2705, %2648
  %shft.chk933 = icmp ult i64 %2648, 64
  %2707 = select i1 %shft.chk933, i64 %2706, i64 0
  %2708 = trunc i64 %2707 to i32
  %2709 = lshr i64 %2700, %2648
  %shft.chk934 = icmp ult i64 %2648, 64
  %2710 = select i1 %shft.chk934, i64 %2709, i64 0
  %2711 = trunc i64 %2710 to i32
  %2712 = trunc i64 %2647 to i32
  %2713 = xor i32 %2711, %2712
  %region_0_446_constant_132935 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %2714 = xor i32 %2713, %region_0_446_constant_132935
  %2715 = zext i32 %2714 to i64
  %2716 = mul i64 %2715, %region_0_446_constant_92925
  %2717 = trunc i64 %2716 to i32
  %2718 = xor i32 %2708, %2717
  %region_0_446_constant_139936 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %2719 = xor i32 %2718, %region_0_446_constant_139936
  %2720 = zext i32 %2719 to i64
  %2721 = mul i64 %2720, %region_0_446_constant_64918
  %2722 = lshr i64 %2721, %2648
  %shft.chk937 = icmp ult i64 %2648, 64
  %2723 = select i1 %shft.chk937, i64 %2722, i64 0
  %2724 = trunc i64 %2723 to i32
  %2725 = lshr i64 %2716, %2648
  %shft.chk938 = icmp ult i64 %2648, 64
  %2726 = select i1 %shft.chk938, i64 %2725, i64 0
  %2727 = trunc i64 %2726 to i32
  %2728 = trunc i64 %2676 to i32
  %2729 = xor i32 %2727, %2728
  %region_0_446_constant_150939 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %2730 = xor i32 %2729, %region_0_446_constant_150939
  %2731 = zext i32 %2730 to i64
  %2732 = mul i64 %2731, %region_0_446_constant_64918
  %2733 = trunc i64 %2732 to i32
  %2734 = xor i32 %2724, %2733
  %region_0_446_constant_157940 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2735 = xor i32 %2734, %region_0_446_constant_157940
  %2736 = zext i32 %2735 to i64
  %2737 = mul i64 %2736, %region_0_446_constant_92925
  %2738 = lshr i64 %2737, %2648
  %shft.chk941 = icmp ult i64 %2648, 64
  %2739 = select i1 %shft.chk941, i64 %2738, i64 0
  %2740 = trunc i64 %2739 to i32
  %2741 = lshr i64 %2732, %2648
  %shft.chk942 = icmp ult i64 %2648, 64
  %2742 = select i1 %shft.chk942, i64 %2741, i64 0
  %2743 = trunc i64 %2742 to i32
  %2744 = trunc i64 %2687 to i32
  %2745 = xor i32 %2743, %2744
  %region_0_446_constant_168943 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %2746 = xor i32 %2745, %region_0_446_constant_168943
  %2747 = zext i32 %2746 to i64
  %2748 = mul i64 %2747, %region_0_446_constant_92925
  %2749 = trunc i64 %2748 to i32
  %2750 = xor i32 %2740, %2749
  %region_0_446_constant_175944 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %2751 = xor i32 %2750, %region_0_446_constant_175944
  %2752 = zext i32 %2751 to i64
  %2753 = mul i64 %2752, %region_0_446_constant_64918
  %2754 = lshr i64 %2753, %2648
  %shft.chk945 = icmp ult i64 %2648, 64
  %2755 = select i1 %shft.chk945, i64 %2754, i64 0
  %2756 = trunc i64 %2755 to i32
  %2757 = lshr i64 %2748, %2648
  %shft.chk946 = icmp ult i64 %2648, 64
  %2758 = select i1 %shft.chk946, i64 %2757, i64 0
  %2759 = trunc i64 %2758 to i32
  %2760 = trunc i64 %2705 to i32
  %2761 = xor i32 %2759, %2760
  %region_0_446_constant_186947 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2762 = xor i32 %2761, %region_0_446_constant_186947
  %2763 = zext i32 %2762 to i64
  %2764 = mul i64 %2763, %region_0_446_constant_64918
  %2765 = trunc i64 %2764 to i32
  %2766 = xor i32 %2756, %2765
  %region_0_446_constant_193948 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2767 = xor i32 %2766, %region_0_446_constant_193948
  %2768 = zext i32 %2767 to i64
  %2769 = mul i64 %2768, %region_0_446_constant_92925
  %2770 = lshr i64 %2769, %2648
  %shft.chk949 = icmp ult i64 %2648, 64
  %2771 = select i1 %shft.chk949, i64 %2770, i64 0
  %2772 = trunc i64 %2771 to i32
  %2773 = lshr i64 %2764, %2648
  %shft.chk950 = icmp ult i64 %2648, 64
  %2774 = select i1 %shft.chk950, i64 %2773, i64 0
  %2775 = trunc i64 %2774 to i32
  %2776 = trunc i64 %2721 to i32
  %2777 = xor i32 %2775, %2776
  %region_0_446_constant_204951 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2778 = xor i32 %2777, %region_0_446_constant_204951
  %2779 = zext i32 %2778 to i64
  %2780 = mul i64 %2779, %region_0_446_constant_92925
  %2781 = trunc i64 %2780 to i32
  %2782 = xor i32 %2772, %2781
  %region_0_446_constant_211952 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2783 = xor i32 %2782, %region_0_446_constant_211952
  %2784 = zext i32 %2783 to i64
  %2785 = mul i64 %2784, %region_0_446_constant_64918
  %2786 = lshr i64 %2785, %2648
  %shft.chk953 = icmp ult i64 %2648, 64
  %2787 = select i1 %shft.chk953, i64 %2786, i64 0
  %2788 = trunc i64 %2787 to i32
  %2789 = lshr i64 %2780, %2648
  %shft.chk954 = icmp ult i64 %2648, 64
  %2790 = select i1 %shft.chk954, i64 %2789, i64 0
  %2791 = trunc i64 %2790 to i32
  %2792 = trunc i64 %2737 to i32
  %2793 = xor i32 %2791, %2792
  %region_0_446_constant_222955 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %2794 = xor i32 %2793, %region_0_446_constant_222955
  %2795 = zext i32 %2794 to i64
  %2796 = mul i64 %2795, %region_0_446_constant_64918
  %2797 = trunc i64 %2796 to i32
  %2798 = xor i32 %2788, %2797
  %region_0_446_constant_229956 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2799 = xor i32 %2798, %region_0_446_constant_229956
  %2800 = zext i32 %2799 to i64
  %2801 = mul i64 %2800, %region_0_446_constant_92925
  %2802 = lshr i64 %2801, %2648
  %shft.chk957 = icmp ult i64 %2648, 64
  %2803 = select i1 %shft.chk957, i64 %2802, i64 0
  %2804 = trunc i64 %2803 to i32
  %2805 = lshr i64 %2796, %2648
  %shft.chk958 = icmp ult i64 %2648, 64
  %2806 = select i1 %shft.chk958, i64 %2805, i64 0
  %2807 = trunc i64 %2806 to i32
  %2808 = trunc i64 %2753 to i32
  %2809 = xor i32 %2807, %2808
  %region_0_446_constant_240959 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %2810 = xor i32 %2809, %region_0_446_constant_240959
  %2811 = zext i32 %2810 to i64
  %2812 = mul i64 %2811, %region_0_446_constant_92925
  %2813 = trunc i64 %2812 to i32
  %2814 = xor i32 %2804, %2813
  %region_0_446_constant_247960 = load i32, i32* bitcast ([4 x i8]* @41 to i32*), align 4
  %2815 = xor i32 %2814, %region_0_446_constant_247960
  br label %concatenate.272.merge914

concat_index_from_operand_id1961:                 ; preds = %concatenate.pivot.1.1094
  %2816 = phi i32 [ 1, %concatenate.pivot.1.1094 ]
  %2817 = sub nsw i32 %2623, %2816
  %2818 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %2819 = load i64, i64* %2818, align 8, !invariant.load !6
  %2820 = trunc i64 %2819 to i32
  %2821 = zext i32 %2820 to i64
  %2822 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2823 = lshr i64 %2819, %2822
  %shft.chk962 = icmp ult i64 %2822, 64
  %2824 = select i1 %shft.chk962, i64 %2823, i64 0
  %2825 = trunc i64 %2824 to i32
  %2826 = zext i32 %2825 to i64
  %2827 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2828 = shl i64 %2826, %2827
  %shft.chk963 = icmp ult i64 %2827, 64
  %2829 = select i1 %shft.chk963, i64 %2828, i64 0
  %2830 = or i64 %2821, %2829
  %2831 = mul nuw nsw i32 %2624, 1
  %2832 = add nuw nsw i32 0, %2831
  %2833 = zext i32 %2832 to i64
  %2834 = add i64 %2830, %2833
  %2835 = trunc i64 %2834 to i32
  %2836 = zext i32 %2835 to i64
  %region_0_446_constant_64964 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %2837 = mul i64 %2836, %region_0_446_constant_64964
  %2838 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2839 = lshr i64 %2837, %2838
  %shft.chk965 = icmp ult i64 %2838, 64
  %2840 = select i1 %shft.chk965, i64 %2839, i64 0
  %2841 = trunc i64 %2840 to i32
  %2842 = icmp ult i64 %2834, %2830
  %2843 = zext i1 %2842 to i8
  %2844 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %2845 = load i64, i64* %2844, align 8, !invariant.load !6
  %2846 = trunc i64 %2845 to i32
  %2847 = zext i32 %2846 to i64
  %2848 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2849 = lshr i64 %2845, %2848
  %shft.chk966 = icmp ult i64 %2848, 64
  %2850 = select i1 %shft.chk966, i64 %2849, i64 0
  %2851 = trunc i64 %2850 to i32
  %2852 = zext i32 %2851 to i64
  %2853 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %2854 = shl i64 %2852, %2853
  %shft.chk967 = icmp ult i64 %2853, 64
  %2855 = select i1 %shft.chk967, i64 %2854, i64 0
  %2856 = or i64 %2847, %2855
  %region_0_446_constant_80968 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %2857 = add i64 %2856, %region_0_446_constant_80968
  %2858 = trunc i8 %2843 to i1
  %2859 = select i1 %2858, i64 %2857, i64 %2856
  %2860 = lshr i64 %2859, %2838
  %shft.chk969 = icmp ult i64 %2838, 64
  %2861 = select i1 %shft.chk969, i64 %2860, i64 0
  %2862 = trunc i64 %2861 to i32
  %2863 = xor i32 %2841, %2862
  %region_0_446_constant_88970 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %2864 = xor i32 %2863, %region_0_446_constant_88970
  %2865 = zext i32 %2864 to i64
  %region_0_446_constant_92971 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %2866 = mul i64 %2865, %region_0_446_constant_92971
  %2867 = lshr i64 %2866, %2838
  %shft.chk972 = icmp ult i64 %2838, 64
  %2868 = select i1 %shft.chk972, i64 %2867, i64 0
  %2869 = trunc i64 %2868 to i32
  %2870 = trunc i64 %2859 to i32
  %2871 = zext i32 %2870 to i64
  %2872 = mul i64 %2871, %region_0_446_constant_92971
  %2873 = trunc i64 %2872 to i32
  %2874 = xor i32 %2869, %2873
  %region_0_446_constant_102973 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %2875 = xor i32 %2874, %region_0_446_constant_102973
  %2876 = zext i32 %2875 to i64
  %2877 = mul i64 %2876, %region_0_446_constant_64964
  %2878 = lshr i64 %2877, %2838
  %shft.chk974 = icmp ult i64 %2838, 64
  %2879 = select i1 %shft.chk974, i64 %2878, i64 0
  %2880 = trunc i64 %2879 to i32
  %2881 = lshr i64 %2872, %2838
  %shft.chk975 = icmp ult i64 %2838, 64
  %2882 = select i1 %shft.chk975, i64 %2881, i64 0
  %2883 = trunc i64 %2882 to i32
  %2884 = lshr i64 %2834, %2838
  %shft.chk976 = icmp ult i64 %2838, 64
  %2885 = select i1 %shft.chk976, i64 %2884, i64 0
  %2886 = trunc i64 %2885 to i32
  %2887 = xor i32 %2883, %2886
  %region_0_446_constant_114977 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %2888 = xor i32 %2887, %region_0_446_constant_114977
  %2889 = zext i32 %2888 to i64
  %2890 = mul i64 %2889, %region_0_446_constant_64964
  %2891 = trunc i64 %2890 to i32
  %2892 = xor i32 %2880, %2891
  %region_0_446_constant_121978 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %2893 = xor i32 %2892, %region_0_446_constant_121978
  %2894 = zext i32 %2893 to i64
  %2895 = mul i64 %2894, %region_0_446_constant_92971
  %2896 = lshr i64 %2895, %2838
  %shft.chk979 = icmp ult i64 %2838, 64
  %2897 = select i1 %shft.chk979, i64 %2896, i64 0
  %2898 = trunc i64 %2897 to i32
  %2899 = lshr i64 %2890, %2838
  %shft.chk980 = icmp ult i64 %2838, 64
  %2900 = select i1 %shft.chk980, i64 %2899, i64 0
  %2901 = trunc i64 %2900 to i32
  %2902 = trunc i64 %2837 to i32
  %2903 = xor i32 %2901, %2902
  %region_0_446_constant_132981 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %2904 = xor i32 %2903, %region_0_446_constant_132981
  %2905 = zext i32 %2904 to i64
  %2906 = mul i64 %2905, %region_0_446_constant_92971
  %2907 = trunc i64 %2906 to i32
  %2908 = xor i32 %2898, %2907
  %region_0_446_constant_139982 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %2909 = xor i32 %2908, %region_0_446_constant_139982
  %2910 = zext i32 %2909 to i64
  %2911 = mul i64 %2910, %region_0_446_constant_64964
  %2912 = lshr i64 %2911, %2838
  %shft.chk983 = icmp ult i64 %2838, 64
  %2913 = select i1 %shft.chk983, i64 %2912, i64 0
  %2914 = trunc i64 %2913 to i32
  %2915 = lshr i64 %2906, %2838
  %shft.chk984 = icmp ult i64 %2838, 64
  %2916 = select i1 %shft.chk984, i64 %2915, i64 0
  %2917 = trunc i64 %2916 to i32
  %2918 = trunc i64 %2866 to i32
  %2919 = xor i32 %2917, %2918
  %region_0_446_constant_150985 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %2920 = xor i32 %2919, %region_0_446_constant_150985
  %2921 = zext i32 %2920 to i64
  %2922 = mul i64 %2921, %region_0_446_constant_64964
  %2923 = trunc i64 %2922 to i32
  %2924 = xor i32 %2914, %2923
  %region_0_446_constant_157986 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %2925 = xor i32 %2924, %region_0_446_constant_157986
  %2926 = zext i32 %2925 to i64
  %2927 = mul i64 %2926, %region_0_446_constant_92971
  %2928 = lshr i64 %2927, %2838
  %shft.chk987 = icmp ult i64 %2838, 64
  %2929 = select i1 %shft.chk987, i64 %2928, i64 0
  %2930 = trunc i64 %2929 to i32
  %2931 = lshr i64 %2922, %2838
  %shft.chk988 = icmp ult i64 %2838, 64
  %2932 = select i1 %shft.chk988, i64 %2931, i64 0
  %2933 = trunc i64 %2932 to i32
  %2934 = trunc i64 %2877 to i32
  %2935 = xor i32 %2933, %2934
  %region_0_446_constant_168989 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %2936 = xor i32 %2935, %region_0_446_constant_168989
  %2937 = zext i32 %2936 to i64
  %2938 = mul i64 %2937, %region_0_446_constant_92971
  %2939 = trunc i64 %2938 to i32
  %2940 = xor i32 %2930, %2939
  %region_0_446_constant_175990 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %2941 = xor i32 %2940, %region_0_446_constant_175990
  %2942 = zext i32 %2941 to i64
  %2943 = mul i64 %2942, %region_0_446_constant_64964
  %2944 = lshr i64 %2943, %2838
  %shft.chk991 = icmp ult i64 %2838, 64
  %2945 = select i1 %shft.chk991, i64 %2944, i64 0
  %2946 = trunc i64 %2945 to i32
  %2947 = lshr i64 %2938, %2838
  %shft.chk992 = icmp ult i64 %2838, 64
  %2948 = select i1 %shft.chk992, i64 %2947, i64 0
  %2949 = trunc i64 %2948 to i32
  %2950 = trunc i64 %2895 to i32
  %2951 = xor i32 %2949, %2950
  %region_0_446_constant_186993 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %2952 = xor i32 %2951, %region_0_446_constant_186993
  %2953 = zext i32 %2952 to i64
  %2954 = mul i64 %2953, %region_0_446_constant_64964
  %2955 = trunc i64 %2954 to i32
  %2956 = xor i32 %2946, %2955
  %region_0_446_constant_193994 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %2957 = xor i32 %2956, %region_0_446_constant_193994
  %2958 = zext i32 %2957 to i64
  %2959 = mul i64 %2958, %region_0_446_constant_92971
  %2960 = lshr i64 %2959, %2838
  %shft.chk995 = icmp ult i64 %2838, 64
  %2961 = select i1 %shft.chk995, i64 %2960, i64 0
  %2962 = trunc i64 %2961 to i32
  %2963 = lshr i64 %2954, %2838
  %shft.chk996 = icmp ult i64 %2838, 64
  %2964 = select i1 %shft.chk996, i64 %2963, i64 0
  %2965 = trunc i64 %2964 to i32
  %2966 = trunc i64 %2911 to i32
  %2967 = xor i32 %2965, %2966
  %region_0_446_constant_204997 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %2968 = xor i32 %2967, %region_0_446_constant_204997
  %2969 = zext i32 %2968 to i64
  %2970 = mul i64 %2969, %region_0_446_constant_92971
  %2971 = trunc i64 %2970 to i32
  %2972 = xor i32 %2962, %2971
  %region_0_446_constant_211998 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %2973 = xor i32 %2972, %region_0_446_constant_211998
  %2974 = zext i32 %2973 to i64
  %2975 = mul i64 %2974, %region_0_446_constant_64964
  %2976 = lshr i64 %2975, %2838
  %shft.chk999 = icmp ult i64 %2838, 64
  %2977 = select i1 %shft.chk999, i64 %2976, i64 0
  %2978 = trunc i64 %2977 to i32
  %2979 = lshr i64 %2970, %2838
  %shft.chk1000 = icmp ult i64 %2838, 64
  %2980 = select i1 %shft.chk1000, i64 %2979, i64 0
  %2981 = trunc i64 %2980 to i32
  %2982 = trunc i64 %2927 to i32
  %2983 = xor i32 %2981, %2982
  %region_0_446_constant_2221001 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %2984 = xor i32 %2983, %region_0_446_constant_2221001
  %2985 = zext i32 %2984 to i64
  %2986 = mul i64 %2985, %region_0_446_constant_64964
  %2987 = trunc i64 %2986 to i32
  %2988 = xor i32 %2978, %2987
  %region_0_446_constant_2291002 = load i32, i32* bitcast ([4 x i8]* @40 to i32*), align 4
  %2989 = xor i32 %2988, %region_0_446_constant_2291002
  %2990 = zext i32 %2989 to i64
  %2991 = mul i64 %2990, %region_0_446_constant_92971
  %2992 = trunc i64 %2991 to i32
  br label %concatenate.272.merge914

concat_index_from_operand_id21003:                ; preds = %concatenate.pivot.2.1096
  %2993 = phi i32 [ 2, %concatenate.pivot.2.1096 ]
  %2994 = sub nsw i32 %2623, %2993
  %2995 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %2996 = load i64, i64* %2995, align 8, !invariant.load !6
  %2997 = trunc i64 %2996 to i32
  %2998 = zext i32 %2997 to i64
  %2999 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3000 = lshr i64 %2996, %2999
  %shft.chk1004 = icmp ult i64 %2999, 64
  %3001 = select i1 %shft.chk1004, i64 %3000, i64 0
  %3002 = trunc i64 %3001 to i32
  %3003 = zext i32 %3002 to i64
  %3004 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3005 = shl i64 %3003, %3004
  %shft.chk1005 = icmp ult i64 %3004, 64
  %3006 = select i1 %shft.chk1005, i64 %3005, i64 0
  %3007 = or i64 %2998, %3006
  %3008 = mul nuw nsw i32 %2624, 1
  %3009 = add nuw nsw i32 0, %3008
  %3010 = zext i32 %3009 to i64
  %3011 = add i64 %3007, %3010
  %3012 = icmp ult i64 %3011, %3007
  %3013 = zext i1 %3012 to i8
  %3014 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %3015 = load i64, i64* %3014, align 8, !invariant.load !6
  %3016 = trunc i64 %3015 to i32
  %3017 = zext i32 %3016 to i64
  %3018 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3019 = lshr i64 %3015, %3018
  %shft.chk1006 = icmp ult i64 %3018, 64
  %3020 = select i1 %shft.chk1006, i64 %3019, i64 0
  %3021 = trunc i64 %3020 to i32
  %3022 = zext i32 %3021 to i64
  %3023 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3024 = shl i64 %3022, %3023
  %shft.chk1007 = icmp ult i64 %3023, 64
  %3025 = select i1 %shft.chk1007, i64 %3024, i64 0
  %3026 = or i64 %3017, %3025
  %region_0_446_constant_801008 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %3027 = add i64 %3026, %region_0_446_constant_801008
  %3028 = trunc i8 %3013 to i1
  %3029 = select i1 %3028, i64 %3027, i64 %3026
  %3030 = trunc i64 %3029 to i32
  %3031 = zext i32 %3030 to i64
  %region_0_446_constant_921009 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %3032 = mul i64 %3031, %region_0_446_constant_921009
  %3033 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3034 = lshr i64 %3032, %3033
  %shft.chk1010 = icmp ult i64 %3033, 64
  %3035 = select i1 %shft.chk1010, i64 %3034, i64 0
  %3036 = trunc i64 %3035 to i32
  %3037 = lshr i64 %3011, %3033
  %shft.chk1011 = icmp ult i64 %3033, 64
  %3038 = select i1 %shft.chk1011, i64 %3037, i64 0
  %3039 = trunc i64 %3038 to i32
  %3040 = xor i32 %3036, %3039
  %region_0_446_constant_1141012 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %3041 = xor i32 %3040, %region_0_446_constant_1141012
  %3042 = zext i32 %3041 to i64
  %region_0_446_constant_641013 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %3043 = mul i64 %3042, %region_0_446_constant_641013
  %3044 = lshr i64 %3043, %3033
  %shft.chk1014 = icmp ult i64 %3033, 64
  %3045 = select i1 %shft.chk1014, i64 %3044, i64 0
  %3046 = trunc i64 %3045 to i32
  %3047 = trunc i64 %3011 to i32
  %3048 = zext i32 %3047 to i64
  %3049 = mul i64 %3048, %region_0_446_constant_641013
  %3050 = trunc i64 %3049 to i32
  %3051 = xor i32 %3046, %3050
  %region_0_446_constant_1321015 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %3052 = xor i32 %3051, %region_0_446_constant_1321015
  %3053 = zext i32 %3052 to i64
  %3054 = mul i64 %3053, %region_0_446_constant_921009
  %3055 = lshr i64 %3054, %3033
  %shft.chk1016 = icmp ult i64 %3033, 64
  %3056 = select i1 %shft.chk1016, i64 %3055, i64 0
  %3057 = trunc i64 %3056 to i32
  %3058 = lshr i64 %3049, %3033
  %shft.chk1017 = icmp ult i64 %3033, 64
  %3059 = select i1 %shft.chk1017, i64 %3058, i64 0
  %3060 = trunc i64 %3059 to i32
  %3061 = lshr i64 %3029, %3033
  %shft.chk1018 = icmp ult i64 %3033, 64
  %3062 = select i1 %shft.chk1018, i64 %3061, i64 0
  %3063 = trunc i64 %3062 to i32
  %3064 = xor i32 %3060, %3063
  %region_0_446_constant_881019 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %3065 = xor i32 %3064, %region_0_446_constant_881019
  %3066 = zext i32 %3065 to i64
  %3067 = mul i64 %3066, %region_0_446_constant_921009
  %3068 = trunc i64 %3067 to i32
  %3069 = xor i32 %3057, %3068
  %region_0_446_constant_1501020 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %3070 = xor i32 %3069, %region_0_446_constant_1501020
  %3071 = zext i32 %3070 to i64
  %3072 = mul i64 %3071, %region_0_446_constant_641013
  %3073 = lshr i64 %3072, %3033
  %shft.chk1021 = icmp ult i64 %3033, 64
  %3074 = select i1 %shft.chk1021, i64 %3073, i64 0
  %3075 = trunc i64 %3074 to i32
  %3076 = lshr i64 %3067, %3033
  %shft.chk1022 = icmp ult i64 %3033, 64
  %3077 = select i1 %shft.chk1022, i64 %3076, i64 0
  %3078 = trunc i64 %3077 to i32
  %3079 = trunc i64 %3032 to i32
  %3080 = xor i32 %3078, %3079
  %region_0_446_constant_1021023 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %3081 = xor i32 %3080, %region_0_446_constant_1021023
  %3082 = zext i32 %3081 to i64
  %3083 = mul i64 %3082, %region_0_446_constant_641013
  %3084 = trunc i64 %3083 to i32
  %3085 = xor i32 %3075, %3084
  %region_0_446_constant_1681024 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %3086 = xor i32 %3085, %region_0_446_constant_1681024
  %3087 = zext i32 %3086 to i64
  %3088 = mul i64 %3087, %region_0_446_constant_921009
  %3089 = lshr i64 %3088, %3033
  %shft.chk1025 = icmp ult i64 %3033, 64
  %3090 = select i1 %shft.chk1025, i64 %3089, i64 0
  %3091 = trunc i64 %3090 to i32
  %3092 = lshr i64 %3083, %3033
  %shft.chk1026 = icmp ult i64 %3033, 64
  %3093 = select i1 %shft.chk1026, i64 %3092, i64 0
  %3094 = trunc i64 %3093 to i32
  %3095 = trunc i64 %3043 to i32
  %3096 = xor i32 %3094, %3095
  %region_0_446_constant_1211027 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %3097 = xor i32 %3096, %region_0_446_constant_1211027
  %3098 = zext i32 %3097 to i64
  %3099 = mul i64 %3098, %region_0_446_constant_921009
  %3100 = trunc i64 %3099 to i32
  %3101 = xor i32 %3091, %3100
  %region_0_446_constant_1861028 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %3102 = xor i32 %3101, %region_0_446_constant_1861028
  %3103 = zext i32 %3102 to i64
  %3104 = mul i64 %3103, %region_0_446_constant_641013
  %3105 = lshr i64 %3104, %3033
  %shft.chk1029 = icmp ult i64 %3033, 64
  %3106 = select i1 %shft.chk1029, i64 %3105, i64 0
  %3107 = trunc i64 %3106 to i32
  %3108 = lshr i64 %3099, %3033
  %shft.chk1030 = icmp ult i64 %3033, 64
  %3109 = select i1 %shft.chk1030, i64 %3108, i64 0
  %3110 = trunc i64 %3109 to i32
  %3111 = trunc i64 %3054 to i32
  %3112 = xor i32 %3110, %3111
  %region_0_446_constant_1391031 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %3113 = xor i32 %3112, %region_0_446_constant_1391031
  %3114 = zext i32 %3113 to i64
  %3115 = mul i64 %3114, %region_0_446_constant_641013
  %3116 = trunc i64 %3115 to i32
  %3117 = xor i32 %3107, %3116
  %region_0_446_constant_2041032 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %3118 = xor i32 %3117, %region_0_446_constant_2041032
  %3119 = zext i32 %3118 to i64
  %3120 = mul i64 %3119, %region_0_446_constant_921009
  %3121 = lshr i64 %3120, %3033
  %shft.chk1033 = icmp ult i64 %3033, 64
  %3122 = select i1 %shft.chk1033, i64 %3121, i64 0
  %3123 = trunc i64 %3122 to i32
  %3124 = lshr i64 %3115, %3033
  %shft.chk1034 = icmp ult i64 %3033, 64
  %3125 = select i1 %shft.chk1034, i64 %3124, i64 0
  %3126 = trunc i64 %3125 to i32
  %3127 = trunc i64 %3072 to i32
  %3128 = xor i32 %3126, %3127
  %region_0_446_constant_1571035 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %3129 = xor i32 %3128, %region_0_446_constant_1571035
  %3130 = zext i32 %3129 to i64
  %3131 = mul i64 %3130, %region_0_446_constant_921009
  %3132 = trunc i64 %3131 to i32
  %3133 = xor i32 %3123, %3132
  %region_0_446_constant_2221036 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %3134 = xor i32 %3133, %region_0_446_constant_2221036
  %3135 = zext i32 %3134 to i64
  %3136 = mul i64 %3135, %region_0_446_constant_641013
  %3137 = lshr i64 %3136, %3033
  %shft.chk1037 = icmp ult i64 %3033, 64
  %3138 = select i1 %shft.chk1037, i64 %3137, i64 0
  %3139 = trunc i64 %3138 to i32
  %3140 = lshr i64 %3131, %3033
  %shft.chk1038 = icmp ult i64 %3033, 64
  %3141 = select i1 %shft.chk1038, i64 %3140, i64 0
  %3142 = trunc i64 %3141 to i32
  %3143 = trunc i64 %3088 to i32
  %3144 = xor i32 %3142, %3143
  %region_0_446_constant_1751039 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %3145 = xor i32 %3144, %region_0_446_constant_1751039
  %3146 = zext i32 %3145 to i64
  %3147 = mul i64 %3146, %region_0_446_constant_641013
  %3148 = trunc i64 %3147 to i32
  %3149 = xor i32 %3139, %3148
  %region_0_446_constant_2401040 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %3150 = xor i32 %3149, %region_0_446_constant_2401040
  %3151 = zext i32 %3150 to i64
  %3152 = mul i64 %3151, %region_0_446_constant_921009
  %3153 = lshr i64 %3152, %3033
  %shft.chk1041 = icmp ult i64 %3033, 64
  %3154 = select i1 %shft.chk1041, i64 %3153, i64 0
  %3155 = trunc i64 %3154 to i32
  %3156 = lshr i64 %3147, %3033
  %shft.chk1042 = icmp ult i64 %3033, 64
  %3157 = select i1 %shft.chk1042, i64 %3156, i64 0
  %3158 = trunc i64 %3157 to i32
  %3159 = trunc i64 %3104 to i32
  %3160 = xor i32 %3158, %3159
  %region_0_446_constant_1931043 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %3161 = xor i32 %3160, %region_0_446_constant_1931043
  %3162 = zext i32 %3161 to i64
  %3163 = mul i64 %3162, %region_0_446_constant_921009
  %3164 = trunc i64 %3163 to i32
  %3165 = xor i32 %3155, %3164
  %region_0_446_constant_2571044 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %3166 = xor i32 %3165, %region_0_446_constant_2571044
  %3167 = zext i32 %3166 to i64
  %3168 = mul i64 %3167, %region_0_446_constant_641013
  %3169 = lshr i64 %3168, %3033
  %shft.chk1045 = icmp ult i64 %3033, 64
  %3170 = select i1 %shft.chk1045, i64 %3169, i64 0
  %3171 = trunc i64 %3170 to i32
  %3172 = lshr i64 %3163, %3033
  %shft.chk1046 = icmp ult i64 %3033, 64
  %3173 = select i1 %shft.chk1046, i64 %3172, i64 0
  %3174 = trunc i64 %3173 to i32
  %3175 = trunc i64 %3120 to i32
  %3176 = xor i32 %3174, %3175
  %region_0_446_constant_2111047 = load i32, i32* bitcast ([4 x i8]* @39 to i32*), align 4
  %3177 = xor i32 %3176, %region_0_446_constant_2111047
  %3178 = zext i32 %3177 to i64
  %3179 = mul i64 %3178, %region_0_446_constant_641013
  %3180 = trunc i64 %3179 to i32
  %3181 = xor i32 %3171, %3180
  %region_0_446_constant_2661048 = load i32, i32* bitcast ([4 x i8]* @38 to i32*), align 4
  %3182 = xor i32 %3181, %region_0_446_constant_2661048
  br label %concatenate.272.merge914

concat_index_from_operand_id31049:                ; preds = %concatenate.pivot.3.1097
  %3183 = phi i32 [ 3, %concatenate.pivot.3.1097 ]
  %3184 = sub nsw i32 %2623, %3183
  %3185 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 0
  %3186 = load i64, i64* %3185, align 8, !invariant.load !6
  %3187 = trunc i64 %3186 to i32
  %3188 = zext i32 %3187 to i64
  %3189 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3190 = lshr i64 %3186, %3189
  %shft.chk1050 = icmp ult i64 %3189, 64
  %3191 = select i1 %shft.chk1050, i64 %3190, i64 0
  %3192 = trunc i64 %3191 to i32
  %3193 = zext i32 %3192 to i64
  %3194 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3195 = shl i64 %3193, %3194
  %shft.chk1051 = icmp ult i64 %3194, 64
  %3196 = select i1 %shft.chk1051, i64 %3195, i64 0
  %3197 = or i64 %3188, %3196
  %3198 = mul nuw nsw i32 %2624, 1
  %3199 = add nuw nsw i32 0, %3198
  %3200 = zext i32 %3199 to i64
  %3201 = add i64 %3197, %3200
  %3202 = icmp ult i64 %3201, %3197
  %3203 = zext i1 %3202 to i8
  %3204 = getelementptr inbounds [2 x i64], [2 x i64]* %1, i32 0, i32 1
  %3205 = load i64, i64* %3204, align 8, !invariant.load !6
  %3206 = trunc i64 %3205 to i32
  %3207 = zext i32 %3206 to i64
  %3208 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3209 = lshr i64 %3205, %3208
  %shft.chk1052 = icmp ult i64 %3208, 64
  %3210 = select i1 %shft.chk1052, i64 %3209, i64 0
  %3211 = trunc i64 %3210 to i32
  %3212 = zext i32 %3211 to i64
  %3213 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3214 = shl i64 %3212, %3213
  %shft.chk1053 = icmp ult i64 %3213, 64
  %3215 = select i1 %shft.chk1053, i64 %3214, i64 0
  %3216 = or i64 %3207, %3215
  %region_0_446_constant_801054 = load i64, i64* bitcast ([8 x i8]* @26 to i64*), align 8
  %3217 = add i64 %3216, %region_0_446_constant_801054
  %3218 = trunc i8 %3203 to i1
  %3219 = select i1 %3218, i64 %3217, i64 %3216
  %3220 = trunc i64 %3219 to i32
  %3221 = zext i32 %3220 to i64
  %region_0_446_constant_921055 = load i64, i64* bitcast ([8 x i8]* @21 to i64*), align 8
  %3222 = mul i64 %3221, %region_0_446_constant_921055
  %3223 = load i64, i64* bitcast ([8 x i8]* @buffer_for_constant_185 to i64*), align 8, !invariant.load !6
  %3224 = lshr i64 %3222, %3223
  %shft.chk1056 = icmp ult i64 %3223, 64
  %3225 = select i1 %shft.chk1056, i64 %3224, i64 0
  %3226 = trunc i64 %3225 to i32
  %3227 = lshr i64 %3201, %3223
  %shft.chk1057 = icmp ult i64 %3223, 64
  %3228 = select i1 %shft.chk1057, i64 %3227, i64 0
  %3229 = trunc i64 %3228 to i32
  %3230 = xor i32 %3226, %3229
  %region_0_446_constant_1141058 = load i32, i32* bitcast ([4 x i8]* @25 to i32*), align 4
  %3231 = xor i32 %3230, %region_0_446_constant_1141058
  %3232 = zext i32 %3231 to i64
  %region_0_446_constant_641059 = load i64, i64* bitcast ([8 x i8]* @19 to i64*), align 8
  %3233 = mul i64 %3232, %region_0_446_constant_641059
  %3234 = lshr i64 %3233, %3223
  %shft.chk1060 = icmp ult i64 %3223, 64
  %3235 = select i1 %shft.chk1060, i64 %3234, i64 0
  %3236 = trunc i64 %3235 to i32
  %3237 = trunc i64 %3201 to i32
  %3238 = zext i32 %3237 to i64
  %3239 = mul i64 %3238, %region_0_446_constant_641059
  %3240 = trunc i64 %3239 to i32
  %3241 = xor i32 %3236, %3240
  %region_0_446_constant_1321061 = load i32, i32* bitcast ([4 x i8]* @31 to i32*), align 4
  %3242 = xor i32 %3241, %region_0_446_constant_1321061
  %3243 = zext i32 %3242 to i64
  %3244 = mul i64 %3243, %region_0_446_constant_921055
  %3245 = lshr i64 %3244, %3223
  %shft.chk1062 = icmp ult i64 %3223, 64
  %3246 = select i1 %shft.chk1062, i64 %3245, i64 0
  %3247 = trunc i64 %3246 to i32
  %3248 = lshr i64 %3239, %3223
  %shft.chk1063 = icmp ult i64 %3223, 64
  %3249 = select i1 %shft.chk1063, i64 %3248, i64 0
  %3250 = trunc i64 %3249 to i32
  %3251 = lshr i64 %3219, %3223
  %shft.chk1064 = icmp ult i64 %3223, 64
  %3252 = select i1 %shft.chk1064, i64 %3251, i64 0
  %3253 = trunc i64 %3252 to i32
  %3254 = xor i32 %3250, %3253
  %region_0_446_constant_881065 = load i32, i32* bitcast ([4 x i8]* @28 to i32*), align 4
  %3255 = xor i32 %3254, %region_0_446_constant_881065
  %3256 = zext i32 %3255 to i64
  %3257 = mul i64 %3256, %region_0_446_constant_921055
  %3258 = trunc i64 %3257 to i32
  %3259 = xor i32 %3247, %3258
  %region_0_446_constant_1501066 = load i32, i32* bitcast ([4 x i8]* @30 to i32*), align 4
  %3260 = xor i32 %3259, %region_0_446_constant_1501066
  %3261 = zext i32 %3260 to i64
  %3262 = mul i64 %3261, %region_0_446_constant_641059
  %3263 = lshr i64 %3262, %3223
  %shft.chk1067 = icmp ult i64 %3223, 64
  %3264 = select i1 %shft.chk1067, i64 %3263, i64 0
  %3265 = trunc i64 %3264 to i32
  %3266 = lshr i64 %3257, %3223
  %shft.chk1068 = icmp ult i64 %3223, 64
  %3267 = select i1 %shft.chk1068, i64 %3266, i64 0
  %3268 = trunc i64 %3267 to i32
  %3269 = trunc i64 %3222 to i32
  %3270 = xor i32 %3268, %3269
  %region_0_446_constant_1021069 = load i32, i32* bitcast ([4 x i8]* @27 to i32*), align 4
  %3271 = xor i32 %3270, %region_0_446_constant_1021069
  %3272 = zext i32 %3271 to i64
  %3273 = mul i64 %3272, %region_0_446_constant_641059
  %3274 = trunc i64 %3273 to i32
  %3275 = xor i32 %3265, %3274
  %region_0_446_constant_1681070 = load i32, i32* bitcast ([4 x i8]* @29 to i32*), align 4
  %3276 = xor i32 %3275, %region_0_446_constant_1681070
  %3277 = zext i32 %3276 to i64
  %3278 = mul i64 %3277, %region_0_446_constant_921055
  %3279 = lshr i64 %3278, %3223
  %shft.chk1071 = icmp ult i64 %3223, 64
  %3280 = select i1 %shft.chk1071, i64 %3279, i64 0
  %3281 = trunc i64 %3280 to i32
  %3282 = lshr i64 %3273, %3223
  %shft.chk1072 = icmp ult i64 %3223, 64
  %3283 = select i1 %shft.chk1072, i64 %3282, i64 0
  %3284 = trunc i64 %3283 to i32
  %3285 = trunc i64 %3233 to i32
  %3286 = xor i32 %3284, %3285
  %region_0_446_constant_1211073 = load i32, i32* bitcast ([4 x i8]* @24 to i32*), align 4
  %3287 = xor i32 %3286, %region_0_446_constant_1211073
  %3288 = zext i32 %3287 to i64
  %3289 = mul i64 %3288, %region_0_446_constant_921055
  %3290 = trunc i64 %3289 to i32
  %3291 = xor i32 %3281, %3290
  %region_0_446_constant_1861074 = load i32, i32* bitcast ([4 x i8]* @23 to i32*), align 4
  %3292 = xor i32 %3291, %region_0_446_constant_1861074
  %3293 = zext i32 %3292 to i64
  %3294 = mul i64 %3293, %region_0_446_constant_641059
  %3295 = lshr i64 %3294, %3223
  %shft.chk1075 = icmp ult i64 %3223, 64
  %3296 = select i1 %shft.chk1075, i64 %3295, i64 0
  %3297 = trunc i64 %3296 to i32
  %3298 = lshr i64 %3289, %3223
  %shft.chk1076 = icmp ult i64 %3223, 64
  %3299 = select i1 %shft.chk1076, i64 %3298, i64 0
  %3300 = trunc i64 %3299 to i32
  %3301 = trunc i64 %3244 to i32
  %3302 = xor i32 %3300, %3301
  %region_0_446_constant_1391077 = load i32, i32* bitcast ([4 x i8]* @34 to i32*), align 4
  %3303 = xor i32 %3302, %region_0_446_constant_1391077
  %3304 = zext i32 %3303 to i64
  %3305 = mul i64 %3304, %region_0_446_constant_641059
  %3306 = trunc i64 %3305 to i32
  %3307 = xor i32 %3297, %3306
  %region_0_446_constant_2041078 = load i32, i32* bitcast ([4 x i8]* @37 to i32*), align 4
  %3308 = xor i32 %3307, %region_0_446_constant_2041078
  %3309 = zext i32 %3308 to i64
  %3310 = mul i64 %3309, %region_0_446_constant_921055
  %3311 = lshr i64 %3310, %3223
  %shft.chk1079 = icmp ult i64 %3223, 64
  %3312 = select i1 %shft.chk1079, i64 %3311, i64 0
  %3313 = trunc i64 %3312 to i32
  %3314 = lshr i64 %3305, %3223
  %shft.chk1080 = icmp ult i64 %3223, 64
  %3315 = select i1 %shft.chk1080, i64 %3314, i64 0
  %3316 = trunc i64 %3315 to i32
  %3317 = trunc i64 %3262 to i32
  %3318 = xor i32 %3316, %3317
  %region_0_446_constant_1571081 = load i32, i32* bitcast ([4 x i8]* @33 to i32*), align 4
  %3319 = xor i32 %3318, %region_0_446_constant_1571081
  %3320 = zext i32 %3319 to i64
  %3321 = mul i64 %3320, %region_0_446_constant_921055
  %3322 = trunc i64 %3321 to i32
  %3323 = xor i32 %3313, %3322
  %region_0_446_constant_2221082 = load i32, i32* bitcast ([4 x i8]* @36 to i32*), align 4
  %3324 = xor i32 %3323, %region_0_446_constant_2221082
  %3325 = zext i32 %3324 to i64
  %3326 = mul i64 %3325, %region_0_446_constant_641059
  %3327 = lshr i64 %3326, %3223
  %shft.chk1083 = icmp ult i64 %3223, 64
  %3328 = select i1 %shft.chk1083, i64 %3327, i64 0
  %3329 = trunc i64 %3328 to i32
  %3330 = lshr i64 %3321, %3223
  %shft.chk1084 = icmp ult i64 %3223, 64
  %3331 = select i1 %shft.chk1084, i64 %3330, i64 0
  %3332 = trunc i64 %3331 to i32
  %3333 = trunc i64 %3278 to i32
  %3334 = xor i32 %3332, %3333
  %region_0_446_constant_1751085 = load i32, i32* bitcast ([4 x i8]* @32 to i32*), align 4
  %3335 = xor i32 %3334, %region_0_446_constant_1751085
  %3336 = zext i32 %3335 to i64
  %3337 = mul i64 %3336, %region_0_446_constant_641059
  %3338 = trunc i64 %3337 to i32
  %3339 = xor i32 %3329, %3338
  %region_0_446_constant_2401086 = load i32, i32* bitcast ([4 x i8]* @35 to i32*), align 4
  %3340 = xor i32 %3339, %region_0_446_constant_2401086
  %3341 = zext i32 %3340 to i64
  %3342 = mul i64 %3341, %region_0_446_constant_921055
  %3343 = lshr i64 %3342, %3223
  %shft.chk1087 = icmp ult i64 %3223, 64
  %3344 = select i1 %shft.chk1087, i64 %3343, i64 0
  %3345 = trunc i64 %3344 to i32
  %3346 = lshr i64 %3337, %3223
  %shft.chk1088 = icmp ult i64 %3223, 64
  %3347 = select i1 %shft.chk1088, i64 %3346, i64 0
  %3348 = trunc i64 %3347 to i32
  %3349 = trunc i64 %3294 to i32
  %3350 = xor i32 %3348, %3349
  %region_0_446_constant_1931089 = load i32, i32* bitcast ([4 x i8]* @22 to i32*), align 4
  %3351 = xor i32 %3350, %region_0_446_constant_1931089
  %3352 = zext i32 %3351 to i64
  %3353 = mul i64 %3352, %region_0_446_constant_921055
  %3354 = trunc i64 %3353 to i32
  %3355 = xor i32 %3345, %3354
  %region_0_446_constant_2571090 = load i32, i32* bitcast ([4 x i8]* @20 to i32*), align 4
  %3356 = xor i32 %3355, %region_0_446_constant_2571090
  %3357 = zext i32 %3356 to i64
  %3358 = mul i64 %3357, %region_0_446_constant_641059
  %3359 = trunc i64 %3358 to i32
  br label %concatenate.272.merge914

concatenate.pivot.2.1091:                         ; preds = %concatenate.272.merge561
  %3360 = icmp ult i32 %2623, 2
  br i1 %3360, label %concatenate.pivot.1.1092, label %concatenate.pivot.3.1095

concatenate.pivot.1.1092:                         ; preds = %concatenate.pivot.2.1091
  %3361 = icmp ult i32 %2623, 1
  br i1 %3361, label %concatenate.pivot.0.1093, label %concatenate.pivot.1.1094

concatenate.pivot.0.1093:                         ; preds = %concatenate.pivot.1.1092
  br label %concat_index_from_operand_id0915

concatenate.pivot.1.1094:                         ; preds = %concatenate.pivot.1.1092
  br label %concat_index_from_operand_id1961

concatenate.pivot.3.1095:                         ; preds = %concatenate.pivot.2.1091
  %3362 = icmp ult i32 %2623, 3
  br i1 %3362, label %concatenate.pivot.2.1096, label %concatenate.pivot.3.1097

concatenate.pivot.2.1096:                         ; preds = %concatenate.pivot.3.1095
  br label %concat_index_from_operand_id21003

concatenate.pivot.3.1097:                         ; preds = %concatenate.pivot.3.1095
  br label %concat_index_from_operand_id31049

concatenate.272.merge914:                         ; preds = %concat_index_from_operand_id31049, %concat_index_from_operand_id21003, %concat_index_from_operand_id1961, %concat_index_from_operand_id0915
  %3363 = phi i32 [ %2815, %concat_index_from_operand_id0915 ], [ %2992, %concat_index_from_operand_id1961 ], [ %3182, %concat_index_from_operand_id21003 ], [ %3359, %concat_index_from_operand_id31049 ]
  %region_0_446_constant_2731098 = load i32, i32* bitcast ([4 x i8]* @18 to i32*), align 4
  %3364 = lshr i32 %3363, %region_0_446_constant_2731098
  %shft.chk1099 = icmp ult i32 %region_0_446_constant_2731098, 32
  %3365 = select i1 %shft.chk1099, i32 %3364, i32 0
  %3366 = uitofp i32 %3365 to float
  %region_0_446_constant_2771100 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %3367 = load i32, i32* %5, align 4, !invariant.load !6
  %3368 = sitofp i32 %3367 to float
  %region_0_446_constant_2781101 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.2791102 = fmul float %3368, %region_0_446_constant_2781101
  %region_0_446_constant_2801103 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %3369 = fcmp oge float %region_0_446_constant_2771100, %multiply.2791102
  %3370 = fcmp une float %region_0_446_constant_2771100, %region_0_446_constant_2771100
  %3371 = or i1 %3369, %3370
  %3372 = select i1 %3371, float %region_0_446_constant_2771100, float %multiply.2791102
  %3373 = fcmp ole float %region_0_446_constant_2801103, %3372
  %3374 = fcmp une float %region_0_446_constant_2801103, %region_0_446_constant_2801103
  %3375 = or i1 %3373, %3374
  %3376 = select i1 %3375, float %region_0_446_constant_2801103, float %3372
  %multiply.2821104 = fmul float %3376, %3376
  %region_0_446_constant_2831105 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.2841106 = fmul float %multiply.2821104, %region_0_446_constant_2831105
  %region_0_446_constant_2851107 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.2861108 = fadd float %multiply.2841106, %region_0_446_constant_2851107
  %multiply.2871109 = fmul float %add.2861108, %multiply.2821104
  %region_0_446_constant_2881110 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.2891111 = fadd float %multiply.2871109, %region_0_446_constant_2881110
  %multiply.2901112 = fmul float %add.2891111, %multiply.2821104
  %region_0_446_constant_2911113 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.2921114 = fadd float %multiply.2901112, %region_0_446_constant_2911113
  %multiply.2931115 = fmul float %add.2921114, %multiply.2821104
  %region_0_446_constant_2941116 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.2951117 = fadd float %multiply.2931115, %region_0_446_constant_2941116
  %multiply.2961118 = fmul float %add.2951117, %multiply.2821104
  %region_0_446_constant_2971119 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.2981120 = fadd float %multiply.2961118, %region_0_446_constant_2971119
  %multiply.2991121 = fmul float %add.2981120, %multiply.2821104
  %region_0_446_constant_3001122 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.3011123 = fadd float %multiply.2991121, %region_0_446_constant_3001122
  %multiply.3021124 = fmul float %add.3011123, %multiply.2821104
  %region_0_446_constant_3031125 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.3041126 = fadd float %multiply.3021124, %region_0_446_constant_3031125
  %multiply.3051127 = fmul float %3376, %add.3041126
  %region_0_446_constant_3061128 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.3071129 = fadd float %multiply.2841106, %region_0_446_constant_3061128
  %multiply.3081130 = fmul float %add.3071129, %multiply.2821104
  %region_0_446_constant_3091131 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.3101132 = fadd float %multiply.3081130, %region_0_446_constant_3091131
  %multiply.3111133 = fmul float %add.3101132, %multiply.2821104
  %region_0_446_constant_3121134 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.3131135 = fadd float %multiply.3111133, %region_0_446_constant_3121134
  %multiply.3141136 = fmul float %add.3131135, %multiply.2821104
  %region_0_446_constant_3151137 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.3161138 = fadd float %multiply.3141136, %region_0_446_constant_3151137
  %multiply.3171139 = fmul float %add.3161138, %multiply.2821104
  %region_0_446_constant_3181140 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.3191141 = fadd float %multiply.3171139, %region_0_446_constant_3181140
  %divide.3201142 = fdiv float %multiply.3051127, %add.3191141
  %region_0_446_constant_2771143 = load float, float* bitcast ([4 x i8]* @4 to float*), align 4
  %3377 = load i32, i32* %3, align 4, !invariant.load !6
  %3378 = sitofp i32 %3377 to float
  %region_0_446_constant_2781144 = load float, float* bitcast ([4 x i8]* @3 to float*), align 4
  %multiply.3211145 = fmul float %3378, %region_0_446_constant_2781144
  %region_0_446_constant_2801146 = load float, float* bitcast ([4 x i8]* @2 to float*), align 4
  %3379 = fcmp oge float %region_0_446_constant_2771143, %multiply.3211145
  %3380 = fcmp une float %region_0_446_constant_2771143, %region_0_446_constant_2771143
  %3381 = or i1 %3379, %3380
  %3382 = select i1 %3381, float %region_0_446_constant_2771143, float %multiply.3211145
  %3383 = fcmp ole float %region_0_446_constant_2801146, %3382
  %3384 = fcmp une float %region_0_446_constant_2801146, %region_0_446_constant_2801146
  %3385 = or i1 %3383, %3384
  %3386 = select i1 %3385, float %region_0_446_constant_2801146, float %3382
  %multiply.3231147 = fmul float %3386, %3386
  %region_0_446_constant_2831148 = load float, float* bitcast ([4 x i8]* @9 to float*), align 4
  %multiply.3241149 = fmul float %multiply.3231147, %region_0_446_constant_2831148
  %region_0_446_constant_2851150 = load float, float* bitcast ([4 x i8]* @16 to float*), align 4
  %add.3251151 = fadd float %multiply.3241149, %region_0_446_constant_2851150
  %multiply.3261152 = fmul float %add.3251151, %multiply.3231147
  %region_0_446_constant_2881153 = load float, float* bitcast ([4 x i8]* @15 to float*), align 4
  %add.3271154 = fadd float %multiply.3261152, %region_0_446_constant_2881153
  %multiply.3281155 = fmul float %add.3271154, %multiply.3231147
  %region_0_446_constant_2911156 = load float, float* bitcast ([4 x i8]* @14 to float*), align 4
  %add.3291157 = fadd float %multiply.3281155, %region_0_446_constant_2911156
  %multiply.3301158 = fmul float %add.3291157, %multiply.3231147
  %region_0_446_constant_2941159 = load float, float* bitcast ([4 x i8]* @13 to float*), align 4
  %add.3311160 = fadd float %multiply.3301158, %region_0_446_constant_2941159
  %multiply.3321161 = fmul float %add.3311160, %multiply.3231147
  %region_0_446_constant_2971162 = load float, float* bitcast ([4 x i8]* @12 to float*), align 4
  %add.3331163 = fadd float %multiply.3321161, %region_0_446_constant_2971162
  %multiply.3341164 = fmul float %add.3331163, %multiply.3231147
  %region_0_446_constant_3001165 = load float, float* bitcast ([4 x i8]* @11 to float*), align 4
  %add.3351166 = fadd float %multiply.3341164, %region_0_446_constant_3001165
  %multiply.3361167 = fmul float %add.3351166, %multiply.3231147
  %region_0_446_constant_3031168 = load float, float* bitcast ([4 x i8]* @10 to float*), align 4
  %add.3371169 = fadd float %multiply.3361167, %region_0_446_constant_3031168
  %multiply.3381170 = fmul float %3386, %add.3371169
  %region_0_446_constant_3061171 = load float, float* bitcast ([4 x i8]* @8 to float*), align 4
  %add.3391172 = fadd float %multiply.3241149, %region_0_446_constant_3061171
  %multiply.3401173 = fmul float %add.3391172, %multiply.3231147
  %region_0_446_constant_3091174 = load float, float* bitcast ([4 x i8]* @7 to float*), align 4
  %add.3411175 = fadd float %multiply.3401173, %region_0_446_constant_3091174
  %multiply.3421176 = fmul float %add.3411175, %multiply.3231147
  %region_0_446_constant_3121177 = load float, float* bitcast ([4 x i8]* @6 to float*), align 4
  %add.3431178 = fadd float %multiply.3421176, %region_0_446_constant_3121177
  %multiply.3441179 = fmul float %add.3431178, %multiply.3231147
  %region_0_446_constant_3151180 = load float, float* bitcast ([4 x i8]* @5 to float*), align 4
  %add.3451181 = fadd float %multiply.3441179, %region_0_446_constant_3151180
  %multiply.3461182 = fmul float %add.3451181, %multiply.3231147
  %region_0_446_constant_3181183 = load float, float* bitcast ([4 x i8]* @1 to float*), align 4
  %add.3471184 = fadd float %multiply.3461182, %region_0_446_constant_3181183
  %divide.3481185 = fdiv float %multiply.3381170, %add.3471184
  %subtract.3491186 = fsub float %divide.3201142, %divide.3481185
  %region_0_446_constant_3501187 = load float, float* bitcast ([4 x i8]* @17 to float*), align 4
  %multiply.3511188 = fmul float %subtract.3491186, %region_0_446_constant_3501187
  %multiply.3531189 = fmul float %3366, %multiply.3511188
  %add.3551190 = fadd float %multiply.3531189, %divide.3481185
  %3387 = call float @llvm.fabs.f32(float %add.3551190)
  %region_0_446_constant_3581191 = load float, float* bitcast ([4 x i8]* @64 to float*), align 4
  %compare.3601192 = fcmp oeq float %3387, %region_0_446_constant_3581191
  %3388 = zext i1 %compare.3601192 to i8
  %region_0_446_constant_341193 = load float, float* bitcast ([4 x i8]* @63 to float*), align 4
  %multiply.3621194 = fmul float %add.3551190, %region_0_446_constant_341193
  %3389 = fneg float %add.3551190
  %multiply.3641195 = fmul float %3389, %add.3551190
  %3390 = call float @__nv_log1pf(float %multiply.3641195)
  %3391 = fneg float %3390
  %region_0_446_constant_3671196 = load float, float* bitcast ([4 x i8]* @44 to float*), align 4
  %compare.3691197 = fcmp olt float %3391, %region_0_446_constant_3671196
  %3392 = zext i1 %compare.3691197 to i8
  %region_0_446_constant_3701198 = load float, float* bitcast ([4 x i8]* @62 to float*), align 4
  %region_0_446_constant_3721199 = load float, float* bitcast ([4 x i8]* @61 to float*), align 4
  %3393 = trunc i8 %3392 to i1
  %3394 = select i1 %3393, float %region_0_446_constant_3701198, float %region_0_446_constant_3721199
  %region_0_446_constant_3751200 = load float, float* bitcast ([4 x i8]* @60 to float*), align 4
  %region_0_446_constant_3771201 = load float, float* bitcast ([4 x i8]* @59 to float*), align 4
  %3395 = trunc i8 %3392 to i1
  %3396 = select i1 %3395, float %region_0_446_constant_3751200, float %region_0_446_constant_3771201
  %region_0_446_constant_3801202 = load float, float* bitcast ([4 x i8]* @58 to float*), align 4
  %region_0_446_constant_3821203 = load float, float* bitcast ([4 x i8]* @57 to float*), align 4
  %3397 = trunc i8 %3392 to i1
  %3398 = select i1 %3397, float %region_0_446_constant_3801202, float %region_0_446_constant_3821203
  %region_0_446_constant_3851204 = load float, float* bitcast ([4 x i8]* @56 to float*), align 4
  %region_0_446_constant_3871205 = load float, float* bitcast ([4 x i8]* @55 to float*), align 4
  %3399 = trunc i8 %3392 to i1
  %3400 = select i1 %3399, float %region_0_446_constant_3851204, float %region_0_446_constant_3871205
  %region_0_446_constant_3901206 = load float, float* bitcast ([4 x i8]* @54 to float*), align 4
  %region_0_446_constant_3921207 = load float, float* bitcast ([4 x i8]* @53 to float*), align 4
  %3401 = trunc i8 %3392 to i1
  %3402 = select i1 %3401, float %region_0_446_constant_3901206, float %region_0_446_constant_3921207
  %region_0_446_constant_3951208 = load float, float* bitcast ([4 x i8]* @52 to float*), align 4
  %region_0_446_constant_3971209 = load float, float* bitcast ([4 x i8]* @51 to float*), align 4
  %3403 = trunc i8 %3392 to i1
  %3404 = select i1 %3403, float %region_0_446_constant_3951208, float %region_0_446_constant_3971209
  %region_0_446_constant_4001210 = load float, float* bitcast ([4 x i8]* @50 to float*), align 4
  %region_0_446_constant_4021211 = load float, float* bitcast ([4 x i8]* @49 to float*), align 4
  %3405 = trunc i8 %3392 to i1
  %3406 = select i1 %3405, float %region_0_446_constant_4001210, float %region_0_446_constant_4021211
  %region_0_446_constant_4051212 = load float, float* bitcast ([4 x i8]* @48 to float*), align 4
  %region_0_446_constant_4071213 = load float, float* bitcast ([4 x i8]* @47 to float*), align 4
  %3407 = trunc i8 %3392 to i1
  %3408 = select i1 %3407, float %region_0_446_constant_4051212, float %region_0_446_constant_4071213
  %region_0_446_constant_4101214 = load float, float* bitcast ([4 x i8]* @46 to float*), align 4
  %region_0_446_constant_4121215 = load float, float* bitcast ([4 x i8]* @45 to float*), align 4
  %3409 = trunc i8 %3392 to i1
  %3410 = select i1 %3409, float %region_0_446_constant_4101214, float %region_0_446_constant_4121215
  %region_0_446_constant_4151216 = load float, float* bitcast ([4 x i8]* @43 to float*), align 4
  %add.4171217 = fadd float %3391, %region_0_446_constant_4151216
  %3411 = call float @__nv_sqrtf(float %3391)
  %region_0_446_constant_4191218 = load float, float* bitcast ([4 x i8]* @42 to float*), align 4
  %add.4211219 = fadd float %3411, %region_0_446_constant_4191218
  %3412 = trunc i8 %3392 to i1
  %3413 = select i1 %3412, float %add.4171217, float %add.4211219
  %multiply.4231220 = fmul float %3410, %3413
  %add.4241221 = fadd float %3408, %multiply.4231220
  %multiply.4251222 = fmul float %add.4241221, %3413
  %add.4261223 = fadd float %3406, %multiply.4251222
  %multiply.4271224 = fmul float %add.4261223, %3413
  %add.4281225 = fadd float %3404, %multiply.4271224
  %multiply.4291226 = fmul float %add.4281225, %3413
  %add.4301227 = fadd float %3402, %multiply.4291226
  %multiply.4311228 = fmul float %add.4301227, %3413
  %add.4321229 = fadd float %3400, %multiply.4311228
  %multiply.4331230 = fmul float %add.4321229, %3413
  %add.4341231 = fadd float %3398, %multiply.4331230
  %multiply.4351232 = fmul float %add.4341231, %3413
  %add.4361233 = fadd float %3396, %multiply.4351232
  %multiply.4371234 = fmul float %add.4361233, %3413
  %add.4381235 = fadd float %3394, %multiply.4371234
  %multiply.4391236 = fmul float %add.4381235, %add.3551190
  %3414 = trunc i8 %3388 to i1
  %3415 = select i1 %3414, float %multiply.3621194, float %multiply.4391236
  %region_0_446_constant_4411237 = load float, float* bitcast ([4 x i8]* @0 to float*), align 4
  %multiply.4431238 = fmul float %3415, %region_0_446_constant_4411237
  %3416 = fcmp oge float %2618, %multiply.4431238
  %3417 = fcmp une float %2618, %2618
  %3418 = or i1 %3416, %3417
  %maximum.4441239 = select i1 %3418, float %2618, float %multiply.4431238
  %3419 = fcmp ole float %2596, %maximum.4441239
  %3420 = fcmp une float %2596, %2596
  %3421 = or i1 %3419, %3420
  %minimum.4451240 = select i1 %3421, float %2596, float %maximum.4441239
  %3422 = bitcast [16 x [16 x float]]* %7 to float*
  %3423 = getelementptr inbounds float, float* %3422, i32 %linear_index3
  store float %minimum.4451240, float* %3423, align 4
  br label %fusion.in_bounds-after
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() #0

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() #0

; Function Attrs: inaccessiblememonly nocallback nofree nosync nounwind willreturn
declare void @llvm.assume(i1 noundef) #1

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.fabs.f32(float) #2

; Function Attrs: nounwind readnone
declare float @__nv_log1pf(float) #3

; Function Attrs: nounwind readnone
declare float @__nv_sqrtf(float) #3

attributes #0 = { nounwind readnone speculatable }
attributes #1 = { inaccessiblememonly nocallback nofree nosync nounwind willreturn }
attributes #2 = { nocallback nofree nosync nounwind readnone speculatable willreturn }
attributes #3 = { nounwind readnone }

!nvvm.annotations = !{!0, !1, !2, !3}

!0 = !{void (i8*)* @rng_get_and_update_state, !"kernel", i32 1}
!1 = !{void (i8*)* @rng_get_and_update_state, !"reqntidx", i32 1}
!2 = !{void (i8*, i8*, i8*, i8*)* @fusion, !"kernel", i32 1}
!3 = !{void (i8*, i8*, i8*, i8*)* @fusion, !"reqntidx", i32 64}
!4 = !{i32 0, i32 1}
!5 = !{i32 0, i32 64}
!6 = !{}
