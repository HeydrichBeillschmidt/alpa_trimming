BufferAssignment:
allocation 0: 0x2af1460, size 16384, output shape is |f32[256,16]|, maybe-live-out:
 value: <2 cublas-gemm.1 @0> (size=16384,offset=0): f32[256,16]{1,0}
allocation 1: 0x2af1510, size 16384, parameter 0, shape |f32[256,16]| at ShapeIndex {}:
 value: <0 Arg_0.1 @0> (size=16384,offset=0): f32[256,16]{1,0}
allocation 2: 0x2af15c0, size 1024, parameter 1, shape |f32[16,16]| at ShapeIndex {}:
 value: <1 Arg_1.2 @0> (size=1024,offset=0): f32[16,16]{1,0}

Total bytes used: 33792 (33.0KiB)

Used values:
<0 Arg_0.1 @0>
 positions:
  Arg_0.1
 uses:
  cublas-gemm.1, operand 0
 from instruction:%Arg_0.1 = f32[256,16]{1,0} parameter(0)
<1 Arg_1.2 @0>
 positions:
  Arg_1.2
 uses:
  cublas-gemm.1, operand 1
 from instruction:%Arg_1.2 = f32[16,16]{1,0} parameter(1)
<2 cublas-gemm.1 @0>
 positions:
  cublas-gemm.1
 uses:
 from instruction:%cublas-gemm.1 = f32[256,16]{1,0} custom-call(f32[256,16]{1,0} %Arg_0.1, f32[16,16]{1,0} %Arg_1.2), custom_call_target="__cublas$gemm", metadata={op_name="jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"4096\",\"rhs_stride\":\"256\",\"selected_algorithm\":\"-1\"}"


HloLiveRange (max 3):
  InstructionSequence:
    0:Arg_1.2
    1:Arg_0.1
    2:cublas-gemm.1
  BufferLiveRange:
    Arg_0.1{}:0-3
    Arg_1.2{}:0-3
    cublas-gemm.1{}:2-3
  Live ranges at 2 (peak):
    Arg_0.1: 16384 bytes
    Arg_1.2: 1024 bytes
    cublas-gemm.1: 16384 bytes
