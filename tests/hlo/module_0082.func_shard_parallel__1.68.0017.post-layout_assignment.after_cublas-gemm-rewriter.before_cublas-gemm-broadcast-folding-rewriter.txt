HloModule func_shard_parallel__1.68, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias) }

scatter_add_reducer.50 {
  parameter.51 = f32[] parameter(0)
  parameter.52 = f32[] parameter(1)
  ROOT add.53 = f32[] add(parameter.51, parameter.52)
}

add {
  x = f32[] parameter(0)
  y = f32[] parameter(1)
  ROOT add = f32[] add(x, y)
}

add.1 {
  x.1 = f32[] parameter(0)
  y.1 = f32[] parameter(1)
  ROOT add.1 = f32[] add(x.1, y.1)
}

ENTRY func_shard_parallel__1.68_spmd {
  param = s32[] parameter(0), sharding={replicated}
  constant.4 = s32[] constant(1), metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
  add.2 = s32[] add(param, constant.4), metadata={op_type="add" op_name="parallelize(func_shard_parallel)/add" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/base.py" source_line=99}
  param.1 = f32[32,32]{1,0} parameter(1), sharding={replicated}
  param.2 = f32[64,32]{1,0} parameter(3), sharding={devices=[4,1]0,1,2,3}
  constant.5 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="parallelize(func_shard_parallel)/reduce_sum[axes=(0, 1)]" source_file="test_auto_sharding_basic.py" source_line=144}
  broadcast.7 = f32[64,32]{0,1} broadcast(constant.5), dimensions={}
  iota.1 = s32[4,1]{1,0} iota(), iota_dimension=0, metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  partition-id = u32[] partition-id(), metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  convert = s32[] convert(partition-id), metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  constant.13 = s32[] constant(4), metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  multiply.3 = s32[] multiply(convert, constant.13), metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  broadcast.3 = s32[4,1]{1,0} broadcast(multiply.3), dimensions={}, metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  add.3 = s32[4,1]{1,0} add(iota.1, broadcast.3), metadata={op_type="iota" op_name="parallelize(func_shard_parallel)/iota[dtype=int32 shape=(16,) dimension=0]" source_file="test_auto_sharding_basic.py" source_line=125}
  bitcast = s32[1,4,1]{2,1,0} bitcast(add.3)
  all-gather = s32[4,4,1]{2,1,0} all-gather(bitcast), channel_id=1, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true
  bitcast.2 = s32[16,1]{0,1} bitcast(all-gather)
  cublas-gemm.1 = f32[64,32]{1,0} custom-call(param.2, param.1), custom_call_target="__cublas$gemm", metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"2048\",\"rhs_stride\":\"1024\"}"
  gather = f32[64,16]{1,0} gather(cublas-gemm.1, bitcast.2), offset_dims={0}, collapsed_slice_dims={1}, start_index_map={1}, index_vector_dim=1, slice_sizes={64,1}, metadata={op_type="gather" op_name="parallelize(func_shard_parallel)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,)) slice_sizes=(256, 1) unique_indices=False indices_are_sorted=False mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]" source_file="test_auto_sharding_basic.py" source_line=126}
  param.3 = f32[16,16]{1,0} parameter(2), sharding={replicated}
  cublas-gemm.3 = f32[64,16]{1,0} custom-call(gather, param.3), custom_call_target="__cublas$gemm", metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"1024\",\"rhs_stride\":\"256\"}"
  param.4 = f32[64,16]{1,0} parameter(4), sharding={devices=[4,1]0,1,2,3}
  subtract.0 = f32[64,16]{1,0} subtract(cublas-gemm.3, param.4), metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="test_auto_sharding_basic.py" source_line=144}
  constant.15 = f32[] constant(0.00048828125), metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=144}
  broadcast.8 = f32[64,16]{1,0} broadcast(constant.15), dimensions={}
  multiply.4 = f32[64,16]{1,0} multiply(subtract.0, broadcast.8), metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="test_auto_sharding_basic.py" source_line=144}
  cublas-gemm.5 = f32[64,16]{0,1} custom-call(multiply.4, param.3), custom_call_target="__cublas$gemm", metadata={op_type="dot_general" op_name="parallelize(func_shard_parallel)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"1024\",\"rhs_stride\":\"256\"}"
  scatter = f32[64,32]{0,1} scatter(broadcast.7, bitcast.2, cublas-gemm.5), update_window_dims={0}, inserted_window_dims={1}, scatter_dims_to_operand_dims={1}, index_vector_dim=1, to_apply=scatter_add_reducer.50, metadata={op_type="scatter-add" op_name="parallelize(func_shard_parallel)/scatter-add[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(1,)) indices_are_sorted=False unique_indices=False mode=GatherScatterMode.PROMISE_IN_BOUNDS]" source_file="test_auto_sharding_basic.py" source_line=126}
  cublas-gemm.7 = f32[32,32]{1,0} custom-call(param.2, scatter), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"2048\",\"rhs_stride\":\"2048\"}"
  all-reduce = f32[32,32]{1,0} all-reduce(cublas-gemm.7), channel_id=2, replica_groups={{0}}, to_apply=add, metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  constant.20 = f32[] constant(0.01), metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  broadcast.5 = f32[32,32]{1,0} broadcast(constant.20), dimensions={}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  multiply.5 = f32[32,32]{1,0} multiply(all-reduce, broadcast.5), metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  subtract.1 = f32[32,32]{1,0} subtract(param.1, multiply.5), metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  cublas-gemm.9 = f32[16,16]{1,0} custom-call(gather, multiply.4), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"1024\",\"rhs_stride\":\"1024\"}"
  all-reduce.1 = f32[16,16]{1,0} all-reduce(cublas-gemm.9), channel_id=3, replica_groups={{0}}, to_apply=add.1, metadata={op_type="transpose" op_name="parallelize(func_shard_parallel)/transpose[permutation=(1, 0)]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}
  broadcast.6 = f32[16,16]{1,0} broadcast(constant.20), dimensions={}, metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  multiply.6 = f32[16,16]{1,0} multiply(all-reduce.1, broadcast.6), metadata={op_type="mul" op_name="parallelize(func_shard_parallel)/mul" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  subtract.2 = f32[16,16]{1,0} subtract(param.3, multiply.6), metadata={op_type="sub" op_name="parallelize(func_shard_parallel)/sub" source_file="/usr/local/lib/python3.7/dist-packages/flax/optim/sgd.py" source_line=45}
  ROOT tuple = (s32[], f32[32,32]{1,0}, f32[16,16]{1,0}) tuple(add.2, subtract.1, subtract.2)
}

