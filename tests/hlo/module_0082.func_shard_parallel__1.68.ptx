//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_70
.address_size 64

	// .globl	add_2
.visible .global .align 128 .b8 buffer_for_constant_4[4] = {1, 0, 0, 0};

.visible .entry add_2(
	.param .u64 add_2_param_0,
	.param .u64 add_2_param_1
)
.reqntid 1, 1, 1
{
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<3>;

	ld.param.u64 	%rd1, [add_2_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.u32 	%r1, [%rd2];
	add.s32 	%r2, %r1, 1;
	st.global.u32 	[%rd2], %r2;
	ret;

}
	// .globl	fusion_3
.visible .entry fusion_3(
	.param .u64 fusion_3_param_0
)
.reqntid 1, 1, 1
{
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<3>;

	ld.param.u64 	%rd1, [fusion_3_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.nc.u32 	%r1, [%rd2+128];
	shl.b32 	%r2, %r1, 2;
	or.b32  	%r3, %r2, 1;
	or.b32  	%r4, %r2, 2;
	or.b32  	%r5, %r2, 3;
	st.global.v4.u32 	[%rd2], {%r2, %r3, %r4, %r5};
	ret;

}
	// .globl	fusion_4
.visible .entry fusion_4(
	.param .u64 fusion_4_param_0
)
.reqntid 256, 1, 1
{
	.reg .b32 	%r<17>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<17>;

	ld.param.u64 	%rd1, [fusion_4_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r2, %r1, 2;
	shr.u32 	%r3, %r1, 2;
	mul.wide.u32 	%rd3, %r2, 4;
	add.s64 	%rd4, %rd2, %rd3;
	and.b32  	%r4, %r1, 3;
	mul.wide.u32 	%rd5, %r4, 16;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.v4.u32 	{%r5, %r6, %r7, %r8}, [%rd6+13312];
	max.s32 	%r9, %r5, 0;
	min.u32 	%r10, %r9, 31;
	mul.wide.u32 	%rd7, %r3, 128;
	add.s64 	%rd8, %rd2, %rd7;
	mul.wide.u32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	max.s32 	%r11, %r6, 0;
	min.u32 	%r12, %r11, 31;
	mul.wide.u32 	%rd11, %r12, 4;
	add.s64 	%rd12, %rd8, %rd11;
	ld.global.nc.f32 	%f2, [%rd12];
	max.s32 	%r13, %r7, 0;
	min.u32 	%r14, %r13, 31;
	mul.wide.u32 	%rd13, %r14, 4;
	add.s64 	%rd14, %rd8, %rd13;
	ld.global.nc.f32 	%f3, [%rd14];
	max.s32 	%r15, %r8, 0;
	min.u32 	%r16, %r15, 31;
	mul.wide.u32 	%rd15, %r16, 4;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.nc.f32 	%f4, [%rd16];
	st.global.v4.f32 	[%rd4+8192], {%f1, %f2, %f3, %f4};
	ret;

}
	// .globl	fusion_2
.visible .entry fusion_2(
	.param .u64 fusion_2_param_0,
	.param .u64 fusion_2_param_1
)
.reqntid 256, 1, 1
{
	.reg .b32 	%r<3>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [fusion_2_param_0];
	ld.param.u64 	%rd2, [fusion_2_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r2, %r1, 2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.global.nc.v4.f32 	{%f5, %f6, %f7, %f8}, [%rd7];
	sub.rn.f32 	%f9, %f1, %f5;
	mul.rn.f32 	%f10, %f9, 0f3A000000;
	sub.rn.f32 	%f11, %f2, %f6;
	mul.rn.f32 	%f12, %f11, 0f3A000000;
	sub.rn.f32 	%f13, %f3, %f7;
	mul.rn.f32 	%f14, %f13, 0f3A000000;
	sub.rn.f32 	%f15, %f4, %f8;
	mul.rn.f32 	%f16, %f15, 0f3A000000;
	st.global.v4.f32 	[%rd6], {%f10, %f12, %f14, %f16};
	ret;

}
	// .globl	input_fusion_scatter
.visible .entry input_fusion_scatter(
	.param .u64 input_fusion_scatter_param_0
)
.reqntid 256, 1, 1
{
	.reg .b32 	%r<6>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd1, [input_fusion_scatter_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r2, %r1, 10;
	mov.u32 	%r3, %tid.x;
	shl.b32 	%r4, %r3, 2;
	or.b32  	%r5, %r2, %r4;
	mul.wide.u32 	%rd3, %r5, 4;
	add.s64 	%rd4, %rd2, %rd3;
	mov.f32 	%f1, 0f00000000;
	st.global.v4.f32 	[%rd4], {%f1, %f1, %f1, %f1};
	ret;

}
	// .globl	input_fusion_scatter__1
.visible .entry input_fusion_scatter__1(
	.param .u64 input_fusion_scatter__1_param_0
)
.reqntid 1024, 1, 1
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<14>;

	ld.param.u64 	%rd3, [input_fusion_scatter__1_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	shr.u32 	%r2, %r1, 8;
	mul.wide.u32 	%rd5, %r2, 16;
	add.s64 	%rd6, %rd4, %rd5;
	bfe.u32 	%r3, %r1, 6, 2;
	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u32 	%r4, [%rd8+13312];
	setp.lt.u32 	%p1, %r4, 32;
	@%p1 bra 	LBB5_2;
	bra.uni 	LBB5_1;
LBB5_2:
	mul.wide.u32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd4, %rd9;
	add.s64 	%rd1, %rd10, 8192;
	and.b32  	%r5, %r1, 63;
	mul.wide.u32 	%rd11, %r4, 256;
	add.s64 	%rd12, %rd4, %rd11;
	mul.wide.u32 	%rd13, %r5, 4;
	add.s64 	%rd2, %rd12, %rd13;
	ld.global.nc.f32 	%f1, [%rd1];
	atom.global.add.f32 	%f2, [%rd2], %f1;
LBB5_1:
	ret;

}
	// .globl	concatenate_1
.visible .entry concatenate_1(
	.param .u64 concatenate_1_param_0
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<10>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<61>;

	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	shl.b32 	%r7, %r5, 10;
	shl.b32 	%r8, %r6, 2;
	or.b32  	%r1, %r7, %r8;
	setp.lt.u32 	%p1, %r1, 1280;
	@%p1 bra 	LBB6_2;
	bra.uni 	LBB6_1;
LBB6_2:
	ld.param.u64 	%rd28, [concatenate_1_param_0];
	cvta.to.global.u64 	%rd1, %rd28;
	add.s64 	%rd2, %rd1, 8192;
	add.s64 	%rd3, %rd1, 12288;
	or.b32  	%r2, %r1, 1;
	setp.gt.u32 	%p2, %r1, 1023;
	mul.wide.u32 	%rd52, %r1, 4;
	@%p2 bra 	LBB6_4;
	cvt.u64.u32 	%rd53, %r1;
	add.s64 	%rd54, %rd2, %rd52;
	bra.uni 	LBB6_5;
LBB6_4:
	add.s32 	%r9, %r1, -1024;
	mul.wide.u32 	%rd29, %r9, 4;
	add.s64 	%rd54, %rd3, %rd29;
	cvt.u64.u32 	%rd53, %r1;
LBB6_5:
	or.b32  	%r3, %r1, 2;
	ld.global.nc.f32 	%f1, [%rd54];
	shl.b64 	%rd31, %rd53, 2;
	add.s64 	%rd32, %rd1, %rd31;
	st.global.f32 	[%rd32], %f1;
	setp.gt.u32 	%p3, %r2, 1023;
	@%p3 bra 	LBB6_7;
	cvt.u64.u32 	%rd55, %r2;
	add.s64 	%rd36, %rd2, %rd52;
	add.s64 	%rd56, %rd36, 4;
	bra.uni 	LBB6_8;
LBB6_7:
	add.s64 	%rd34, %rd3, %rd52;
	add.s64 	%rd56, %rd34, -4092;
	cvt.u64.u32 	%rd55, %r2;
LBB6_8:
	or.b32  	%r4, %r1, 3;
	ld.global.nc.f32 	%f2, [%rd56];
	shl.b64 	%rd37, %rd55, 2;
	add.s64 	%rd38, %rd1, %rd37;
	st.global.f32 	[%rd38], %f2;
	setp.gt.u32 	%p4, %r3, 1023;
	@%p4 bra 	LBB6_10;
	cvt.u64.u32 	%rd57, %r3;
	add.s64 	%rd42, %rd2, %rd52;
	add.s64 	%rd58, %rd42, 8;
	bra.uni 	LBB6_11;
LBB6_10:
	add.s64 	%rd40, %rd3, %rd52;
	add.s64 	%rd58, %rd40, -4088;
	cvt.u64.u32 	%rd57, %r3;
LBB6_11:
	ld.global.nc.f32 	%f3, [%rd58];
	shl.b64 	%rd43, %rd57, 2;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.f32 	[%rd44], %f3;
	setp.gt.u32 	%p5, %r4, 1023;
	@%p5 bra 	LBB6_13;
	cvt.u64.u32 	%rd59, %r4;
	add.s64 	%rd48, %rd2, %rd52;
	add.s64 	%rd60, %rd48, 12;
	bra.uni 	LBB6_14;
LBB6_13:
	add.s64 	%rd46, %rd3, %rd52;
	add.s64 	%rd60, %rd46, -4084;
	cvt.u64.u32 	%rd59, %r4;
LBB6_14:
	ld.global.nc.f32 	%f4, [%rd60];
	shl.b64 	%rd49, %rd59, 2;
	add.s64 	%rd50, %rd1, %rd49;
	st.global.f32 	[%rd50], %f4;
LBB6_1:
	ret;

}
	// .globl	slice_3
.visible .entry slice_3(
	.param .u64 slice_3_param_0
)
.reqntid 64, 1, 1
{
	.reg .b32 	%r<3>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd1, [slice_3_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r2, %r1, 2;
	mul.wide.u32 	%rd3, %r2, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4+4096];
	st.global.v4.f32 	[%rd4+9216], {%f1, %f2, %f3, %f4};
	ret;

}
	// .globl	slice_2
.visible .entry slice_2(
	.param .u64 slice_2_param_0
)
.reqntid 256, 1, 1
{
	.reg .b32 	%r<3>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd1, [slice_2_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r2, %r1, 2;
	mul.wide.u32 	%rd3, %r2, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd4];
	st.global.v4.f32 	[%rd4+5120], {%f1, %f2, %f3, %f4};
	ret;

}
	// .globl	fusion_5
.visible .entry fusion_5(
	.param .u64 fusion_5_param_0,
	.param .u64 fusion_5_param_1,
	.param .u64 fusion_5_param_2
)
.reqntid 1024, 1, 1
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<12>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<29>;

	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r4, %r3, 10;
	or.b32  	%r2, %r4, %r1;
	setp.lt.u32 	%p1, %r2, 1280;
	@%p1 bra 	LBB9_2;
	bra.uni 	LBB9_1;
LBB9_2:
	ld.param.u64 	%rd13, [fusion_5_param_0];
	ld.param.u64 	%rd14, [fusion_5_param_2];
	cvta.to.global.u64 	%rd15, %rd14;
	ld.param.u64 	%rd16, [fusion_5_param_1];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd13;
	setp.gt.u32 	%p2, %r2, 1023;
	add.s32 	%r11, %r2, -1024;
	@%p2 bra 	LBB9_4;
	add.s64 	%rd3, %rd15, 5120;
	and.b32  	%r8, %r1, 31;
	shr.u32 	%r9, %r2, 5;
	mul.wide.u32 	%rd21, %r9, 128;
	add.s64 	%rd22, %rd2, %rd21;
	mul.wide.u32 	%rd23, %r8, 4;
	add.s64 	%rd28, %rd22, %rd23;
	add.s64 	%rd24, %rd3, %rd21;
	add.s64 	%rd27, %rd24, %rd23;
	bra.uni 	LBB9_5;
LBB9_4:
	add.s64 	%rd4, %rd15, 9216;
	and.b32  	%r6, %r1, 15;
	shr.u32 	%r7, %r11, 4;
	mul.wide.u32 	%rd17, %r7, 64;
	add.s64 	%rd18, %rd1, %rd17;
	mul.wide.u32 	%rd19, %r6, 4;
	add.s64 	%rd28, %rd18, %rd19;
	add.s64 	%rd20, %rd4, %rd17;
	add.s64 	%rd27, %rd20, %rd19;
LBB9_5:
	setp.lt.u32 	%p3, %r2, 1024;
	ld.global.f32 	%f2, [%rd28];
	ld.global.nc.f32 	%f3, [%rd27];
	mul.rn.f32 	%f4, %f3, 0fBC23D70A;
	add.rn.f32 	%f1, %f2, %f4;
	@%p3 bra 	LBB9_8;
	bra.uni 	LBB9_6;
LBB9_8:
	mul.wide.u32 	%rd25, %r2, 4;
	add.s64 	%rd11, %rd2, %rd25;
	st.global.f32 	[%rd11], %f1;
LBB9_6:
	@%p2 bra 	LBB9_7;
	bra.uni 	LBB9_1;
LBB9_7:
	mul.wide.u32 	%rd26, %r11, 4;
	add.s64 	%rd12, %rd1, %rd26;
	st.global.f32 	[%rd12], %f1;
LBB9_1:
	ret;

}
