BufferAssignment:
allocation 0: 0x15a0f750, size 32768, output shape is |f32[256,32]|, maybe-live-out:
 value: <2 cublas-gemm.1 @0> (size=32768,offset=0): f32[256,32]{1,0}
allocation 1: 0x15a0f800, size 32768, parameter 0, shape |f32[256,32]| at ShapeIndex {}:
 value: <0 Arg_0.1 @0> (size=32768,offset=0): f32[256,32]{1,0}
allocation 2: 0x15a0f8b0, size 4096, parameter 1, shape |f32[32,32]| at ShapeIndex {}:
 value: <1 Arg_1.2 @0> (size=4096,offset=0): f32[32,32]{1,0}

Total bytes used: 69632 (68.0KiB)

Used values:
<0 Arg_0.1 @0>
 positions:
  Arg_0.1
 uses:
  cublas-gemm.1, operand 0
 from instruction:%Arg_0.1 = f32[256,32]{1,0} parameter(0)
<1 Arg_1.2 @0>
 positions:
  Arg_1.2
 uses:
  cublas-gemm.1, operand 1
 from instruction:%Arg_1.2 = f32[32,32]{1,0} parameter(1)
<2 cublas-gemm.1 @0>
 positions:
  cublas-gemm.1
 uses:
 from instruction:%cublas-gemm.1 = f32[256,32]{1,0} custom-call(f32[256,32]{1,0} %Arg_0.1, f32[32,32]{1,0} %Arg_1.2), custom_call_target="__cublas$gemm", metadata={op_name="jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.7/dist-packages/flax/linen/linear.py" source_line=190}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"8192\",\"rhs_stride\":\"1024\",\"selected_algorithm\":\"4\"}"


HloLiveRange (max 3):
  InstructionSequence:
    0:Arg_1.2
    1:Arg_0.1
    2:cublas-gemm.1
  BufferLiveRange:
    Arg_0.1{}:0-3
    Arg_1.2{}:0-3
    cublas-gemm.1{}:2-3
  Live ranges at 2 (peak):
    Arg_0.1: 32768 bytes
    Arg_1.2: 4096 bytes
    cublas-gemm.1: 32768 bytes
